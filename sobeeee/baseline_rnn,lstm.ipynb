{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0b69bf",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8474f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "from scipy import stats #Analysis \n",
    "from scipy.stats import norm \n",
    "\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib# Importb.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800f577b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\pythontemp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507af345",
   "metadata": {},
   "source": [
    "# 1. Data Load  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23305c3e",
   "metadata": {},
   "source": [
    "**치매 예방을 위한 라이프로그 치매 분류**\n",
    "\n",
    "**9,327 rows × 66 columns**\n",
    "\n",
    "For more details https://aihub.or.kr/problem_contest/nipa-learning-platform/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb32c5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>summary_date</th>\n",
       "      <th>activity_average_met</th>\n",
       "      <th>activity_cal_active</th>\n",
       "      <th>activity_cal_total</th>\n",
       "      <th>activity_class_5min</th>\n",
       "      <th>activity_daily_movement</th>\n",
       "      <th>activity_high</th>\n",
       "      <th>activity_inactive</th>\n",
       "      <th>activity_inactivity_alerts</th>\n",
       "      <th>...</th>\n",
       "      <th>sleep_temperature_deviation</th>\n",
       "      <th>sleep_temperature_trend_deviation</th>\n",
       "      <th>timezone</th>\n",
       "      <th>sleep_total</th>\n",
       "      <th>CONVERT(activity_class_5min USING utf8)</th>\n",
       "      <th>CONVERT(activity_met_1min USING utf8)</th>\n",
       "      <th>CONVERT(sleep_hr_5min USING utf8)</th>\n",
       "      <th>CONVERT(sleep_hypnogram_5min USING utf8)</th>\n",
       "      <th>CONVERT(sleep_rmssd_5min USING utf8)</th>\n",
       "      <th>DIAG_NM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nia+404@rowan.kr</td>\n",
       "      <td>2020-11-27</td>\n",
       "      <td>1.71875</td>\n",
       "      <td>730</td>\n",
       "      <td>2944</td>\n",
       "      <td>...</td>\n",
       "      <td>14346</td>\n",
       "      <td>0</td>\n",
       "      <td>417</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>2/1/1/1/1/1/2/2/1/1/1/1/1/1/2/2/2/3/2/2/2/2/2/...</td>\n",
       "      <td>0.9/0.9/1.4/1.9/1.1/0.9/0.9/1.1/1.3/1/0.9/1.1/...</td>\n",
       "      <td>0/73/73/73/72/71/70/71/71/71/70/70/73/72/74/74...</td>\n",
       "      <td>4/2/4/3/3/1/2/2/2/2/2/2/3/3/3/4/4/3/2/2/2/2/2/...</td>\n",
       "      <td>0/10/10/10/11/11/10/12/18/13/14/12/10/10/18/17...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nia+404@rowan.kr</td>\n",
       "      <td>2020-11-28</td>\n",
       "      <td>1.40625</td>\n",
       "      <td>342</td>\n",
       "      <td>2449</td>\n",
       "      <td>...</td>\n",
       "      <td>6352</td>\n",
       "      <td>0</td>\n",
       "      <td>473</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/2/2/2/2/2/2/...</td>\n",
       "      <td>1.2/1.1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....</td>\n",
       "      <td>69/70/69/69/70/72/71/72/70/69/69/69/68/68/63/6...</td>\n",
       "      <td>2/4/2/2/2/2/3/1/2/2/4/4/2/2/2/2/2/2/2/2/2/2/4/...</td>\n",
       "      <td>23/23/26/24/18/13/15/14/17/20/24/30/23/25/22/1...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nia+404@rowan.kr</td>\n",
       "      <td>2020-11-29</td>\n",
       "      <td>1.46875</td>\n",
       "      <td>401</td>\n",
       "      <td>2544</td>\n",
       "      <td>...</td>\n",
       "      <td>7297</td>\n",
       "      <td>0</td>\n",
       "      <td>586</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/1/2/1/1/1/1/2/2/2/2/2/1/1/1/1/1/2/...</td>\n",
       "      <td>1.1/1.1/1.2/1.1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....</td>\n",
       "      <td>0/74/73/73/74/74/74/71/71/70/70/69/70/68/66/69...</td>\n",
       "      <td>4/2/4/4/1/1/1/4/4/4/4/4/4/4/2/3/4/2/2/4/2/2/2/...</td>\n",
       "      <td>0/11/14/20/13/14/14/16/27/29/27/20/19/19/14/12...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nia+404@rowan.kr</td>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>27</td>\n",
       "      <td>1850</td>\n",
       "      <td>...</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>2/1/2/2/1/2/1/1/2/1/1/1/1/1/2/1/1/1/1/1/2/2/2/...</td>\n",
       "      <td>0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/...</td>\n",
       "      <td>73/70/71/72/75/75/73/70/70/70/67/63/63/63/63/6...</td>\n",
       "      <td>4/4/4/4/3/3/3/2/4/4/4/2/2/2/2/2/2/2/2/4/2/2/2/...</td>\n",
       "      <td>24/28/19/17/12/10/17/20/23/23/25/31/26/25/34/3...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nia+404@rowan.kr</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>1.46875</td>\n",
       "      <td>333</td>\n",
       "      <td>2518</td>\n",
       "      <td>...</td>\n",
       "      <td>5861</td>\n",
       "      <td>0</td>\n",
       "      <td>646</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/2/2/3/3/2/...</td>\n",
       "      <td>0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....</td>\n",
       "      <td>0/0/0/0/0/0/0/0/69/69/71/69/65/66/64/64/65/66/...</td>\n",
       "      <td>4/4/4/4/4/4/4/4/4/4/4/2/2/2/2/3/3/2/4/4/4/2/2/...</td>\n",
       "      <td>0/0/0/0/0/0/0/0/21/22/26/23/19/29/22/17/14/13/...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>nia+206@rowan.kr</td>\n",
       "      <td>2020-12-22</td>\n",
       "      <td>1.34375</td>\n",
       "      <td>227</td>\n",
       "      <td>2316</td>\n",
       "      <td>...</td>\n",
       "      <td>3863</td>\n",
       "      <td>3</td>\n",
       "      <td>735</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>2/1/1/2/2/1/1/1/1/2/2/3/2/1/1/1/1/1/1/1/1/1/1/...</td>\n",
       "      <td>1.1/1.4/1.2/0.9/1.2/1.1/0.9/0.9/0.9/1/0.9/0.9/...</td>\n",
       "      <td>0/54/54/54/55/56/0/0/55/52/52/53/54/54/56/57/6...</td>\n",
       "      <td>4/4/4/2/2/2/2/4/2/2/2/1/1/1/1/4/2/2/3/3/3/3/3/...</td>\n",
       "      <td>0/35/39/28/26/41/0/0/52/31/27/30/21/22/27/40/3...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>nia+206@rowan.kr</td>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>1.34375</td>\n",
       "      <td>249</td>\n",
       "      <td>2351</td>\n",
       "      <td>...</td>\n",
       "      <td>4411</td>\n",
       "      <td>1</td>\n",
       "      <td>780</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/1/1/1/1/2/3/2/2/2/1/1/1/1/1/1/1/1/...</td>\n",
       "      <td>0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....</td>\n",
       "      <td>68/66/67/67/68/69/69/70/71/71/71/69/72/70/70/7...</td>\n",
       "      <td>4/4/2/2/2/1/1/1/1/1/1/2/2/3/3/2/2/2/2/2/3/4/2/...</td>\n",
       "      <td>26/16/19/18/16/18/19/17/15/16/15/13/11/13/13/1...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>nia+206@rowan.kr</td>\n",
       "      <td>2020-12-26</td>\n",
       "      <td>1.53125</td>\n",
       "      <td>570</td>\n",
       "      <td>2682</td>\n",
       "      <td>...</td>\n",
       "      <td>11057</td>\n",
       "      <td>1</td>\n",
       "      <td>518</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.41</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/2/2/2/2/2/2/1/1/1/1/1/1/1/1/1/1/1/...</td>\n",
       "      <td>0.9/0.9/0.9/0.9/1.1/0.9/0.9/1.1/0.9/0.9/0.9/0....</td>\n",
       "      <td>0/0/65/65/66/67/69/72/73/72/73/74/73/74/75/75/...</td>\n",
       "      <td>4/4/2/2/2/2/2/2/2/2/2/2/4/4/4/4/2/3/3/3/2/2/4/...</td>\n",
       "      <td>0/0/13/12/12/13/12/12/12/16/13/12/13/17/13/18/...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>nia+206@rowan.kr</td>\n",
       "      <td>2020-12-27</td>\n",
       "      <td>1.34375</td>\n",
       "      <td>295</td>\n",
       "      <td>2331</td>\n",
       "      <td>...</td>\n",
       "      <td>5135</td>\n",
       "      <td>4</td>\n",
       "      <td>578</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/2/3/2/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/...</td>\n",
       "      <td>1.1/0.9/0.9/0.9/1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/...</td>\n",
       "      <td>0/0/0/64/64/64/64/65/65/66/66/66/66/68/68/68/6...</td>\n",
       "      <td>4/4/4/4/2/2/2/2/2/2/2/2/2/3/3/3/3/2/4/4/2/2/2/...</td>\n",
       "      <td>0/0/0/14/17/18/14/14/17/15/17/12/17/12/13/12/1...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>nia+206@rowan.kr</td>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>1.31250</td>\n",
       "      <td>186</td>\n",
       "      <td>2280</td>\n",
       "      <td>...</td>\n",
       "      <td>3178</td>\n",
       "      <td>0</td>\n",
       "      <td>778</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/1/1/1/1/1/1/2/2/0/0/0/0/0/0/0/0/0/...</td>\n",
       "      <td>0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....</td>\n",
       "      <td>0/60/60/61/60/60/60/62/64/64/64/64/65/68/70/70...</td>\n",
       "      <td>4/4/1/2/2/2/2/1/2/2/2/1/1/1/3/3/1/1/2/2/3/3/3/...</td>\n",
       "      <td>0/14/15/14/15/16/17/16/16/18/18/20/23/15/21/20...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 EMAIL summary_date  activity_average_met  \\\n",
       "0     nia+404@rowan.kr   2020-11-27               1.71875   \n",
       "1     nia+404@rowan.kr   2020-11-28               1.40625   \n",
       "2     nia+404@rowan.kr   2020-11-29               1.46875   \n",
       "3     nia+404@rowan.kr   2020-11-30               0.34375   \n",
       "4     nia+404@rowan.kr   2020-12-01               1.46875   \n",
       "...                ...          ...                   ...   \n",
       "9322  nia+206@rowan.kr   2020-12-22               1.34375   \n",
       "9323  nia+206@rowan.kr   2020-12-24               1.34375   \n",
       "9324  nia+206@rowan.kr   2020-12-26               1.53125   \n",
       "9325  nia+206@rowan.kr   2020-12-27               1.34375   \n",
       "9326  nia+206@rowan.kr   2020-12-28               1.31250   \n",
       "\n",
       "      activity_cal_active  activity_cal_total activity_class_5min  \\\n",
       "0                     730                2944                 ...   \n",
       "1                     342                2449                 ...   \n",
       "2                     401                2544                 ...   \n",
       "3                      27                1850                 ...   \n",
       "4                     333                2518                 ...   \n",
       "...                   ...                 ...                 ...   \n",
       "9322                  227                2316                 ...   \n",
       "9323                  249                2351                 ...   \n",
       "9324                  570                2682                 ...   \n",
       "9325                  295                2331                 ...   \n",
       "9326                  186                2280                 ...   \n",
       "\n",
       "      activity_daily_movement  activity_high  activity_inactive  \\\n",
       "0                       14346              0                417   \n",
       "1                        6352              0                473   \n",
       "2                        7297              0                586   \n",
       "3                         491              0                176   \n",
       "4                        5861              0                646   \n",
       "...                       ...            ...                ...   \n",
       "9322                     3863              3                735   \n",
       "9323                     4411              1                780   \n",
       "9324                    11057              1                518   \n",
       "9325                     5135              4                578   \n",
       "9326                     3178              0                778   \n",
       "\n",
       "      activity_inactivity_alerts  ...  sleep_temperature_deviation  \\\n",
       "0                              0  ...                        -0.12   \n",
       "1                              0  ...                        -0.32   \n",
       "2                              0  ...                         0.07   \n",
       "3                              0  ...                        -0.41   \n",
       "4                              0  ...                        -0.27   \n",
       "...                          ...  ...                          ...   \n",
       "9322                           2  ...                        -0.16   \n",
       "9323                           4  ...                        -0.09   \n",
       "9324                           1  ...                         0.41   \n",
       "9325                           0  ...                         0.27   \n",
       "9326                           1  ...                        -0.05   \n",
       "\n",
       "      sleep_temperature_trend_deviation timezone  sleep_total  \\\n",
       "0                                 99.99      NaN           \\r   \n",
       "1                                 99.99      NaN           \\r   \n",
       "2                                 99.99      NaN           \\r   \n",
       "3                                 99.99      NaN           \\r   \n",
       "4                                 99.99      NaN           \\r   \n",
       "...                                 ...      ...          ...   \n",
       "9322                              99.99      NaN           \\r   \n",
       "9323                              99.99      NaN           \\r   \n",
       "9324                              99.99      NaN           \\r   \n",
       "9325                              99.99      NaN           \\r   \n",
       "9326                              99.99      NaN           \\r   \n",
       "\n",
       "                CONVERT(activity_class_5min USING utf8)  \\\n",
       "0     2/1/1/1/1/1/2/2/1/1/1/1/1/1/2/2/2/3/2/2/2/2/2/...   \n",
       "1     1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/2/2/2/2/2/2/...   \n",
       "2     1/1/1/1/1/1/1/2/1/1/1/1/2/2/2/2/2/1/1/1/1/1/2/...   \n",
       "3     2/1/2/2/1/2/1/1/2/1/1/1/1/1/2/1/1/1/1/1/2/2/2/...   \n",
       "4     1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/2/2/3/3/2/...   \n",
       "...                                                 ...   \n",
       "9322  2/1/1/2/2/1/1/1/1/2/2/3/2/1/1/1/1/1/1/1/1/1/1/...   \n",
       "9323  1/1/1/1/1/1/1/1/1/1/2/3/2/2/2/1/1/1/1/1/1/1/1/...   \n",
       "9324  1/1/1/1/1/1/2/2/2/2/2/2/1/1/1/1/1/1/1/1/1/1/1/...   \n",
       "9325  1/1/1/2/3/2/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/...   \n",
       "9326  1/1/1/1/1/1/1/1/1/1/1/1/2/2/0/0/0/0/0/0/0/0/0/...   \n",
       "\n",
       "                  CONVERT(activity_met_1min USING utf8)  \\\n",
       "0     0.9/0.9/1.4/1.9/1.1/0.9/0.9/1.1/1.3/1/0.9/1.1/...   \n",
       "1     1.2/1.1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....   \n",
       "2     1.1/1.1/1.2/1.1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....   \n",
       "3     0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/...   \n",
       "4     0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....   \n",
       "...                                                 ...   \n",
       "9322  1.1/1.4/1.2/0.9/1.2/1.1/0.9/0.9/0.9/1/0.9/0.9/...   \n",
       "9323  0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....   \n",
       "9324  0.9/0.9/0.9/0.9/1.1/0.9/0.9/1.1/0.9/0.9/0.9/0....   \n",
       "9325  1.1/0.9/0.9/0.9/1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/...   \n",
       "9326  0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....   \n",
       "\n",
       "                      CONVERT(sleep_hr_5min USING utf8)  \\\n",
       "0     0/73/73/73/72/71/70/71/71/71/70/70/73/72/74/74...   \n",
       "1     69/70/69/69/70/72/71/72/70/69/69/69/68/68/63/6...   \n",
       "2     0/74/73/73/74/74/74/71/71/70/70/69/70/68/66/69...   \n",
       "3     73/70/71/72/75/75/73/70/70/70/67/63/63/63/63/6...   \n",
       "4     0/0/0/0/0/0/0/0/69/69/71/69/65/66/64/64/65/66/...   \n",
       "...                                                 ...   \n",
       "9322  0/54/54/54/55/56/0/0/55/52/52/53/54/54/56/57/6...   \n",
       "9323  68/66/67/67/68/69/69/70/71/71/71/69/72/70/70/7...   \n",
       "9324  0/0/65/65/66/67/69/72/73/72/73/74/73/74/75/75/...   \n",
       "9325  0/0/0/64/64/64/64/65/65/66/66/66/66/68/68/68/6...   \n",
       "9326  0/60/60/61/60/60/60/62/64/64/64/64/65/68/70/70...   \n",
       "\n",
       "               CONVERT(sleep_hypnogram_5min USING utf8)  \\\n",
       "0     4/2/4/3/3/1/2/2/2/2/2/2/3/3/3/4/4/3/2/2/2/2/2/...   \n",
       "1     2/4/2/2/2/2/3/1/2/2/4/4/2/2/2/2/2/2/2/2/2/2/4/...   \n",
       "2     4/2/4/4/1/1/1/4/4/4/4/4/4/4/2/3/4/2/2/4/2/2/2/...   \n",
       "3     4/4/4/4/3/3/3/2/4/4/4/2/2/2/2/2/2/2/2/4/2/2/2/...   \n",
       "4     4/4/4/4/4/4/4/4/4/4/4/2/2/2/2/3/3/2/4/4/4/2/2/...   \n",
       "...                                                 ...   \n",
       "9322  4/4/4/2/2/2/2/4/2/2/2/1/1/1/1/4/2/2/3/3/3/3/3/...   \n",
       "9323  4/4/2/2/2/1/1/1/1/1/1/2/2/3/3/2/2/2/2/2/3/4/2/...   \n",
       "9324  4/4/2/2/2/2/2/2/2/2/2/2/4/4/4/4/2/3/3/3/2/2/4/...   \n",
       "9325  4/4/4/4/2/2/2/2/2/2/2/2/2/3/3/3/3/2/4/4/2/2/2/...   \n",
       "9326  4/4/1/2/2/2/2/1/2/2/2/1/1/1/3/3/1/1/2/2/3/3/3/...   \n",
       "\n",
       "                   CONVERT(sleep_rmssd_5min USING utf8)  DIAG_NM  \n",
       "0     0/10/10/10/11/11/10/12/18/13/14/12/10/10/18/17...       CN  \n",
       "1     23/23/26/24/18/13/15/14/17/20/24/30/23/25/22/1...       CN  \n",
       "2     0/11/14/20/13/14/14/16/27/29/27/20/19/19/14/12...       CN  \n",
       "3     24/28/19/17/12/10/17/20/23/23/25/31/26/25/34/3...       CN  \n",
       "4     0/0/0/0/0/0/0/0/21/22/26/23/19/29/22/17/14/13/...       CN  \n",
       "...                                                 ...      ...  \n",
       "9322  0/35/39/28/26/41/0/0/52/31/27/30/21/22/27/40/3...       CN  \n",
       "9323  26/16/19/18/16/18/19/17/15/16/15/13/11/13/13/1...       CN  \n",
       "9324  0/0/13/12/12/13/12/12/12/16/13/12/13/17/13/18/...       CN  \n",
       "9325  0/0/0/14/17/18/14/14/17/15/17/12/17/12/13/12/1...       CN  \n",
       "9326  0/14/15/14/15/16/17/16/16/18/18/20/23/15/21/20...       CN  \n",
       "\n",
       "[9327 rows x 66 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"input/\"\n",
    "\n",
    "# original data : data \n",
    "data =  pd.read_csv(path + \"demential_data.csv\", parse_dates=['summary_date']) # 애초에 datatime 유형의 데이터를 읽어 올 때부터 형변환하기\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c72751",
   "metadata": {},
   "source": [
    "# 2. EDA  \n",
    "\n",
    "**9,327 rows × 66 columns**\n",
    "\n",
    "148명에 대한 데일리 라이프로그 데이터  \n",
    "크게 **1. 기본 정보 2. 걸음거리 3.수면 4.컨버팅 데이터**로 나눌 수 있겠습니다. \n",
    "\n",
    "**4. converting**\n",
    "- CONVERT(activity_class_5min USING utf8)    \n",
    "- CONVERT(activity_met_1min USING utf8)      \n",
    "- CONVERT(sleep_hr_5min USING utf8)          \n",
    "- CONVERT(sleep_hypnogram_5min USING utf8)  \n",
    "- CONVERT(sleep_rmssd_5min USING utf8) \n",
    "\n",
    "**y variable**\n",
    "- DIAG_NM \n",
    "    - 정상(CN), 경도인지 장애(MCI), 치매(Dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b8f4ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9327 entries, 0 to 9326\n",
      "Data columns (total 66 columns):\n",
      " #   Column                                    Non-Null Count  Dtype         \n",
      "---  ------                                    --------------  -----         \n",
      " 0   EMAIL                                     9327 non-null   object        \n",
      " 1   summary_date                              9327 non-null   datetime64[ns]\n",
      " 2   activity_average_met                      9327 non-null   float64       \n",
      " 3   activity_cal_active                       9327 non-null   int64         \n",
      " 4   activity_cal_total                        9327 non-null   int64         \n",
      " 5   activity_class_5min                       9327 non-null   object        \n",
      " 6   activity_daily_movement                   9327 non-null   int64         \n",
      " 7   activity_high                             9327 non-null   int64         \n",
      " 8   activity_inactive                         9327 non-null   int64         \n",
      " 9   activity_inactivity_alerts                9327 non-null   int64         \n",
      " 10  activity_low                              9327 non-null   int64         \n",
      " 11  activity_medium                           9327 non-null   int64         \n",
      " 12  activity_met_1min                         9327 non-null   object        \n",
      " 13  activity_met_min_high                     9327 non-null   int64         \n",
      " 14  activity_met_min_inactive                 9327 non-null   int64         \n",
      " 15  activity_met_min_low                      9327 non-null   int64         \n",
      " 16  activity_met_min_medium                   9327 non-null   int64         \n",
      " 17  activity_non_wear                         9327 non-null   int64         \n",
      " 18  activity_rest                             9327 non-null   int64         \n",
      " 19  activity_score                            9327 non-null   int64         \n",
      " 20  activity_score_meet_daily_targets         9327 non-null   int64         \n",
      " 21  activity_score_move_every_hour            9327 non-null   int64         \n",
      " 22  activity_score_recovery_time              9327 non-null   int64         \n",
      " 23  activity_score_stay_active                9327 non-null   int64         \n",
      " 24  activity_score_training_frequency         9327 non-null   int64         \n",
      " 25  activity_score_training_volume            9327 non-null   int64         \n",
      " 26  activity_steps                            9327 non-null   int64         \n",
      " 27  activity_total                            9327 non-null   int64         \n",
      " 28  sleep_awake                               9327 non-null   int64         \n",
      " 29  sleep_breath_average                      9327 non-null   float64       \n",
      " 30  sleep_deep                                9327 non-null   int64         \n",
      " 31  sleep_duration                            9327 non-null   int64         \n",
      " 32  sleep_efficiency                          9327 non-null   int64         \n",
      " 33  sleep_hr_5min                             9327 non-null   object        \n",
      " 34  sleep_hr_average                          9327 non-null   float64       \n",
      " 35  sleep_hr_lowest                           9327 non-null   int64         \n",
      " 36  sleep_hypnogram_5min                      9327 non-null   object        \n",
      " 37  sleep_is_longest                          9327 non-null   int64         \n",
      " 38  sleep_light                               9327 non-null   int64         \n",
      " 39  sleep_midpoint_at_delta                   9327 non-null   int64         \n",
      " 40  sleep_midpoint_time                       9327 non-null   int64         \n",
      " 41  sleep_onset_latency                       9327 non-null   int64         \n",
      " 42  sleep_period_id                           9327 non-null   int64         \n",
      " 43  sleep_rem                                 9327 non-null   int64         \n",
      " 44  sleep_restless                            9327 non-null   int64         \n",
      " 45  sleep_rmssd                               9327 non-null   int64         \n",
      " 46  sleep_rmssd_5min                          9327 non-null   object        \n",
      " 47  sleep_score                               9327 non-null   int64         \n",
      " 48  sleep_score_alignment                     9327 non-null   int64         \n",
      " 49  sleep_score_deep                          9327 non-null   int64         \n",
      " 50  sleep_score_disturbances                  9327 non-null   int64         \n",
      " 51  sleep_score_efficiency                    9327 non-null   int64         \n",
      " 52  sleep_score_latency                       9327 non-null   int64         \n",
      " 53  sleep_score_rem                           9327 non-null   int64         \n",
      " 54  sleep_score_total                         9327 non-null   int64         \n",
      " 55  sleep_temperature_delta                   9327 non-null   float64       \n",
      " 56  sleep_temperature_deviation               9327 non-null   float64       \n",
      " 57  sleep_temperature_trend_deviation         9327 non-null   float64       \n",
      " 58  timezone                                  0 non-null      float64       \n",
      " 59  sleep_total                               9327 non-null   object        \n",
      " 60  CONVERT(activity_class_5min USING utf8)   9327 non-null   object        \n",
      " 61  CONVERT(activity_met_1min USING utf8)     9327 non-null   object        \n",
      " 62  CONVERT(sleep_hr_5min USING utf8)         9327 non-null   object        \n",
      " 63  CONVERT(sleep_hypnogram_5min USING utf8)  9327 non-null   object        \n",
      " 64  CONVERT(sleep_rmssd_5min USING utf8)      9327 non-null   object        \n",
      " 65  DIAG_NM                                   9327 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(7), int64(45), object(13)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4808801",
   "metadata": {},
   "source": [
    "# 3. Preprocessing \n",
    "\n",
    "**9,327 rows × 66 columns**\n",
    "\n",
    "**9,327 rows × 5 columns**\n",
    "\n",
    "**===================Default 하게 진행======================**\n",
    "\n",
    "RNN, LSTM 모델에서는 converting features 사용할 예정  \n",
    "각각의 converting data가 가지는 길이가 다르기 때문에 sig1, sig2, sig3... sig5 data로 나누어 사용할 것  \n",
    "\n",
    "**feature processing**\n",
    "- astype : float \n",
    "- sig1_data, sig2_data, sig3_data, sig4_data, sig5_data \n",
    "    - CONVERT(activity_class_5min USING utf8)    \n",
    "    - CONVERT(activity_met_1min USING utf8)      \n",
    "    - CONVERT(sleep_hr_5min USING utf8)          \n",
    "    - CONVERT(sleep_hypnogram_5min USING utf8)  \n",
    "    - CONVERT(sleep_rmssd_5min USING utf8) \n",
    "\n",
    "**=========================Y Varible=========================**\n",
    "\n",
    "- DIAG_NM \n",
    "    - 정상(CN), 경도인지 장애(MCI), 치매(Dem)\n",
    "    - Lable Encoding \n",
    "    - 모델마다 y 변수를 자동으로 label encoding 기능을 지원하기도 하지만 공통적으로 진행하고 넘어갑시다\n",
    "    \n",
    "So we may use 5 columns for training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8110629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data를 따로 관리합니다 \n",
    "# original data -> data \n",
    "# preprocessing data -> processed_data\n",
    "processed_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf93fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection - column drop \n",
    "# 사용할 column만 잘라냅니다\n",
    "processed_data = processed_data.iloc[:, 60:66]\n",
    "# 9327 x 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a9bbe7",
   "metadata": {},
   "source": [
    "processed_data -> 5개의 sig1_data, sig2_data, sig3_data, sig4_data, sig5_data 으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b4b35b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  278  279  280  \\\n",
       "0     2.0  1.0  1.0  1.0  1.0  1.0  2.0  2.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
       "1     1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
       "2     1.0  1.0  1.0  1.0  1.0  1.0  1.0  2.0  1.0  1.0  ...  1.0  2.0  2.0   \n",
       "3     2.0  1.0  2.0  2.0  1.0  2.0  1.0  1.0  2.0  1.0  ...  1.0  1.0  1.0   \n",
       "4     1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9322  2.0  1.0  1.0  2.0  2.0  1.0  1.0  1.0  1.0  2.0  ...  1.0  1.0  1.0   \n",
       "9323  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  2.0  1.0  1.0   \n",
       "9324  1.0  1.0  1.0  1.0  1.0  1.0  2.0  2.0  2.0  2.0  ...  1.0  1.0  1.0   \n",
       "9325  1.0  1.0  1.0  2.0  3.0  2.0  1.0  1.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
       "9326  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
       "\n",
       "      281  282  283  284  285  286  287  \n",
       "0     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "1     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "2     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "3     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "4     2.0  2.0  2.0  2.0  1.0  1.0  1.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "9322  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "9323  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "9324  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "9325  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "9326  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "\n",
       "[9327 rows x 288 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signal 01\n",
    "signal_list = list(processed_data['CONVERT(activity_class_5min USING utf8)'].str.split('/'))\n",
    "sig1_data = pd.DataFrame(signal_list).drop(288, axis=1).astype('float')\n",
    "\n",
    "sig1_data # 9327 x 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e193737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1430</th>\n",
       "      <th>1431</th>\n",
       "      <th>1432</th>\n",
       "      <th>1433</th>\n",
       "      <th>1434</th>\n",
       "      <th>1435</th>\n",
       "      <th>1436</th>\n",
       "      <th>1437</th>\n",
       "      <th>1438</th>\n",
       "      <th>1439</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 1440 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...  1430  \\\n",
       "0      0.9   0.9   1.4   1.9   1.1   0.9   0.9   1.1   1.3   1.0  ...   0.9   \n",
       "1      1.2   1.1   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  ...   1.0   \n",
       "2      1.1   1.1   1.2   1.1   0.9   0.9   0.9   0.9   0.9   0.9  ...   0.9   \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4      0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  ...   0.9   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "9322   1.1   1.4   1.2   0.9   1.2   1.1   0.9   0.9   0.9   1.0  ...   0.9   \n",
       "9323   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  ...   0.9   \n",
       "9324   0.9   0.9   0.9   0.9   1.1   0.9   0.9   1.1   0.9   0.9  ...   0.9   \n",
       "9325   1.1   0.9   0.9   0.9   1.0   0.9   0.9   0.9   0.9   0.9  ...   0.9   \n",
       "9326   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  ...   0.9   \n",
       "\n",
       "      1431  1432  1433  1434  1435  1436  1437  1438  1439  \n",
       "0      0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "1      0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "2      0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4      0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "9322   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "9323   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "9324   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "9325   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "9326   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "\n",
       "[9327 rows x 1440 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signal 02\n",
    "signal_list = list(processed_data['CONVERT(activity_met_1min USING utf8)'].str.split('/'))\n",
    "sig2_data = pd.DataFrame(signal_list).drop(1440, axis=1).astype('float')\n",
    "\n",
    "sig2_data # 9327 x 1440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29928e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>68.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9    ...  171  \\\n",
       "0      0.0  73.0  73.0  73.0  72.0  71.0  70.0  71.0  71.0  71.0  ...  0.0   \n",
       "1     69.0  70.0  69.0  69.0  70.0  72.0  71.0  72.0  70.0  69.0  ...  0.0   \n",
       "2      0.0  74.0  73.0  73.0  74.0  74.0  74.0  71.0  71.0  70.0  ...  0.0   \n",
       "3     73.0  70.0  71.0  72.0  75.0  75.0  73.0  70.0  70.0  70.0  ...  0.0   \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  69.0  69.0  ...  0.0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...  ...   \n",
       "9322   0.0  54.0  54.0  54.0  55.0  56.0   0.0   0.0  55.0  52.0  ...  0.0   \n",
       "9323  68.0  66.0  67.0  67.0  68.0  69.0  69.0  70.0  71.0  71.0  ...  0.0   \n",
       "9324   0.0   0.0  65.0  65.0  66.0  67.0  69.0  72.0  73.0  72.0  ...  0.0   \n",
       "9325   0.0   0.0   0.0  64.0  64.0  64.0  64.0  65.0  65.0  66.0  ...  0.0   \n",
       "9326   0.0  60.0  60.0  61.0  60.0  60.0  60.0  62.0  64.0  64.0  ...  0.0   \n",
       "\n",
       "      172  173  174  175  176  177  178  179  180  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "9322  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9323  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9324  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9325  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9326  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[9327 rows x 181 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signal 03\n",
    "signal_list = list(processed_data['CONVERT(sleep_hr_5min USING utf8)'].str.split('/'))\n",
    "sig3_data = pd.DataFrame(signal_list).drop(181, axis=1).transpose() # 결측치 처리를 위한 transpose \n",
    "\n",
    "# 결측치 처리 Nan, '', ' ' -> 0으로 replace \n",
    "sig3_data = sig3_data.fillna(0) \n",
    "sig3_data = sig3_data.replace('', 0)\n",
    "sig3_data = sig3_data.replace(' ', 0)\n",
    "sig3_data = sig3_data.astype('float').transpose() # 되돌리기 \n",
    "\n",
    "sig3_data # 9327 x 181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffa76205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  170  171  172  \\\n",
       "0     4.0  2.0  4.0  3.0  3.0  1.0  2.0  2.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "1     2.0  4.0  2.0  2.0  2.0  2.0  3.0  1.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "2     4.0  2.0  4.0  4.0  1.0  1.0  1.0  4.0  4.0  4.0  ...  0.0  0.0  0.0   \n",
       "3     4.0  4.0  4.0  4.0  3.0  3.0  3.0  2.0  4.0  4.0  ...  0.0  0.0  0.0   \n",
       "4     4.0  4.0  4.0  4.0  4.0  4.0  4.0  4.0  4.0  4.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9322  4.0  4.0  4.0  2.0  2.0  2.0  2.0  4.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "9323  4.0  4.0  2.0  2.0  2.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0   \n",
       "9324  4.0  4.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "9325  4.0  4.0  4.0  4.0  2.0  2.0  2.0  2.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "9326  4.0  4.0  1.0  2.0  2.0  2.0  2.0  1.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      173  174  175  176  177  178  179  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "9322  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9323  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9324  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9325  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9326  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[9327 rows x 180 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signal 04\n",
    "signal_list = list(processed_data['CONVERT(sleep_hypnogram_5min USING utf8)'].str.split('/'))\n",
    "sig4_data = pd.DataFrame(signal_list).drop(180, axis=1).transpose() # 결측치 처리를 위한 transpose \n",
    "\n",
    "# 결측치 처리 Nan, '', ' ' -> 0으로 replace \n",
    "sig4_data = sig4_data.fillna(0) \n",
    "sig4_data = sig4_data.replace('', 0)\n",
    "sig4_data = sig4_data.replace(' ', 0)\n",
    "sig4_data = sig4_data.astype('float').transpose() # 되돌리기 \n",
    "\n",
    "sig4_data # 9327 x 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470df2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>26.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9    ...  171  \\\n",
       "0      0.0  10.0  10.0  10.0  11.0  11.0  10.0  12.0  18.0  13.0  ...  0.0   \n",
       "1     23.0  23.0  26.0  24.0  18.0  13.0  15.0  14.0  17.0  20.0  ...  0.0   \n",
       "2      0.0  11.0  14.0  20.0  13.0  14.0  14.0  16.0  27.0  29.0  ...  0.0   \n",
       "3     24.0  28.0  19.0  17.0  12.0  10.0  17.0  20.0  23.0  23.0  ...  0.0   \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  21.0  22.0  ...  0.0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...  ...   \n",
       "9322   0.0  35.0  39.0  28.0  26.0  41.0   0.0   0.0  52.0  31.0  ...  0.0   \n",
       "9323  26.0  16.0  19.0  18.0  16.0  18.0  19.0  17.0  15.0  16.0  ...  0.0   \n",
       "9324   0.0   0.0  13.0  12.0  12.0  13.0  12.0  12.0  12.0  16.0  ...  0.0   \n",
       "9325   0.0   0.0   0.0  14.0  17.0  18.0  14.0  14.0  17.0  15.0  ...  0.0   \n",
       "9326   0.0  14.0  15.0  14.0  15.0  16.0  17.0  16.0  16.0  18.0  ...  0.0   \n",
       "\n",
       "      172  173  174  175  176  177  178  179  180  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "9322  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9323  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9324  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9325  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9326  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[9327 rows x 181 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signal 05\n",
    "signal_list = list(processed_data['CONVERT(sleep_rmssd_5min USING utf8)'].str.split('/'))\n",
    "sig5_data = pd.DataFrame(signal_list).drop(181, axis=1).transpose() # 결측치 처리를 위한 transpose \n",
    "\n",
    "# 결측치 처리 Nan, '', ' ' -> 0으로 replace \n",
    "sig5_data = sig5_data.fillna(0) \n",
    "sig5_data = sig5_data.replace('', 0)\n",
    "sig5_data = sig5_data.replace(' ', 0)\n",
    "sig5_data = sig5_data.astype('float').transpose() # 되돌리기 \n",
    "\n",
    "sig5_data # 9327 x 181"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520471a",
   "metadata": {},
   "source": [
    "# 4. Data Split + y Variable Processing \n",
    "\n",
    "모든 전처리와 y variable labeling이 완료되었다.  \n",
    "train / valid / test data로 분할하고 용도에 맞게 model을 돌리도록 하자.\n",
    "\n",
    "- **3d - array 처리** \n",
    "    - Sequential model 의 input layer에는 ndarray 데이터만 들어가야 함\n",
    "- **y 변수 두 가지로 encoding** \n",
    "    - Label Encoding (le) -> sparse_categorical_crossentropy 실험 \n",
    "    - One Hot Encoding (ohe) -> categorical_crossentropy 실험 \n",
    "- **train / test** \n",
    "    - Sequential model option 중 validation dataset을 자동으로 나눠주는 것이 존재 하므로 \n",
    "\n",
    "\n",
    "- 즉 다음과 같은 조합이 하나의 dataset 이며  \n",
    "    - input_data_X1 + input_data_y_le + input_data_y_ohe\n",
    "    - 실험은 한 모델당 두 가지 y 변수로 2번 실험하면 됨\n",
    "    - **inpu_data_X1_le + input_data_y_le**\n",
    "    - **inpu_data_X1_ohe + input_data_y_ohe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831f2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# data를 따로 관리\n",
    "# preprocessing data -> processed_data\n",
    "# input data -> input_data => y 변수와 X 변수 분할 관리 \n",
    "input_data_X1 = sig1_data.copy()\n",
    "input_data_X2 = sig2_data.copy()\n",
    "input_data_X3 = sig3_data.copy()\n",
    "input_data_X4 = sig4_data.copy()\n",
    "input_data_X5 = sig5_data.copy()\n",
    "\n",
    "input_data_y = processed_data['DIAG_NM'] .copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c15d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d-array 처리 - X variables \n",
    "input_data_X1 =  input_data_X1.to_numpy().reshape(input_data_X1.shape[0], input_data_X1.shape[1], 1)\n",
    "input_data_X2 =  input_data_X2.to_numpy().reshape(input_data_X2.shape[0], input_data_X2.shape[1], 1)\n",
    "input_data_X3 =  input_data_X3.to_numpy().reshape(input_data_X3.shape[0], input_data_X3.shape[1], 1)\n",
    "input_data_X4 =  input_data_X4.to_numpy().reshape(input_data_X4.shape[0], input_data_X4.shape[1], 1)\n",
    "input_data_X5 =  input_data_X5.to_numpy().reshape(input_data_X5.shape[0], input_data_X5.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f97b7439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original gain_label \n",
      " ['CN' 'Dem' 'MCI']\n",
      "gain_lable label \n",
      " {0, 1, 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LabelEncoder - y variable \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "input_data_y_label = le.fit_transform(list(input_data_y))\n",
    "\n",
    "print(\"original gain_label \\n\", le.classes_)\n",
    "print(\"gain_lable label \\n\", set(input_data_y_label))\n",
    "\n",
    "input_data_y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7662a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[array(['CN', 'Dem', 'MCI'], dtype=object)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CN</th>\n",
       "      <th>Dem</th>\n",
       "      <th>MCI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CN  Dem  MCI\n",
       "0     1.0  0.0  0.0\n",
       "1     1.0  0.0  0.0\n",
       "2     1.0  0.0  0.0\n",
       "3     1.0  0.0  0.0\n",
       "4     1.0  0.0  0.0\n",
       "...   ...  ...  ...\n",
       "9322  1.0  0.0  0.0\n",
       "9323  1.0  0.0  0.0\n",
       "9324  1.0  0.0  0.0\n",
       "9325  1.0  0.0  0.0\n",
       "9326  1.0  0.0  0.0\n",
       "\n",
       "[9327 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Hot Encdoer - y variable \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "input_data_y.shape\n",
    "input_data_y.values.reshape(-1,1).shape\n",
    "ohe.fit(input_data_y.values.reshape(-1,1))\n",
    "one_hot_encoded = ohe.transform(input_data_y.values.reshape(-1,1))\n",
    "\n",
    "print(type(ohe.categories_))\n",
    "print(ohe.categories_)\n",
    "\n",
    "input_data_y_ohe = pd.DataFrame(one_hot_encoded, columns=ohe.categories_[0])\n",
    "input_data_y_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed348655",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train_label, X1_test_label, y1_train_label, y1_test_label = train_test_split(\n",
    "    input_data_X1, input_data_y_label, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X2_train_label, X2_test_label, y2_train_label, y2_test_label = train_test_split(\n",
    "    input_data_X2, input_data_y_label, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X3_train_label, X3_test_label, y3_train_label, y3_test_label = train_test_split(\n",
    "    input_data_X3, input_data_y_label, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X4_train_label, X4_test_label, y4_train_label, y4_test_label = train_test_split(\n",
    "    input_data_X4, input_data_y_label, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X5_train_label, X5_test_label, y5_train_label, y5_test_label = train_test_split(\n",
    "    input_data_X5, input_data_y_label, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac7943f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train_ohe, X1_test_ohe, y1_train_ohe, y1_test_ohe = train_test_split(\n",
    "    input_data_X1, input_data_y_ohe, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X2_train_ohe, X2_test_ohe, y2_train_ohe, y2_test_ohe = train_test_split(\n",
    "    input_data_X2, input_data_y_ohe, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X3_train_ohe, X3_test_ohe, y3_train_ohe, y3_test_ohe = train_test_split(\n",
    "    input_data_X3, input_data_y_ohe, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X4_train_ohe, X4_test_ohe, y4_train_ohe, y4_test_ohe = train_test_split(\n",
    "    input_data_X4, input_data_y_ohe, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X5_train_ohe, X5_test_ohe, y5_train_ohe, y5_test_ohe = train_test_split(\n",
    "    input_data_X5, input_data_y_ohe, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be2558ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1_train_label : (7461, 288, 1)\n",
      "X1_train_ohe : (7461, 288, 1)\n",
      "y1_train_label : (7461,)\n",
      "y1_train_ohe : (7461, 3)\n",
      "X1_test_label : (1866, 288, 1)\n",
      "X1_test_ohe : (1866, 288, 1)\n",
      "y1_test_label : (1866,)\n",
      "y1_test_ohe : (1866, 3)\n"
     ]
    }
   ],
   "source": [
    "# 데이터가 잘 분할되었을까요?\n",
    "# 잘 분할되었습니다 \n",
    "# 7461 + 1866 = 9327\n",
    "print(\"X1_train_label :\" ,X1_train_label.shape)\n",
    "print(\"X1_train_ohe :\" ,X1_train_ohe.shape)\n",
    "print(\"y1_train_label :\" ,y1_train_label.shape)\n",
    "print(\"y1_train_ohe :\" ,y1_train_ohe.shape)\n",
    "print(\"X1_test_label :\" ,X1_test_label.shape)\n",
    "print(\"X1_test_ohe :\" ,X1_test_ohe.shape)\n",
    "print(\"y1_test_label :\" ,y1_test_label.shape)\n",
    "print(\"y1_test_ohe :\" ,y1_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e012e",
   "metadata": {},
   "source": [
    "# 5. Modeling - DL \n",
    "\n",
    "- **simple RNN** \n",
    "    - ohe **0.638**\n",
    "    - le **0.639**\n",
    "- **multi-layer RNN**\n",
    "    - ohe 0.637\n",
    "    - le 0.637\n",
    "- **simple LSTM**\n",
    "    - ohe \n",
    "    - le \n",
    "- **multi-layer LSTM**\n",
    "    - ohe \n",
    "    - le "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab57c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Activation, SimpleRNN\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "128be75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "units = 50\n",
    "timesteps = 1\n",
    "inputs = (X3_train_ohe.shape[1],X3_train_ohe.shape[2])\n",
    "outputs = 3\n",
    "epochs = 1000\n",
    "batch_size = 256\n",
    "# early_stop 하기 위해 관찰 값 monitor\n",
    "# 관찰 값이 loss 이므로 최소화 시키는 방향으로 training이 진행 (default 깂 Auto)\n",
    "# patience : 성능이 증가하지 않더라도 epoch을 얼마나 더 진행해볼 것인가?\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode = 'min'\n",
    "                               , patience=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517759bd",
   "metadata": {},
   "source": [
    "### simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "937825c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_rnn_ohe():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units, input_shape = inputs, return_sequences = False)) \n",
    "    model.add(Dense(outputs)) #target 개수 = 3\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    #y를 one-hot encoding 시 'categorical_crossentropy 사용\n",
    "    #y를 label encoding 시 'sparse_categorical_crossentropy' 사용\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = adam, metrics = ['accuracy']) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a58086c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5968 samples, validate on 1493 samples\n",
      "Epoch 1/1000\n",
      "5968/5968 [==============================] - 1s 140us/sample - loss: 0.8827 - acc: 0.6160 - val_loss: 0.8314 - val_acc: 0.6450\n",
      "Epoch 2/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8314 - acc: 0.6376 - val_loss: 0.8273 - val_acc: 0.6450\n",
      "Epoch 3/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8263 - acc: 0.6376 - val_loss: 0.8286 - val_acc: 0.6450\n",
      "Epoch 4/1000\n",
      "5968/5968 [==============================] - 1s 121us/sample - loss: 0.8268 - acc: 0.6384 - val_loss: 0.8273 - val_acc: 0.6457\n",
      "Epoch 5/1000\n",
      "5968/5968 [==============================] - 1s 121us/sample - loss: 0.8252 - acc: 0.6389 - val_loss: 0.8259 - val_acc: 0.6457\n",
      "Epoch 6/1000\n",
      "5968/5968 [==============================] - 1s 120us/sample - loss: 0.8353 - acc: 0.6344 - val_loss: 0.8274 - val_acc: 0.6443\n",
      "Epoch 7/1000\n",
      "5968/5968 [==============================] - 1s 120us/sample - loss: 0.8258 - acc: 0.6389 - val_loss: 0.8276 - val_acc: 0.6443\n",
      "Epoch 8/1000\n",
      "5968/5968 [==============================] - 1s 119us/sample - loss: 0.8258 - acc: 0.6392 - val_loss: 0.8267 - val_acc: 0.6457\n",
      "Epoch 9/1000\n",
      "5968/5968 [==============================] - 1s 119us/sample - loss: 0.8265 - acc: 0.6392 - val_loss: 0.8266 - val_acc: 0.6457\n",
      "Epoch 10/1000\n",
      "5968/5968 [==============================] - 1s 121us/sample - loss: 0.8245 - acc: 0.6392 - val_loss: 0.8265 - val_acc: 0.6457\n",
      "Epoch 11/1000\n",
      "5968/5968 [==============================] - 1s 119us/sample - loss: 0.8244 - acc: 0.6396 - val_loss: 0.8265 - val_acc: 0.6457\n",
      "Epoch 12/1000\n",
      "5968/5968 [==============================] - 1s 119us/sample - loss: 0.8235 - acc: 0.6394 - val_loss: 0.8282 - val_acc: 0.6457\n",
      "Epoch 13/1000\n",
      "5968/5968 [==============================] - 1s 119us/sample - loss: 0.8230 - acc: 0.6396 - val_loss: 0.8262 - val_acc: 0.6457\n",
      "Epoch 14/1000\n",
      "5968/5968 [==============================] - 1s 119us/sample - loss: 0.8220 - acc: 0.6392 - val_loss: 0.8280 - val_acc: 0.6457\n",
      "Epoch 15/1000\n",
      "5968/5968 [==============================] - 1s 120us/sample - loss: 0.8242 - acc: 0.6392 - val_loss: 0.8251 - val_acc: 0.6457\n",
      "Epoch 16/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8228 - acc: 0.6392 - val_loss: 0.8267 - val_acc: 0.6457\n",
      "Epoch 17/1000\n",
      "5968/5968 [==============================] - 1s 120us/sample - loss: 0.8229 - acc: 0.6392 - val_loss: 0.8247 - val_acc: 0.6457\n",
      "Epoch 18/1000\n",
      "5968/5968 [==============================] - 1s 119us/sample - loss: 0.8213 - acc: 0.6396 - val_loss: 0.8235 - val_acc: 0.6457\n",
      "Epoch 19/1000\n",
      "5968/5968 [==============================] - 1s 120us/sample - loss: 0.8210 - acc: 0.6374 - val_loss: 0.9010 - val_acc: 0.6450\n",
      "Epoch 20/1000\n",
      "5968/5968 [==============================] - 1s 121us/sample - loss: 0.8287 - acc: 0.6392 - val_loss: 0.8250 - val_acc: 0.6457\n",
      "Epoch 21/1000\n",
      "5968/5968 [==============================] - 1s 120us/sample - loss: 0.8228 - acc: 0.6392 - val_loss: 0.8188 - val_acc: 0.6457\n",
      "Epoch 22/1000\n",
      "5968/5968 [==============================] - 1s 120us/sample - loss: 0.8261 - acc: 0.6377 - val_loss: 0.8284 - val_acc: 0.6457\n",
      "Epoch 23/1000\n",
      "5968/5968 [==============================] - 1s 119us/sample - loss: 0.8251 - acc: 0.6394 - val_loss: 0.8275 - val_acc: 0.6457\n",
      "Epoch 24/1000\n",
      "5968/5968 [==============================] - 1s 120us/sample - loss: 0.8258 - acc: 0.6392 - val_loss: 0.8322 - val_acc: 0.6457\n",
      "Epoch 25/1000\n",
      "5968/5968 [==============================] - 1s 120us/sample - loss: 0.8270 - acc: 0.6392 - val_loss: 0.8305 - val_acc: 0.6457\n",
      "Epoch 26/1000\n",
      "5968/5968 [==============================] - 1s 120us/sample - loss: 0.8257 - acc: 0.6392 - val_loss: 0.8269 - val_acc: 0.6457\n",
      "Epoch 27/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8258 - acc: 0.6392 - val_loss: 0.8275 - val_acc: 0.6457\n",
      "Epoch 28/1000\n",
      "5968/5968 [==============================] - 1s 120us/sample - loss: 0.8245 - acc: 0.6391 - val_loss: 0.8271 - val_acc: 0.6457\n",
      "Epoch 29/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8249 - acc: 0.6391 - val_loss: 0.8283 - val_acc: 0.6457\n",
      "Epoch 30/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8243 - acc: 0.6392 - val_loss: 0.8268 - val_acc: 0.6457\n",
      "Epoch 31/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8254 - acc: 0.6392 - val_loss: 0.8265 - val_acc: 0.6457\n",
      "Epoch 32/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8245 - acc: 0.6392 - val_loss: 0.8265 - val_acc: 0.6457\n",
      "Epoch 33/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8243 - acc: 0.6392 - val_loss: 0.8271 - val_acc: 0.6457\n",
      "Epoch 34/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8236 - acc: 0.6396 - val_loss: 0.8287 - val_acc: 0.6457\n",
      "Epoch 35/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8241 - acc: 0.6394 - val_loss: 0.8265 - val_acc: 0.6457\n",
      "Epoch 36/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8236 - acc: 0.6394 - val_loss: 0.8274 - val_acc: 0.6457\n",
      "Epoch 37/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8236 - acc: 0.6399 - val_loss: 0.8267 - val_acc: 0.6457\n",
      "Epoch 38/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8239 - acc: 0.6399 - val_loss: 0.8265 - val_acc: 0.6457\n",
      "Epoch 39/1000\n",
      "5968/5968 [==============================] - 1s 131us/sample - loss: 0.8238 - acc: 0.6397 - val_loss: 0.8264 - val_acc: 0.6457\n",
      "Epoch 40/1000\n",
      "5968/5968 [==============================] - 1s 133us/sample - loss: 0.8233 - acc: 0.6396 - val_loss: 0.8282 - val_acc: 0.6470\n",
      "Epoch 41/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8235 - acc: 0.6402 - val_loss: 0.8258 - val_acc: 0.6463\n",
      "Epoch 42/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8236 - acc: 0.6399 - val_loss: 0.8252 - val_acc: 0.6463\n",
      "Epoch 43/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8228 - acc: 0.6401 - val_loss: 0.8261 - val_acc: 0.6463\n",
      "Epoch 44/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8231 - acc: 0.6399 - val_loss: 0.8243 - val_acc: 0.6463\n",
      "Epoch 45/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8228 - acc: 0.6402 - val_loss: 0.8243 - val_acc: 0.6463\n",
      "Epoch 46/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8232 - acc: 0.6401 - val_loss: 0.8264 - val_acc: 0.6463\n",
      "Epoch 47/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8228 - acc: 0.6399 - val_loss: 0.8246 - val_acc: 0.6463\n",
      "Epoch 48/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8230 - acc: 0.6401 - val_loss: 0.8240 - val_acc: 0.6463\n",
      "Epoch 49/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8237 - acc: 0.6402 - val_loss: 0.8233 - val_acc: 0.6463\n",
      "Epoch 50/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8209 - acc: 0.6401 - val_loss: 0.8248 - val_acc: 0.6463\n",
      "Epoch 51/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8228 - acc: 0.6402 - val_loss: 0.8236 - val_acc: 0.6463\n",
      "Epoch 52/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8218 - acc: 0.6404 - val_loss: 0.8207 - val_acc: 0.6463\n",
      "Epoch 53/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8208 - acc: 0.6399 - val_loss: 0.8242 - val_acc: 0.6457\n",
      "Epoch 54/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8220 - acc: 0.6402 - val_loss: 0.8205 - val_acc: 0.6450\n",
      "Epoch 55/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8219 - acc: 0.6399 - val_loss: 0.8215 - val_acc: 0.6457\n",
      "Epoch 56/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8249 - acc: 0.6394 - val_loss: 0.8276 - val_acc: 0.6457\n",
      "Epoch 57/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8247 - acc: 0.6396 - val_loss: 0.8265 - val_acc: 0.6457\n",
      "Epoch 58/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8246 - acc: 0.6389 - val_loss: 0.8312 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8259 - acc: 0.6396 - val_loss: 0.8268 - val_acc: 0.6457\n",
      "Epoch 60/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8242 - acc: 0.6396 - val_loss: 0.8267 - val_acc: 0.6457\n",
      "Epoch 61/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8248 - acc: 0.6396 - val_loss: 0.8269 - val_acc: 0.6457\n",
      "Epoch 62/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8245 - acc: 0.6396 - val_loss: 0.8266 - val_acc: 0.6457\n",
      "Epoch 63/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8243 - acc: 0.6397 - val_loss: 0.8265 - val_acc: 0.6457\n",
      "Epoch 64/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8251 - acc: 0.6397 - val_loss: 0.8278 - val_acc: 0.6457\n",
      "Epoch 65/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8243 - acc: 0.6396 - val_loss: 0.8264 - val_acc: 0.6457\n",
      "Epoch 66/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8242 - acc: 0.6396 - val_loss: 0.8258 - val_acc: 0.6457\n",
      "Epoch 67/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8257 - acc: 0.6397 - val_loss: 0.8254 - val_acc: 0.6457\n",
      "Epoch 68/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8234 - acc: 0.6396 - val_loss: 0.8256 - val_acc: 0.6457\n",
      "Epoch 69/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8234 - acc: 0.6397 - val_loss: 0.8249 - val_acc: 0.6463\n",
      "Epoch 70/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8230 - acc: 0.6399 - val_loss: 0.8238 - val_acc: 0.6450\n",
      "Epoch 71/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8775 - acc: 0.6154 - val_loss: 0.8276 - val_acc: 0.6450\n",
      "Epoch 72/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8263 - acc: 0.6389 - val_loss: 0.8282 - val_acc: 0.6450\n",
      "Epoch 73/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8263 - acc: 0.6389 - val_loss: 0.8290 - val_acc: 0.6450\n",
      "Epoch 74/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8262 - acc: 0.6389 - val_loss: 0.8277 - val_acc: 0.6450\n",
      "Epoch 75/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8260 - acc: 0.6389 - val_loss: 0.8309 - val_acc: 0.6450\n",
      "Epoch 76/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8259 - acc: 0.6389 - val_loss: 0.8275 - val_acc: 0.6450\n",
      "Epoch 77/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8255 - acc: 0.6389 - val_loss: 0.8278 - val_acc: 0.6450\n",
      "Epoch 78/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8253 - acc: 0.6389 - val_loss: 0.8294 - val_acc: 0.6450\n",
      "Epoch 79/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8257 - acc: 0.6389 - val_loss: 0.8276 - val_acc: 0.6450\n",
      "Epoch 80/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8251 - acc: 0.6389 - val_loss: 0.8272 - val_acc: 0.6450\n",
      "Epoch 81/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8249 - acc: 0.6389 - val_loss: 0.8271 - val_acc: 0.6450\n",
      "Epoch 82/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8249 - acc: 0.6389 - val_loss: 0.8284 - val_acc: 0.6450\n",
      "Epoch 83/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8244 - acc: 0.6387 - val_loss: 0.8269 - val_acc: 0.6450\n",
      "Epoch 84/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8252 - acc: 0.6389 - val_loss: 0.8276 - val_acc: 0.6450\n",
      "Epoch 85/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8254 - acc: 0.6389 - val_loss: 0.8269 - val_acc: 0.6457\n",
      "Epoch 86/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8245 - acc: 0.6389 - val_loss: 0.8275 - val_acc: 0.6450\n",
      "Epoch 87/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8240 - acc: 0.6387 - val_loss: 0.8282 - val_acc: 0.6457\n",
      "Epoch 88/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8244 - acc: 0.6394 - val_loss: 0.8273 - val_acc: 0.6457\n",
      "Epoch 89/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8242 - acc: 0.6392 - val_loss: 0.8270 - val_acc: 0.6457\n",
      "Epoch 90/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8250 - acc: 0.6397 - val_loss: 0.8291 - val_acc: 0.6457\n",
      "Epoch 91/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8241 - acc: 0.6396 - val_loss: 0.8272 - val_acc: 0.6457\n",
      "Epoch 92/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8239 - acc: 0.6394 - val_loss: 0.8270 - val_acc: 0.6457\n",
      "Epoch 93/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8239 - acc: 0.6392 - val_loss: 0.8273 - val_acc: 0.6457\n",
      "Epoch 94/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8241 - acc: 0.6394 - val_loss: 0.8271 - val_acc: 0.6457\n",
      "Epoch 95/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8242 - acc: 0.6392 - val_loss: 0.8274 - val_acc: 0.6457\n",
      "Epoch 96/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8240 - acc: 0.6396 - val_loss: 0.8287 - val_acc: 0.6457\n",
      "Epoch 97/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8244 - acc: 0.6394 - val_loss: 0.8283 - val_acc: 0.6457\n",
      "Epoch 98/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8238 - acc: 0.6392 - val_loss: 0.8268 - val_acc: 0.6457\n",
      "Epoch 99/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8235 - acc: 0.6394 - val_loss: 0.8268 - val_acc: 0.6457\n",
      "Epoch 100/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8237 - acc: 0.6394 - val_loss: 0.8268 - val_acc: 0.6457\n",
      "Epoch 101/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8236 - acc: 0.6396 - val_loss: 0.8265 - val_acc: 0.6457\n",
      "Epoch 102/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8235 - acc: 0.6396 - val_loss: 0.8274 - val_acc: 0.6457\n",
      "Epoch 103/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8238 - acc: 0.6392 - val_loss: 0.8267 - val_acc: 0.6457\n",
      "Epoch 104/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8232 - acc: 0.6399 - val_loss: 0.8265 - val_acc: 0.6457\n",
      "Epoch 105/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8231 - acc: 0.6396 - val_loss: 0.8261 - val_acc: 0.6457\n",
      "Epoch 106/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8236 - acc: 0.6396 - val_loss: 0.8261 - val_acc: 0.6457\n",
      "Epoch 107/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8233 - acc: 0.6397 - val_loss: 0.8276 - val_acc: 0.6457\n",
      "Epoch 108/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8235 - acc: 0.6396 - val_loss: 0.8261 - val_acc: 0.6457\n",
      "Epoch 109/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8229 - acc: 0.6397 - val_loss: 0.8259 - val_acc: 0.6457\n",
      "Epoch 110/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8227 - acc: 0.6396 - val_loss: 0.8258 - val_acc: 0.6457\n",
      "Epoch 111/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8229 - acc: 0.6396 - val_loss: 0.8257 - val_acc: 0.6457\n",
      "Epoch 112/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8217 - acc: 0.6396 - val_loss: 0.8246 - val_acc: 0.6457\n",
      "Epoch 113/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8217 - acc: 0.6397 - val_loss: 0.8257 - val_acc: 0.6463\n",
      "Epoch 114/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8207 - acc: 0.6401 - val_loss: 0.8244 - val_acc: 0.6463\n",
      "Epoch 115/1000\n",
      "5968/5968 [==============================] - 1s 123us/sample - loss: 0.8203 - acc: 0.6399 - val_loss: 0.8219 - val_acc: 0.6463\n",
      "Epoch 116/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8198 - acc: 0.6399 - val_loss: 0.8250 - val_acc: 0.6463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8219 - acc: 0.6401 - val_loss: 0.8246 - val_acc: 0.6463\n",
      "Epoch 118/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8206 - acc: 0.6399 - val_loss: 0.8254 - val_acc: 0.6457\n",
      "Epoch 119/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8226 - acc: 0.6396 - val_loss: 0.8249 - val_acc: 0.6457\n",
      "Epoch 120/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8225 - acc: 0.6396 - val_loss: 0.8268 - val_acc: 0.6463\n",
      "Epoch 121/1000\n",
      "5968/5968 [==============================] - 1s 122us/sample - loss: 0.8222 - acc: 0.6397 - val_loss: 0.8241 - val_acc: 0.6463\n",
      "Epoch 121: early stopping\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding으로 fit\n",
    "signle_rnn_model_ohe = KerasClassifier(build_fn = vanilla_rnn_ohe\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])\n",
    "hist = signle_rnn_model_ohe.fit(X3_train_ohe, y3_train_ohe)\n",
    "y3_pred_rnn_ohe = signle_rnn_model_ohe.predict(X3_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd39133a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN accouracy :  0.6377277599142551\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN accouracy : \", accuracy_score(y3_pred_rnn_ohe,y3_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "570eb80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'], label='loss')\n",
    "plt.plot(hist.history['val_loss'], label='valid loss')\n",
    "plt.xticks(range(len(hist.history['loss'])))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('precision')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cead010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_rnn_label():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units, input_shape = inputs, return_sequences = False)) \n",
    "    model.add(Dense(outputs)) #target 개수 = 3\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    #y를 one-hot encoding 시 'categorical_crossentropy 사용\n",
    "    #y를 label encoding 시 'sparse_categorical_crossentropy' 사용\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy']) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33f27afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding으로 fit\n",
    "signle_rnn_model_label = KerasClassifier(build_fn = vanilla_rnn_label\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea387569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5968 samples, validate on 1493 samples\n",
      "Epoch 1/1000\n",
      "5968/5968 [==============================] - 1s 144us/sample - loss: 0.8602 - acc: 0.6233 - val_loss: 0.8322 - val_acc: 0.6463\n",
      "Epoch 2/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8246 - acc: 0.6389 - val_loss: 0.8247 - val_acc: 0.6463\n",
      "Epoch 3/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8285 - acc: 0.6359 - val_loss: 0.8229 - val_acc: 0.6470\n",
      "Epoch 4/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8218 - acc: 0.6394 - val_loss: 0.8183 - val_acc: 0.6470\n",
      "Epoch 5/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8235 - acc: 0.6377 - val_loss: 0.8228 - val_acc: 0.6470\n",
      "Epoch 6/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8204 - acc: 0.6397 - val_loss: 0.8627 - val_acc: 0.6470\n",
      "Epoch 7/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8270 - acc: 0.6396 - val_loss: 0.8266 - val_acc: 0.6463\n",
      "Epoch 8/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8223 - acc: 0.6392 - val_loss: 0.8260 - val_acc: 0.6463\n",
      "Epoch 9/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8220 - acc: 0.6397 - val_loss: 0.8243 - val_acc: 0.6470\n",
      "Epoch 10/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8208 - acc: 0.6396 - val_loss: 0.8299 - val_acc: 0.6463\n",
      "Epoch 11/1000\n",
      "5968/5968 [==============================] - 1s 132us/sample - loss: 0.9239 - acc: 0.5851 - val_loss: 0.8386 - val_acc: 0.6457\n",
      "Epoch 12/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8293 - acc: 0.6386 - val_loss: 0.8260 - val_acc: 0.6457\n",
      "Epoch 13/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8237 - acc: 0.6384 - val_loss: 0.8233 - val_acc: 0.6457\n",
      "Epoch 14/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8227 - acc: 0.6386 - val_loss: 0.8235 - val_acc: 0.6457\n",
      "Epoch 15/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8232 - acc: 0.6386 - val_loss: 0.8228 - val_acc: 0.6457\n",
      "Epoch 16/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8222 - acc: 0.6386 - val_loss: 0.8225 - val_acc: 0.6457\n",
      "Epoch 17/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8217 - acc: 0.6386 - val_loss: 0.8212 - val_acc: 0.6457\n",
      "Epoch 18/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8210 - acc: 0.6386 - val_loss: 0.8194 - val_acc: 0.6457\n",
      "Epoch 19/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8199 - acc: 0.6386 - val_loss: 0.8193 - val_acc: 0.6457\n",
      "Epoch 20/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8203 - acc: 0.6386 - val_loss: 0.8211 - val_acc: 0.6457\n",
      "Epoch 21/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8191 - acc: 0.6386 - val_loss: 0.8160 - val_acc: 0.6457\n",
      "Epoch 22/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8177 - acc: 0.6384 - val_loss: 0.8137 - val_acc: 0.6457\n",
      "Epoch 23/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8193 - acc: 0.6386 - val_loss: 0.8155 - val_acc: 0.6457\n",
      "Epoch 24/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8158 - acc: 0.6386 - val_loss: 0.8115 - val_acc: 0.6450\n",
      "Epoch 25/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8171 - acc: 0.6387 - val_loss: 0.8131 - val_acc: 0.6457\n",
      "Epoch 26/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8191 - acc: 0.6387 - val_loss: 0.8196 - val_acc: 0.6463\n",
      "Epoch 27/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8179 - acc: 0.6392 - val_loss: 0.8158 - val_acc: 0.6463\n",
      "Epoch 28/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8159 - acc: 0.6389 - val_loss: 0.8170 - val_acc: 0.6463\n",
      "Epoch 29/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8139 - acc: 0.6391 - val_loss: 0.8108 - val_acc: 0.6463\n",
      "Epoch 30/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8127 - acc: 0.6392 - val_loss: 0.8064 - val_acc: 0.6470\n",
      "Epoch 31/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8116 - acc: 0.6389 - val_loss: 0.8075 - val_acc: 0.6470\n",
      "Epoch 32/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8136 - acc: 0.6394 - val_loss: 0.8138 - val_acc: 0.6457\n",
      "Epoch 33/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8168 - acc: 0.6391 - val_loss: 0.8184 - val_acc: 0.6463\n",
      "Epoch 34/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8141 - acc: 0.6391 - val_loss: 0.8082 - val_acc: 0.6457\n",
      "Epoch 35/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8121 - acc: 0.6389 - val_loss: 0.8083 - val_acc: 0.6457\n",
      "Epoch 36/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8124 - acc: 0.6394 - val_loss: 0.8074 - val_acc: 0.6470\n",
      "Epoch 37/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8111 - acc: 0.6396 - val_loss: 0.8044 - val_acc: 0.6463\n",
      "Epoch 38/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8096 - acc: 0.6397 - val_loss: 0.8064 - val_acc: 0.6463\n",
      "Epoch 39/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8096 - acc: 0.6399 - val_loss: 0.8033 - val_acc: 0.6463\n",
      "Epoch 40/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8097 - acc: 0.6392 - val_loss: 0.8056 - val_acc: 0.6463\n",
      "Epoch 41/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8101 - acc: 0.6402 - val_loss: 0.8003 - val_acc: 0.6470\n",
      "Epoch 42/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8108 - acc: 0.6394 - val_loss: 0.8090 - val_acc: 0.6470\n",
      "Epoch 43/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8121 - acc: 0.6401 - val_loss: 0.8181 - val_acc: 0.6450\n",
      "Epoch 44/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8132 - acc: 0.6401 - val_loss: 0.8055 - val_acc: 0.6457\n",
      "Epoch 45/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8116 - acc: 0.6394 - val_loss: 0.8014 - val_acc: 0.6457\n",
      "Epoch 46/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8106 - acc: 0.6397 - val_loss: 0.8046 - val_acc: 0.6443\n",
      "Epoch 47/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8096 - acc: 0.6402 - val_loss: 0.7979 - val_acc: 0.6457\n",
      "Epoch 48/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8086 - acc: 0.6406 - val_loss: 0.8092 - val_acc: 0.6463\n",
      "Epoch 49/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8101 - acc: 0.6408 - val_loss: 0.8040 - val_acc: 0.6457\n",
      "Epoch 50/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8088 - acc: 0.6404 - val_loss: 0.7987 - val_acc: 0.6463\n",
      "Epoch 51/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8078 - acc: 0.6402 - val_loss: 0.8038 - val_acc: 0.6457\n",
      "Epoch 52/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8079 - acc: 0.6404 - val_loss: 0.7996 - val_acc: 0.6463\n",
      "Epoch 53/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8091 - acc: 0.6408 - val_loss: 0.8011 - val_acc: 0.6463\n",
      "Epoch 54/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8070 - acc: 0.6406 - val_loss: 0.8006 - val_acc: 0.6463\n",
      "Epoch 55/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8071 - acc: 0.6406 - val_loss: 0.8006 - val_acc: 0.6463\n",
      "Epoch 56/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8083 - acc: 0.6408 - val_loss: 0.8037 - val_acc: 0.6463\n",
      "Epoch 57/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8096 - acc: 0.6406 - val_loss: 0.8031 - val_acc: 0.6463\n",
      "Epoch 58/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8085 - acc: 0.6406 - val_loss: 0.8028 - val_acc: 0.6463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8078 - acc: 0.6402 - val_loss: 0.8001 - val_acc: 0.6457\n",
      "Epoch 60/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8074 - acc: 0.6397 - val_loss: 0.8001 - val_acc: 0.6463\n",
      "Epoch 61/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8078 - acc: 0.6402 - val_loss: 0.8016 - val_acc: 0.6463\n",
      "Epoch 62/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8078 - acc: 0.6408 - val_loss: 0.7984 - val_acc: 0.6463\n",
      "Epoch 63/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8072 - acc: 0.6406 - val_loss: 0.8029 - val_acc: 0.6463\n",
      "Epoch 64/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8089 - acc: 0.6406 - val_loss: 0.7981 - val_acc: 0.6463\n",
      "Epoch 65/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8075 - acc: 0.6396 - val_loss: 0.8008 - val_acc: 0.6463\n",
      "Epoch 66/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8071 - acc: 0.6406 - val_loss: 0.8030 - val_acc: 0.6463\n",
      "Epoch 67/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8081 - acc: 0.6399 - val_loss: 0.8004 - val_acc: 0.6457\n",
      "Epoch 68/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8082 - acc: 0.6399 - val_loss: 0.7993 - val_acc: 0.6450\n",
      "Epoch 69/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8078 - acc: 0.6392 - val_loss: 0.7997 - val_acc: 0.6463\n",
      "Epoch 70/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8075 - acc: 0.6401 - val_loss: 0.7977 - val_acc: 0.6463\n",
      "Epoch 71/1000\n",
      "5968/5968 [==============================] - 1s 125us/sample - loss: 0.8074 - acc: 0.6404 - val_loss: 0.7971 - val_acc: 0.6463\n",
      "Epoch 72/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8077 - acc: 0.6402 - val_loss: 0.8018 - val_acc: 0.6463\n",
      "Epoch 73/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8076 - acc: 0.6406 - val_loss: 0.7991 - val_acc: 0.6463\n",
      "Epoch 74/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8076 - acc: 0.6402 - val_loss: 0.7977 - val_acc: 0.6463\n",
      "Epoch 75/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8082 - acc: 0.6402 - val_loss: 0.7985 - val_acc: 0.6463\n",
      "Epoch 76/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8060 - acc: 0.6401 - val_loss: 0.8004 - val_acc: 0.6463\n",
      "Epoch 77/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8059 - acc: 0.6404 - val_loss: 0.7995 - val_acc: 0.6463\n",
      "Epoch 78/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8066 - acc: 0.6402 - val_loss: 0.7990 - val_acc: 0.6463\n",
      "Epoch 79/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8067 - acc: 0.6402 - val_loss: 0.8034 - val_acc: 0.6457\n",
      "Epoch 80/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8065 - acc: 0.6406 - val_loss: 0.7999 - val_acc: 0.6457\n",
      "Epoch 81/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8058 - acc: 0.6409 - val_loss: 0.8000 - val_acc: 0.6457\n",
      "Epoch 82/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8067 - acc: 0.6408 - val_loss: 0.7984 - val_acc: 0.6457\n",
      "Epoch 83/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8073 - acc: 0.6408 - val_loss: 0.8071 - val_acc: 0.6457\n",
      "Epoch 84/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8079 - acc: 0.6406 - val_loss: 0.7994 - val_acc: 0.6463\n",
      "Epoch 85/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8058 - acc: 0.6408 - val_loss: 0.8028 - val_acc: 0.6463\n",
      "Epoch 86/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8069 - acc: 0.6409 - val_loss: 0.7964 - val_acc: 0.6457\n",
      "Epoch 87/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8064 - acc: 0.6408 - val_loss: 0.7978 - val_acc: 0.6457\n",
      "Epoch 88/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8074 - acc: 0.6411 - val_loss: 0.8028 - val_acc: 0.6457\n",
      "Epoch 89/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8067 - acc: 0.6409 - val_loss: 0.7972 - val_acc: 0.6457\n",
      "Epoch 90/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8054 - acc: 0.6409 - val_loss: 0.7965 - val_acc: 0.6457\n",
      "Epoch 91/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8051 - acc: 0.6406 - val_loss: 0.7998 - val_acc: 0.6457\n",
      "Epoch 92/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8048 - acc: 0.6409 - val_loss: 0.7979 - val_acc: 0.6457\n",
      "Epoch 93/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8055 - acc: 0.6404 - val_loss: 0.7976 - val_acc: 0.6457\n",
      "Epoch 94/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8056 - acc: 0.6406 - val_loss: 0.7993 - val_acc: 0.6457\n",
      "Epoch 95/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8064 - acc: 0.6413 - val_loss: 0.7993 - val_acc: 0.6457\n",
      "Epoch 96/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8059 - acc: 0.6408 - val_loss: 0.8020 - val_acc: 0.6457\n",
      "Epoch 97/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8052 - acc: 0.6406 - val_loss: 0.8014 - val_acc: 0.6457\n",
      "Epoch 98/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8048 - acc: 0.6408 - val_loss: 0.7993 - val_acc: 0.6457\n",
      "Epoch 99/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8049 - acc: 0.6402 - val_loss: 0.7995 - val_acc: 0.6457\n",
      "Epoch 100/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8041 - acc: 0.6406 - val_loss: 0.8005 - val_acc: 0.6457\n",
      "Epoch 101/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8047 - acc: 0.6401 - val_loss: 0.8028 - val_acc: 0.6457\n",
      "Epoch 102/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8053 - acc: 0.6402 - val_loss: 0.8009 - val_acc: 0.6457\n",
      "Epoch 103/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8038 - acc: 0.6411 - val_loss: 0.7990 - val_acc: 0.6450\n",
      "Epoch 104/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8043 - acc: 0.6404 - val_loss: 0.7978 - val_acc: 0.6457\n",
      "Epoch 105/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8062 - acc: 0.6408 - val_loss: 0.7989 - val_acc: 0.6457\n",
      "Epoch 106/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8043 - acc: 0.6408 - val_loss: 0.8027 - val_acc: 0.6470\n",
      "Epoch 107/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8044 - acc: 0.6402 - val_loss: 0.8014 - val_acc: 0.6457\n",
      "Epoch 108/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8045 - acc: 0.6409 - val_loss: 0.8027 - val_acc: 0.6457\n",
      "Epoch 109/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8070 - acc: 0.6406 - val_loss: 0.7976 - val_acc: 0.6450\n",
      "Epoch 110/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8048 - acc: 0.6413 - val_loss: 0.7985 - val_acc: 0.6450\n",
      "Epoch 111/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8036 - acc: 0.6414 - val_loss: 0.7999 - val_acc: 0.6450\n",
      "Epoch 112/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8041 - acc: 0.6404 - val_loss: 0.7987 - val_acc: 0.6450\n",
      "Epoch 113/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8046 - acc: 0.6406 - val_loss: 0.8033 - val_acc: 0.6457\n",
      "Epoch 114/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8033 - acc: 0.6406 - val_loss: 0.8003 - val_acc: 0.6457\n",
      "Epoch 115/1000\n",
      "5968/5968 [==============================] - 1s 135us/sample - loss: 0.8050 - acc: 0.6413 - val_loss: 0.8026 - val_acc: 0.6470\n",
      "Epoch 116/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8076 - acc: 0.6414 - val_loss: 0.7997 - val_acc: 0.6450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8039 - acc: 0.6399 - val_loss: 0.7974 - val_acc: 0.6450\n",
      "Epoch 118/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8039 - acc: 0.6411 - val_loss: 0.7985 - val_acc: 0.6463\n",
      "Epoch 119/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8041 - acc: 0.6411 - val_loss: 0.7994 - val_acc: 0.6450\n",
      "Epoch 120/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8033 - acc: 0.6409 - val_loss: 0.8036 - val_acc: 0.6450\n",
      "Epoch 121/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8034 - acc: 0.6408 - val_loss: 0.7962 - val_acc: 0.6450\n",
      "Epoch 122/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8056 - acc: 0.6404 - val_loss: 0.7971 - val_acc: 0.6457\n",
      "Epoch 123/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8040 - acc: 0.6404 - val_loss: 0.7997 - val_acc: 0.6450\n",
      "Epoch 124/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8044 - acc: 0.6409 - val_loss: 0.7974 - val_acc: 0.6450\n",
      "Epoch 125/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8029 - acc: 0.6408 - val_loss: 0.7990 - val_acc: 0.6450\n",
      "Epoch 126/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8029 - acc: 0.6408 - val_loss: 0.7982 - val_acc: 0.6463\n",
      "Epoch 127/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8037 - acc: 0.6409 - val_loss: 0.8001 - val_acc: 0.6450\n",
      "Epoch 128/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8033 - acc: 0.6411 - val_loss: 0.8027 - val_acc: 0.6463\n",
      "Epoch 129/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8029 - acc: 0.6409 - val_loss: 0.7990 - val_acc: 0.6450\n",
      "Epoch 130/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8021 - acc: 0.6411 - val_loss: 0.8000 - val_acc: 0.6457\n",
      "Epoch 131/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8040 - acc: 0.6408 - val_loss: 0.7980 - val_acc: 0.6457\n",
      "Epoch 132/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8038 - acc: 0.6409 - val_loss: 0.7970 - val_acc: 0.6450\n",
      "Epoch 133/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8026 - acc: 0.6411 - val_loss: 0.7970 - val_acc: 0.6450\n",
      "Epoch 134/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8030 - acc: 0.6406 - val_loss: 0.7990 - val_acc: 0.6457\n",
      "Epoch 135/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8025 - acc: 0.6409 - val_loss: 0.7999 - val_acc: 0.6450\n",
      "Epoch 136/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8027 - acc: 0.6406 - val_loss: 0.7971 - val_acc: 0.6457\n",
      "Epoch 137/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8030 - acc: 0.6414 - val_loss: 0.7994 - val_acc: 0.6450\n",
      "Epoch 138/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8030 - acc: 0.6404 - val_loss: 0.8019 - val_acc: 0.6450\n",
      "Epoch 139/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8033 - acc: 0.6409 - val_loss: 0.7983 - val_acc: 0.6463\n",
      "Epoch 140/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8038 - acc: 0.6409 - val_loss: 0.7975 - val_acc: 0.6450\n",
      "Epoch 141/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8022 - acc: 0.6409 - val_loss: 0.7966 - val_acc: 0.6457\n",
      "Epoch 142/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8021 - acc: 0.6411 - val_loss: 0.7976 - val_acc: 0.6463\n",
      "Epoch 143/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8021 - acc: 0.6408 - val_loss: 0.8004 - val_acc: 0.6457\n",
      "Epoch 144/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8025 - acc: 0.6413 - val_loss: 0.7965 - val_acc: 0.6463\n",
      "Epoch 145/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8029 - acc: 0.6416 - val_loss: 0.7961 - val_acc: 0.6477\n",
      "Epoch 146/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8036 - acc: 0.6411 - val_loss: 0.7970 - val_acc: 0.6470\n",
      "Epoch 147/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8019 - acc: 0.6401 - val_loss: 0.7992 - val_acc: 0.6457\n",
      "Epoch 148/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8031 - acc: 0.6404 - val_loss: 0.7958 - val_acc: 0.6457\n",
      "Epoch 149/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8028 - acc: 0.6404 - val_loss: 0.7981 - val_acc: 0.6457\n",
      "Epoch 150/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8030 - acc: 0.6413 - val_loss: 0.7974 - val_acc: 0.6463\n",
      "Epoch 151/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8032 - acc: 0.6418 - val_loss: 0.7962 - val_acc: 0.6457\n",
      "Epoch 152/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8020 - acc: 0.6408 - val_loss: 0.8004 - val_acc: 0.6457\n",
      "Epoch 153/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8019 - acc: 0.6414 - val_loss: 0.7974 - val_acc: 0.6457\n",
      "Epoch 154/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8013 - acc: 0.6414 - val_loss: 0.7978 - val_acc: 0.6450\n",
      "Epoch 155/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8015 - acc: 0.6413 - val_loss: 0.8055 - val_acc: 0.6463\n",
      "Epoch 156/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8020 - acc: 0.6413 - val_loss: 0.7999 - val_acc: 0.6470\n",
      "Epoch 157/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8055 - acc: 0.6413 - val_loss: 0.7957 - val_acc: 0.6470\n",
      "Epoch 158/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8020 - acc: 0.6414 - val_loss: 0.8066 - val_acc: 0.6477\n",
      "Epoch 159/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8025 - acc: 0.6413 - val_loss: 0.7953 - val_acc: 0.6470\n",
      "Epoch 160/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8020 - acc: 0.6411 - val_loss: 0.7966 - val_acc: 0.6470\n",
      "Epoch 161/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8027 - acc: 0.6418 - val_loss: 0.7969 - val_acc: 0.6463\n",
      "Epoch 162/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8019 - acc: 0.6419 - val_loss: 0.7981 - val_acc: 0.6470\n",
      "Epoch 163/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8024 - acc: 0.6419 - val_loss: 0.7999 - val_acc: 0.6470\n",
      "Epoch 164/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8012 - acc: 0.6411 - val_loss: 0.7998 - val_acc: 0.6484\n",
      "Epoch 165/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8012 - acc: 0.6421 - val_loss: 0.7958 - val_acc: 0.6463\n",
      "Epoch 166/1000\n",
      "5968/5968 [==============================] - 1s 135us/sample - loss: 0.8010 - acc: 0.6411 - val_loss: 0.7945 - val_acc: 0.6443\n",
      "Epoch 167/1000\n",
      "5968/5968 [==============================] - 1s 137us/sample - loss: 0.8014 - acc: 0.6404 - val_loss: 0.7969 - val_acc: 0.6463\n",
      "Epoch 168/1000\n",
      "5968/5968 [==============================] - 1s 139us/sample - loss: 0.8000 - acc: 0.6414 - val_loss: 0.7990 - val_acc: 0.6457\n",
      "Epoch 169/1000\n",
      "5968/5968 [==============================] - 1s 131us/sample - loss: 0.8012 - acc: 0.6409 - val_loss: 0.8001 - val_acc: 0.6477\n",
      "Epoch 170/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8010 - acc: 0.6416 - val_loss: 0.7970 - val_acc: 0.6463\n",
      "Epoch 171/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8005 - acc: 0.6418 - val_loss: 0.8006 - val_acc: 0.6470\n",
      "Epoch 172/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.7995 - acc: 0.6411 - val_loss: 0.7952 - val_acc: 0.6457\n",
      "Epoch 173/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.7999 - acc: 0.6419 - val_loss: 0.7973 - val_acc: 0.6457\n",
      "Epoch 174/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8006 - acc: 0.6418 - val_loss: 0.7975 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8008 - acc: 0.6421 - val_loss: 0.7981 - val_acc: 0.6470\n",
      "Epoch 176/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8015 - acc: 0.6416 - val_loss: 0.7969 - val_acc: 0.6450\n",
      "Epoch 177/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8016 - acc: 0.6423 - val_loss: 0.7976 - val_acc: 0.6463\n",
      "Epoch 178/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.7994 - acc: 0.6413 - val_loss: 0.7950 - val_acc: 0.6457\n",
      "Epoch 179/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.7994 - acc: 0.6419 - val_loss: 0.8058 - val_acc: 0.6463\n",
      "Epoch 180/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8020 - acc: 0.6413 - val_loss: 0.7959 - val_acc: 0.6457\n",
      "Epoch 181/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.7988 - acc: 0.6414 - val_loss: 0.7964 - val_acc: 0.6463\n",
      "Epoch 182/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.7993 - acc: 0.6416 - val_loss: 0.7977 - val_acc: 0.6477\n",
      "Epoch 183/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.7987 - acc: 0.6421 - val_loss: 0.7973 - val_acc: 0.6457\n",
      "Epoch 184/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8001 - acc: 0.6416 - val_loss: 0.8004 - val_acc: 0.6463\n",
      "Epoch 185/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.7998 - acc: 0.6411 - val_loss: 0.7962 - val_acc: 0.6457\n",
      "Epoch 186/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8004 - acc: 0.6414 - val_loss: 0.7935 - val_acc: 0.6463\n",
      "Epoch 187/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8001 - acc: 0.6411 - val_loss: 0.7981 - val_acc: 0.6477\n",
      "Epoch 188/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.7982 - acc: 0.6421 - val_loss: 0.7965 - val_acc: 0.6470\n",
      "Epoch 189/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.7998 - acc: 0.6424 - val_loss: 0.7995 - val_acc: 0.6470\n",
      "Epoch 190/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.7975 - acc: 0.6411 - val_loss: 0.7969 - val_acc: 0.6463\n",
      "Epoch 191/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8010 - acc: 0.6406 - val_loss: 0.7985 - val_acc: 0.6463\n",
      "Epoch 192/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8010 - acc: 0.6419 - val_loss: 0.7971 - val_acc: 0.6484\n",
      "Epoch 193/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.7998 - acc: 0.6424 - val_loss: 0.8001 - val_acc: 0.6457\n",
      "Epoch 194/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8009 - acc: 0.6414 - val_loss: 0.7985 - val_acc: 0.6450\n",
      "Epoch 195/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.7988 - acc: 0.6414 - val_loss: 0.8089 - val_acc: 0.6470\n",
      "Epoch 196/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8025 - acc: 0.6413 - val_loss: 0.7998 - val_acc: 0.6470\n",
      "Epoch 197/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.7986 - acc: 0.6418 - val_loss: 0.7988 - val_acc: 0.6457\n",
      "Epoch 198/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.7975 - acc: 0.6416 - val_loss: 0.8015 - val_acc: 0.6457\n",
      "Epoch 199/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.7977 - acc: 0.6424 - val_loss: 0.7986 - val_acc: 0.6457\n",
      "Epoch 200/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.7986 - acc: 0.6421 - val_loss: 0.7999 - val_acc: 0.6470\n",
      "Epoch 201/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8006 - acc: 0.6418 - val_loss: 0.7965 - val_acc: 0.6477\n",
      "Epoch 202/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.7984 - acc: 0.6424 - val_loss: 0.8016 - val_acc: 0.6470\n",
      "Epoch 203/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.7998 - acc: 0.6408 - val_loss: 0.8145 - val_acc: 0.6457\n",
      "Epoch 204/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8104 - acc: 0.6414 - val_loss: 0.8032 - val_acc: 0.6457\n",
      "Epoch 205/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8392 - acc: 0.6416 - val_loss: 0.8497 - val_acc: 0.6457\n",
      "Epoch 206/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8320 - acc: 0.6394 - val_loss: 0.8263 - val_acc: 0.6457\n",
      "Epoch 207/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8263 - acc: 0.6394 - val_loss: 0.8238 - val_acc: 0.6457\n",
      "Epoch 208/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8250 - acc: 0.6394 - val_loss: 0.8263 - val_acc: 0.6450\n",
      "Epoch 209/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8257 - acc: 0.6396 - val_loss: 0.8249 - val_acc: 0.6450\n",
      "Epoch 210/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8264 - acc: 0.6392 - val_loss: 0.8236 - val_acc: 0.6457\n",
      "Epoch 211/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8231 - acc: 0.6394 - val_loss: 0.8247 - val_acc: 0.6457\n",
      "Epoch 212/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8243 - acc: 0.6394 - val_loss: 0.8193 - val_acc: 0.6457\n",
      "Epoch 213/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8222 - acc: 0.6391 - val_loss: 0.8225 - val_acc: 0.6463\n",
      "Epoch 214/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8227 - acc: 0.6392 - val_loss: 0.8212 - val_acc: 0.6470\n",
      "Epoch 215/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8172 - acc: 0.6391 - val_loss: 0.8381 - val_acc: 0.6463\n",
      "Epoch 216/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8240 - acc: 0.6394 - val_loss: 0.8512 - val_acc: 0.6289\n",
      "Epoch 217/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8319 - acc: 0.6337 - val_loss: 0.8351 - val_acc: 0.6477\n",
      "Epoch 218/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8246 - acc: 0.6399 - val_loss: 0.8251 - val_acc: 0.6457\n",
      "Epoch 219/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8212 - acc: 0.6392 - val_loss: 0.8239 - val_acc: 0.6457\n",
      "Epoch 220/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8200 - acc: 0.6399 - val_loss: 0.8179 - val_acc: 0.6477\n",
      "Epoch 221/1000\n",
      "5968/5968 [==============================] - 1s 133us/sample - loss: 0.8175 - acc: 0.6392 - val_loss: 0.8019 - val_acc: 0.6484\n",
      "Epoch 222/1000\n",
      "5968/5968 [==============================] - 1s 131us/sample - loss: 0.8124 - acc: 0.6386 - val_loss: 0.7992 - val_acc: 0.6457\n",
      "Epoch 223/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8105 - acc: 0.6404 - val_loss: 0.7999 - val_acc: 0.6450\n",
      "Epoch 224/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8089 - acc: 0.6389 - val_loss: 0.8019 - val_acc: 0.6450\n",
      "Epoch 225/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8096 - acc: 0.6397 - val_loss: 0.8042 - val_acc: 0.6457\n",
      "Epoch 226/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8116 - acc: 0.6399 - val_loss: 0.7966 - val_acc: 0.6450\n",
      "Epoch 227/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8086 - acc: 0.6384 - val_loss: 0.8004 - val_acc: 0.6457\n",
      "Epoch 228/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8082 - acc: 0.6401 - val_loss: 0.7972 - val_acc: 0.6450\n",
      "Epoch 229/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8080 - acc: 0.6404 - val_loss: 0.7963 - val_acc: 0.6457\n",
      "Epoch 230/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8069 - acc: 0.6402 - val_loss: 0.7964 - val_acc: 0.6450\n",
      "Epoch 231/1000\n",
      "5968/5968 [==============================] - 1s 132us/sample - loss: 0.8063 - acc: 0.6404 - val_loss: 0.7953 - val_acc: 0.6457\n",
      "Epoch 232/1000\n",
      "5968/5968 [==============================] - 1s 134us/sample - loss: 0.8057 - acc: 0.6402 - val_loss: 0.7955 - val_acc: 0.6450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/1000\n",
      "5968/5968 [==============================] - 1s 140us/sample - loss: 0.8067 - acc: 0.6406 - val_loss: 0.7961 - val_acc: 0.6457\n",
      "Epoch 234/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8064 - acc: 0.6402 - val_loss: 0.7950 - val_acc: 0.6450\n",
      "Epoch 235/1000\n",
      "5968/5968 [==============================] - 1s 139us/sample - loss: 0.8059 - acc: 0.6402 - val_loss: 0.7956 - val_acc: 0.6450\n",
      "Epoch 236/1000\n",
      "5968/5968 [==============================] - 1s 135us/sample - loss: 0.8066 - acc: 0.6409 - val_loss: 0.7946 - val_acc: 0.6457\n",
      "Epoch 237/1000\n",
      "5968/5968 [==============================] - 1s 142us/sample - loss: 0.8066 - acc: 0.6404 - val_loss: 0.7981 - val_acc: 0.6457\n",
      "Epoch 238/1000\n",
      "5968/5968 [==============================] - 1s 137us/sample - loss: 0.8078 - acc: 0.6404 - val_loss: 0.7965 - val_acc: 0.6457\n",
      "Epoch 239/1000\n",
      "5968/5968 [==============================] - 1s 141us/sample - loss: 0.8055 - acc: 0.6402 - val_loss: 0.7958 - val_acc: 0.6457\n",
      "Epoch 240/1000\n",
      "5968/5968 [==============================] - 1s 140us/sample - loss: 0.8068 - acc: 0.6419 - val_loss: 0.8016 - val_acc: 0.6463\n",
      "Epoch 241/1000\n",
      "5968/5968 [==============================] - 1s 135us/sample - loss: 0.8076 - acc: 0.6402 - val_loss: 0.7959 - val_acc: 0.6463\n",
      "Epoch 242/1000\n",
      "5968/5968 [==============================] - 1s 133us/sample - loss: 0.8071 - acc: 0.6392 - val_loss: 0.7997 - val_acc: 0.6457\n",
      "Epoch 243/1000\n",
      "5968/5968 [==============================] - 1s 139us/sample - loss: 0.8079 - acc: 0.6404 - val_loss: 0.7976 - val_acc: 0.6457\n",
      "Epoch 244/1000\n",
      "5968/5968 [==============================] - 1s 133us/sample - loss: 0.8057 - acc: 0.6414 - val_loss: 0.7951 - val_acc: 0.6470\n",
      "Epoch 245/1000\n",
      "5968/5968 [==============================] - 1s 134us/sample - loss: 0.8065 - acc: 0.6414 - val_loss: 0.7959 - val_acc: 0.6463\n",
      "Epoch 246/1000\n",
      "5968/5968 [==============================] - 1s 133us/sample - loss: 0.8057 - acc: 0.6409 - val_loss: 0.7977 - val_acc: 0.6477\n",
      "Epoch 247/1000\n",
      "5968/5968 [==============================] - 1s 132us/sample - loss: 0.8060 - acc: 0.6408 - val_loss: 0.7999 - val_acc: 0.6470\n",
      "Epoch 248/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8062 - acc: 0.6411 - val_loss: 0.7985 - val_acc: 0.6470\n",
      "Epoch 249/1000\n",
      "5968/5968 [==============================] - 1s 133us/sample - loss: 0.8061 - acc: 0.6413 - val_loss: 0.7963 - val_acc: 0.6457\n",
      "Epoch 250/1000\n",
      "5968/5968 [==============================] - 1s 133us/sample - loss: 0.8048 - acc: 0.6408 - val_loss: 0.7951 - val_acc: 0.6457\n",
      "Epoch 251/1000\n",
      "5968/5968 [==============================] - 1s 133us/sample - loss: 0.8052 - acc: 0.6414 - val_loss: 0.7951 - val_acc: 0.6470\n",
      "Epoch 252/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8042 - acc: 0.6416 - val_loss: 0.7951 - val_acc: 0.6470\n",
      "Epoch 253/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8048 - acc: 0.6406 - val_loss: 0.7954 - val_acc: 0.6450\n",
      "Epoch 254/1000\n",
      "5968/5968 [==============================] - 1s 132us/sample - loss: 0.8053 - acc: 0.6409 - val_loss: 0.7953 - val_acc: 0.6457\n",
      "Epoch 255/1000\n",
      "5968/5968 [==============================] - 1s 132us/sample - loss: 0.8057 - acc: 0.6413 - val_loss: 0.7950 - val_acc: 0.6457\n",
      "Epoch 256/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8043 - acc: 0.6404 - val_loss: 0.7984 - val_acc: 0.6450\n",
      "Epoch 257/1000\n",
      "5968/5968 [==============================] - 1s 126us/sample - loss: 0.8052 - acc: 0.6411 - val_loss: 0.7954 - val_acc: 0.6457\n",
      "Epoch 258/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8048 - acc: 0.6406 - val_loss: 0.7968 - val_acc: 0.6457\n",
      "Epoch 259/1000\n",
      "5968/5968 [==============================] - 1s 135us/sample - loss: 0.8049 - acc: 0.6413 - val_loss: 0.7947 - val_acc: 0.6477\n",
      "Epoch 260/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8057 - acc: 0.6404 - val_loss: 0.7996 - val_acc: 0.6450\n",
      "Epoch 261/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8044 - acc: 0.6408 - val_loss: 0.7965 - val_acc: 0.6457\n",
      "Epoch 262/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8038 - acc: 0.6413 - val_loss: 0.8011 - val_acc: 0.6470\n",
      "Epoch 263/1000\n",
      "5968/5968 [==============================] - 1s 124us/sample - loss: 0.8055 - acc: 0.6408 - val_loss: 0.8001 - val_acc: 0.6457\n",
      "Epoch 264/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8051 - acc: 0.6409 - val_loss: 0.7968 - val_acc: 0.6450\n",
      "Epoch 265/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8037 - acc: 0.6414 - val_loss: 0.7958 - val_acc: 0.6470\n",
      "Epoch 266/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8051 - acc: 0.6416 - val_loss: 0.7973 - val_acc: 0.6463\n",
      "Epoch 267/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8031 - acc: 0.6402 - val_loss: 0.7966 - val_acc: 0.6450\n",
      "Epoch 268/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8039 - acc: 0.6411 - val_loss: 0.7969 - val_acc: 0.6450\n",
      "Epoch 269/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8025 - acc: 0.6411 - val_loss: 0.7975 - val_acc: 0.6457\n",
      "Epoch 270/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8044 - acc: 0.6416 - val_loss: 0.7967 - val_acc: 0.6450\n",
      "Epoch 271/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8036 - acc: 0.6413 - val_loss: 0.7982 - val_acc: 0.6463\n",
      "Epoch 272/1000\n",
      "5968/5968 [==============================] - 1s 131us/sample - loss: 0.8035 - acc: 0.6411 - val_loss: 0.7954 - val_acc: 0.6470\n",
      "Epoch 273/1000\n",
      "5968/5968 [==============================] - 1s 132us/sample - loss: 0.8037 - acc: 0.6397 - val_loss: 0.7949 - val_acc: 0.6450\n",
      "Epoch 274/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8022 - acc: 0.6416 - val_loss: 0.7962 - val_acc: 0.6457\n",
      "Epoch 275/1000\n",
      "5968/5968 [==============================] - 1s 132us/sample - loss: 0.8021 - acc: 0.6413 - val_loss: 0.7978 - val_acc: 0.6457\n",
      "Epoch 276/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8032 - acc: 0.6411 - val_loss: 0.7946 - val_acc: 0.6463\n",
      "Epoch 277/1000\n",
      "5968/5968 [==============================] - 1s 131us/sample - loss: 0.8094 - acc: 0.6408 - val_loss: 0.8156 - val_acc: 0.6470\n",
      "Epoch 278/1000\n",
      "5968/5968 [==============================] - 1s 132us/sample - loss: 0.8144 - acc: 0.6404 - val_loss: 0.8049 - val_acc: 0.6470\n",
      "Epoch 279/1000\n",
      "5968/5968 [==============================] - 1s 127us/sample - loss: 0.8059 - acc: 0.6401 - val_loss: 0.7981 - val_acc: 0.6463\n",
      "Epoch 280/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8040 - acc: 0.6408 - val_loss: 0.7985 - val_acc: 0.6463\n",
      "Epoch 281/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8029 - acc: 0.6408 - val_loss: 0.7969 - val_acc: 0.6457\n",
      "Epoch 282/1000\n",
      "5968/5968 [==============================] - 1s 128us/sample - loss: 0.8061 - acc: 0.6411 - val_loss: 0.8001 - val_acc: 0.6463\n",
      "Epoch 283/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8038 - acc: 0.6416 - val_loss: 0.7976 - val_acc: 0.6450\n",
      "Epoch 284/1000\n",
      "5968/5968 [==============================] - 1s 130us/sample - loss: 0.8051 - acc: 0.6413 - val_loss: 0.7973 - val_acc: 0.6463\n",
      "Epoch 285/1000\n",
      "5968/5968 [==============================] - 1s 129us/sample - loss: 0.8048 - acc: 0.6411 - val_loss: 0.7980 - val_acc: 0.6457\n",
      "Epoch 286/1000\n",
      "5968/5968 [==============================] - 1s 131us/sample - loss: 0.8040 - acc: 0.6409 - val_loss: 0.7978 - val_acc: 0.6450\n",
      "Epoch 286: early stopping\n"
     ]
    }
   ],
   "source": [
    "# label encoding으로 fit\n",
    "signle_rnn_model_label.fit(X3_train_label, y3_train_label)\n",
    "y3_pred_rnn_label = signle_rnn_model_label.predict(X3_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cdc2790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN accouracy :  0.6355841371918542\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN accouracy : \", accuracy_score(y3_pred_rnn_label,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae32af7",
   "metadata": {},
   "source": [
    "### Mulit Layer RNN (One-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c03ae6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_vanilla_rnn_ohe():\n",
    "    model = Sequential()\n",
    "    # return_sequences parameter has to be set True to stack\n",
    "    model.add(SimpleRNN(units, input_shape = inputs, return_sequences = True))   \n",
    "    model.add(SimpleRNN(units, return_sequences = False))\n",
    "    model.add(Dense(outputs))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d645e6f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5968 samples, validate on 1493 samples\n",
      "Epoch 1/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8888 - acc: 0.5995 - val_loss: 0.8322 - val_acc: 0.6457\n",
      "Epoch 2/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8254 - acc: 0.6391 - val_loss: 0.8277 - val_acc: 0.6457\n",
      "Epoch 3/1000\n",
      "5968/5968 [==============================] - 2s 299us/sample - loss: 0.8262 - acc: 0.6389 - val_loss: 0.8267 - val_acc: 0.6457\n",
      "Epoch 4/1000\n",
      "5968/5968 [==============================] - 2s 298us/sample - loss: 0.8256 - acc: 0.6391 - val_loss: 0.8340 - val_acc: 0.6450\n",
      "Epoch 5/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.8269 - acc: 0.6386 - val_loss: 0.8303 - val_acc: 0.6450\n",
      "Epoch 6/1000\n",
      "5968/5968 [==============================] - 2s 299us/sample - loss: 0.8239 - acc: 0.6401 - val_loss: 0.8266 - val_acc: 0.6450\n",
      "Epoch 7/1000\n",
      "5968/5968 [==============================] - 2s 295us/sample - loss: 0.8237 - acc: 0.6394 - val_loss: 0.8250 - val_acc: 0.6450\n",
      "Epoch 8/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.8235 - acc: 0.6392 - val_loss: 0.8258 - val_acc: 0.6450\n",
      "Epoch 9/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8236 - acc: 0.6399 - val_loss: 0.8236 - val_acc: 0.6463\n",
      "Epoch 10/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8223 - acc: 0.6396 - val_loss: 0.8245 - val_acc: 0.6463\n",
      "Epoch 11/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8226 - acc: 0.6404 - val_loss: 0.8259 - val_acc: 0.6470\n",
      "Epoch 12/1000\n",
      "5968/5968 [==============================] - 2s 298us/sample - loss: 0.8232 - acc: 0.6401 - val_loss: 0.8312 - val_acc: 0.6477\n",
      "Epoch 13/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8285 - acc: 0.6408 - val_loss: 0.8266 - val_acc: 0.6484\n",
      "Epoch 14/1000\n",
      "5968/5968 [==============================] - 2s 315us/sample - loss: 0.8251 - acc: 0.6399 - val_loss: 0.8247 - val_acc: 0.6484\n",
      "Epoch 15/1000\n",
      "5968/5968 [==============================] - 2s 305us/sample - loss: 0.8222 - acc: 0.6408 - val_loss: 0.8242 - val_acc: 0.6484\n",
      "Epoch 16/1000\n",
      "5968/5968 [==============================] - 2s 316us/sample - loss: 0.8203 - acc: 0.6402 - val_loss: 0.8179 - val_acc: 0.6490\n",
      "Epoch 17/1000\n",
      "5968/5968 [==============================] - 2s 307us/sample - loss: 0.8200 - acc: 0.6394 - val_loss: 0.8227 - val_acc: 0.6470\n",
      "Epoch 18/1000\n",
      "5968/5968 [==============================] - 2s 305us/sample - loss: 0.8210 - acc: 0.6397 - val_loss: 0.8253 - val_acc: 0.6457\n",
      "Epoch 19/1000\n",
      "5968/5968 [==============================] - 2s 310us/sample - loss: 0.8219 - acc: 0.6399 - val_loss: 0.8237 - val_acc: 0.6477\n",
      "Epoch 20/1000\n",
      "5968/5968 [==============================] - 2s 315us/sample - loss: 0.8209 - acc: 0.6408 - val_loss: 0.8242 - val_acc: 0.6484\n",
      "Epoch 21/1000\n",
      "5968/5968 [==============================] - 2s 307us/sample - loss: 0.8205 - acc: 0.6406 - val_loss: 0.8244 - val_acc: 0.6457\n",
      "Epoch 22/1000\n",
      "5968/5968 [==============================] - 2s 308us/sample - loss: 0.8210 - acc: 0.6372 - val_loss: 0.8196 - val_acc: 0.6470\n",
      "Epoch 23/1000\n",
      "5968/5968 [==============================] - 2s 308us/sample - loss: 0.8204 - acc: 0.6409 - val_loss: 0.8197 - val_acc: 0.6463\n",
      "Epoch 24/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8164 - acc: 0.6371 - val_loss: 0.8086 - val_acc: 0.6490\n",
      "Epoch 25/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8141 - acc: 0.6401 - val_loss: 0.8057 - val_acc: 0.6463\n",
      "Epoch 26/1000\n",
      "5968/5968 [==============================] - 2s 310us/sample - loss: 0.8176 - acc: 0.6402 - val_loss: 0.8178 - val_acc: 0.6470\n",
      "Epoch 27/1000\n",
      "5968/5968 [==============================] - 2s 309us/sample - loss: 0.8155 - acc: 0.6401 - val_loss: 0.8042 - val_acc: 0.6463\n",
      "Epoch 28/1000\n",
      "5968/5968 [==============================] - 2s 320us/sample - loss: 0.8124 - acc: 0.6389 - val_loss: 0.8156 - val_acc: 0.6457\n",
      "Epoch 29/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8127 - acc: 0.6401 - val_loss: 0.8072 - val_acc: 0.6457\n",
      "Epoch 30/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8103 - acc: 0.6399 - val_loss: 0.8029 - val_acc: 0.6457\n",
      "Epoch 31/1000\n",
      "5968/5968 [==============================] - 2s 310us/sample - loss: 0.8099 - acc: 0.6401 - val_loss: 0.8076 - val_acc: 0.6457\n",
      "Epoch 32/1000\n",
      "5968/5968 [==============================] - 2s 309us/sample - loss: 0.8120 - acc: 0.6384 - val_loss: 0.8185 - val_acc: 0.6457\n",
      "Epoch 33/1000\n",
      "5968/5968 [==============================] - 2s 315us/sample - loss: 0.8120 - acc: 0.6391 - val_loss: 0.8077 - val_acc: 0.6457\n",
      "Epoch 34/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8082 - acc: 0.6402 - val_loss: 0.8064 - val_acc: 0.6437\n",
      "Epoch 35/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8102 - acc: 0.6396 - val_loss: 0.8049 - val_acc: 0.6470\n",
      "Epoch 36/1000\n",
      "5968/5968 [==============================] - 2s 306us/sample - loss: 0.8080 - acc: 0.6404 - val_loss: 0.8078 - val_acc: 0.6470\n",
      "Epoch 37/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8136 - acc: 0.6397 - val_loss: 0.8304 - val_acc: 0.6457\n",
      "Epoch 38/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8204 - acc: 0.6392 - val_loss: 0.8163 - val_acc: 0.6470\n",
      "Epoch 39/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.8126 - acc: 0.6401 - val_loss: 0.8110 - val_acc: 0.6457\n",
      "Epoch 40/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8137 - acc: 0.6399 - val_loss: 0.8244 - val_acc: 0.6457\n",
      "Epoch 41/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8143 - acc: 0.6404 - val_loss: 0.8128 - val_acc: 0.6457\n",
      "Epoch 42/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8121 - acc: 0.6399 - val_loss: 0.8159 - val_acc: 0.6457\n",
      "Epoch 43/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8107 - acc: 0.6408 - val_loss: 0.8050 - val_acc: 0.6463\n",
      "Epoch 44/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8084 - acc: 0.6408 - val_loss: 0.8042 - val_acc: 0.6463\n",
      "Epoch 45/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8098 - acc: 0.6416 - val_loss: 0.8198 - val_acc: 0.6497\n",
      "Epoch 46/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8154 - acc: 0.6409 - val_loss: 0.8041 - val_acc: 0.6463\n",
      "Epoch 47/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8073 - acc: 0.6408 - val_loss: 0.8073 - val_acc: 0.6463\n",
      "Epoch 48/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8082 - acc: 0.6411 - val_loss: 0.8026 - val_acc: 0.6484\n",
      "Epoch 49/1000\n",
      "5968/5968 [==============================] - 2s 306us/sample - loss: 0.8048 - acc: 0.6404 - val_loss: 0.8018 - val_acc: 0.6470\n",
      "Epoch 50/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.8118 - acc: 0.6397 - val_loss: 0.8048 - val_acc: 0.6463\n",
      "Epoch 51/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.8085 - acc: 0.6409 - val_loss: 0.8009 - val_acc: 0.6463\n",
      "Epoch 52/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8078 - acc: 0.6408 - val_loss: 0.8140 - val_acc: 0.6463\n",
      "Epoch 53/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8071 - acc: 0.6402 - val_loss: 0.8003 - val_acc: 0.6484\n",
      "Epoch 54/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8073 - acc: 0.6402 - val_loss: 0.8060 - val_acc: 0.6477\n",
      "Epoch 55/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8109 - acc: 0.6414 - val_loss: 0.8115 - val_acc: 0.6484\n",
      "Epoch 56/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8854 - acc: 0.6307 - val_loss: 0.8320 - val_acc: 0.6450\n",
      "Epoch 57/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8284 - acc: 0.6401 - val_loss: 0.8298 - val_acc: 0.6470\n",
      "Epoch 58/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8245 - acc: 0.6396 - val_loss: 0.8457 - val_acc: 0.6477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8380 - acc: 0.6401 - val_loss: 0.8349 - val_acc: 0.6463\n",
      "Epoch 60/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8288 - acc: 0.6397 - val_loss: 0.8346 - val_acc: 0.6470\n",
      "Epoch 61/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.8275 - acc: 0.6397 - val_loss: 0.8343 - val_acc: 0.6470\n",
      "Epoch 62/1000\n",
      "5968/5968 [==============================] - 2s 318us/sample - loss: 0.8248 - acc: 0.6402 - val_loss: 0.8298 - val_acc: 0.6470\n",
      "Epoch 63/1000\n",
      "5968/5968 [==============================] - 2s 315us/sample - loss: 0.8234 - acc: 0.6399 - val_loss: 0.8277 - val_acc: 0.6470\n",
      "Epoch 64/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8210 - acc: 0.6404 - val_loss: 0.8255 - val_acc: 0.6470\n",
      "Epoch 65/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.8232 - acc: 0.6404 - val_loss: 0.8231 - val_acc: 0.6457\n",
      "Epoch 66/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8259 - acc: 0.6404 - val_loss: 0.8258 - val_acc: 0.6470\n",
      "Epoch 67/1000\n",
      "5968/5968 [==============================] - 2s 305us/sample - loss: 0.8225 - acc: 0.6406 - val_loss: 0.8329 - val_acc: 0.6470\n",
      "Epoch 68/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8212 - acc: 0.6399 - val_loss: 0.8343 - val_acc: 0.6457\n",
      "Epoch 69/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8234 - acc: 0.6411 - val_loss: 0.8292 - val_acc: 0.6470\n",
      "Epoch 70/1000\n",
      "5968/5968 [==============================] - 2s 299us/sample - loss: 0.8200 - acc: 0.6414 - val_loss: 0.8238 - val_acc: 0.6457\n",
      "Epoch 71/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8203 - acc: 0.6406 - val_loss: 0.8183 - val_acc: 0.6457\n",
      "Epoch 72/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.8145 - acc: 0.6414 - val_loss: 0.8187 - val_acc: 0.6457\n",
      "Epoch 73/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.8145 - acc: 0.6406 - val_loss: 0.8142 - val_acc: 0.6470\n",
      "Epoch 74/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8156 - acc: 0.6406 - val_loss: 0.8120 - val_acc: 0.6457\n",
      "Epoch 75/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8097 - acc: 0.6406 - val_loss: 0.8034 - val_acc: 0.6477\n",
      "Epoch 76/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8074 - acc: 0.6396 - val_loss: 0.8042 - val_acc: 0.6477\n",
      "Epoch 77/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8079 - acc: 0.6406 - val_loss: 0.8048 - val_acc: 0.6484\n",
      "Epoch 78/1000\n",
      "5968/5968 [==============================] - 2s 313us/sample - loss: 0.8075 - acc: 0.6413 - val_loss: 0.8027 - val_acc: 0.6477\n",
      "Epoch 79/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8064 - acc: 0.6413 - val_loss: 0.8031 - val_acc: 0.6497\n",
      "Epoch 80/1000\n",
      "5968/5968 [==============================] - 2s 320us/sample - loss: 0.8061 - acc: 0.6408 - val_loss: 0.8027 - val_acc: 0.6477\n",
      "Epoch 81/1000\n",
      "5968/5968 [==============================] - 2s 330us/sample - loss: 0.8047 - acc: 0.6409 - val_loss: 0.8012 - val_acc: 0.6497\n",
      "Epoch 82/1000\n",
      "5968/5968 [==============================] - 2s 313us/sample - loss: 0.8058 - acc: 0.6411 - val_loss: 0.8020 - val_acc: 0.6470\n",
      "Epoch 83/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8068 - acc: 0.6419 - val_loss: 0.8024 - val_acc: 0.6497\n",
      "Epoch 84/1000\n",
      "5968/5968 [==============================] - 2s 305us/sample - loss: 0.8053 - acc: 0.6416 - val_loss: 0.8033 - val_acc: 0.6484\n",
      "Epoch 85/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8043 - acc: 0.6418 - val_loss: 0.8021 - val_acc: 0.6490\n",
      "Epoch 86/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8043 - acc: 0.6416 - val_loss: 0.8014 - val_acc: 0.6477\n",
      "Epoch 87/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8044 - acc: 0.6414 - val_loss: 0.8010 - val_acc: 0.6490\n",
      "Epoch 88/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8030 - acc: 0.6416 - val_loss: 0.8033 - val_acc: 0.6477\n",
      "Epoch 89/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8030 - acc: 0.6419 - val_loss: 0.7998 - val_acc: 0.6490\n",
      "Epoch 90/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8022 - acc: 0.6404 - val_loss: 0.8039 - val_acc: 0.6477\n",
      "Epoch 91/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8035 - acc: 0.6423 - val_loss: 0.8011 - val_acc: 0.6477\n",
      "Epoch 92/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8037 - acc: 0.6409 - val_loss: 0.8031 - val_acc: 0.6490\n",
      "Epoch 93/1000\n",
      "5968/5968 [==============================] - 2s 306us/sample - loss: 0.8059 - acc: 0.6411 - val_loss: 0.8020 - val_acc: 0.6477\n",
      "Epoch 94/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8031 - acc: 0.6411 - val_loss: 0.8003 - val_acc: 0.6484\n",
      "Epoch 95/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8012 - acc: 0.6416 - val_loss: 0.8073 - val_acc: 0.6477\n",
      "Epoch 96/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8053 - acc: 0.6409 - val_loss: 0.8053 - val_acc: 0.6477\n",
      "Epoch 97/1000\n",
      "5968/5968 [==============================] - 2s 305us/sample - loss: 0.8015 - acc: 0.6406 - val_loss: 0.8037 - val_acc: 0.6477\n",
      "Epoch 98/1000\n",
      "5968/5968 [==============================] - 2s 311us/sample - loss: 0.8007 - acc: 0.6411 - val_loss: 0.8022 - val_acc: 0.6477\n",
      "Epoch 99/1000\n",
      "5968/5968 [==============================] - 2s 308us/sample - loss: 0.8021 - acc: 0.6416 - val_loss: 0.8000 - val_acc: 0.6477\n",
      "Epoch 100/1000\n",
      "5968/5968 [==============================] - 2s 306us/sample - loss: 0.8023 - acc: 0.6416 - val_loss: 0.8032 - val_acc: 0.6477\n",
      "Epoch 101/1000\n",
      "5968/5968 [==============================] - 2s 307us/sample - loss: 0.8025 - acc: 0.6418 - val_loss: 0.8019 - val_acc: 0.6477\n",
      "Epoch 102/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8012 - acc: 0.6419 - val_loss: 0.7999 - val_acc: 0.6477\n",
      "Epoch 103/1000\n",
      "5968/5968 [==============================] - 2s 306us/sample - loss: 0.8032 - acc: 0.6423 - val_loss: 0.8058 - val_acc: 0.6477\n",
      "Epoch 104/1000\n",
      "5968/5968 [==============================] - 2s 307us/sample - loss: 0.8032 - acc: 0.6424 - val_loss: 0.8095 - val_acc: 0.6477\n",
      "Epoch 105/1000\n",
      "5968/5968 [==============================] - 2s 308us/sample - loss: 0.8037 - acc: 0.6418 - val_loss: 0.8045 - val_acc: 0.6477\n",
      "Epoch 106/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.8034 - acc: 0.6426 - val_loss: 0.8048 - val_acc: 0.6477\n",
      "Epoch 107/1000\n",
      "5968/5968 [==============================] - 2s 307us/sample - loss: 0.8040 - acc: 0.6416 - val_loss: 0.8031 - val_acc: 0.6477\n",
      "Epoch 108/1000\n",
      "5968/5968 [==============================] - 2s 306us/sample - loss: 0.8007 - acc: 0.6424 - val_loss: 0.8028 - val_acc: 0.6477\n",
      "Epoch 109/1000\n",
      "5968/5968 [==============================] - 2s 307us/sample - loss: 0.8011 - acc: 0.6419 - val_loss: 0.8055 - val_acc: 0.6477\n",
      "Epoch 110/1000\n",
      "5968/5968 [==============================] - 2s 309us/sample - loss: 0.7997 - acc: 0.6416 - val_loss: 0.8052 - val_acc: 0.6477\n",
      "Epoch 111/1000\n",
      "5968/5968 [==============================] - 2s 307us/sample - loss: 0.8009 - acc: 0.6423 - val_loss: 0.8046 - val_acc: 0.6484\n",
      "Epoch 112/1000\n",
      "5968/5968 [==============================] - 2s 306us/sample - loss: 0.8011 - acc: 0.6423 - val_loss: 0.8057 - val_acc: 0.6477\n",
      "Epoch 113/1000\n",
      "5968/5968 [==============================] - 2s 305us/sample - loss: 0.8000 - acc: 0.6424 - val_loss: 0.8044 - val_acc: 0.6477\n",
      "Epoch 114/1000\n",
      "5968/5968 [==============================] - 2s 308us/sample - loss: 0.7989 - acc: 0.6431 - val_loss: 0.8050 - val_acc: 0.6477\n",
      "Epoch 115/1000\n",
      "5968/5968 [==============================] - 2s 306us/sample - loss: 0.8010 - acc: 0.6419 - val_loss: 0.8051 - val_acc: 0.6470\n",
      "Epoch 116/1000\n",
      "5968/5968 [==============================] - 2s 306us/sample - loss: 0.8021 - acc: 0.6424 - val_loss: 0.8058 - val_acc: 0.6470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8023 - acc: 0.6424 - val_loss: 0.8040 - val_acc: 0.6477\n",
      "Epoch 118/1000\n",
      "5968/5968 [==============================] - 2s 311us/sample - loss: 0.8029 - acc: 0.6416 - val_loss: 0.8026 - val_acc: 0.6463\n",
      "Epoch 119/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8020 - acc: 0.6419 - val_loss: 0.8083 - val_acc: 0.6457\n",
      "Epoch 120/1000\n",
      "5968/5968 [==============================] - 2s 336us/sample - loss: 0.8009 - acc: 0.6423 - val_loss: 0.8098 - val_acc: 0.6463\n",
      "Epoch 121/1000\n",
      "5968/5968 [==============================] - 2s 334us/sample - loss: 0.8011 - acc: 0.6426 - val_loss: 0.8054 - val_acc: 0.6463\n",
      "Epoch 122/1000\n",
      "5968/5968 [==============================] - 2s 339us/sample - loss: 0.8050 - acc: 0.6433 - val_loss: 0.8056 - val_acc: 0.6463\n",
      "Epoch 123/1000\n",
      "5968/5968 [==============================] - 2s 307us/sample - loss: 0.7996 - acc: 0.6436 - val_loss: 0.8062 - val_acc: 0.6477\n",
      "Epoch 124/1000\n",
      "5968/5968 [==============================] - 2s 297us/sample - loss: 0.7985 - acc: 0.6424 - val_loss: 0.8052 - val_acc: 0.6470\n",
      "Epoch 125/1000\n",
      "5968/5968 [==============================] - 2s 308us/sample - loss: 0.7983 - acc: 0.6431 - val_loss: 0.8034 - val_acc: 0.6470\n",
      "Epoch 126/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.7997 - acc: 0.6424 - val_loss: 0.8035 - val_acc: 0.6477\n",
      "Epoch 127/1000\n",
      "5968/5968 [==============================] - 2s 296us/sample - loss: 0.7987 - acc: 0.6428 - val_loss: 0.8049 - val_acc: 0.6470\n",
      "Epoch 128/1000\n",
      "5968/5968 [==============================] - 2s 299us/sample - loss: 0.8007 - acc: 0.6428 - val_loss: 0.8060 - val_acc: 0.6484\n",
      "Epoch 129/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.7991 - acc: 0.6428 - val_loss: 0.8039 - val_acc: 0.6484\n",
      "Epoch 130/1000\n",
      "5968/5968 [==============================] - 2s 311us/sample - loss: 0.7989 - acc: 0.6433 - val_loss: 0.8054 - val_acc: 0.6477\n",
      "Epoch 131/1000\n",
      "5968/5968 [==============================] - 2s 384us/sample - loss: 0.7980 - acc: 0.6429 - val_loss: 0.8059 - val_acc: 0.6477\n",
      "Epoch 132/1000\n",
      "5968/5968 [==============================] - 2s 357us/sample - loss: 0.8006 - acc: 0.6434 - val_loss: 0.8051 - val_acc: 0.6463\n",
      "Epoch 133/1000\n",
      "5968/5968 [==============================] - 2s 342us/sample - loss: 0.7999 - acc: 0.6443 - val_loss: 0.8050 - val_acc: 0.6463\n",
      "Epoch 134/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.7992 - acc: 0.6419 - val_loss: 0.8025 - val_acc: 0.6470\n",
      "Epoch 135/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.7991 - acc: 0.6431 - val_loss: 0.8051 - val_acc: 0.6470\n",
      "Epoch 136/1000\n",
      "5968/5968 [==============================] - 2s 310us/sample - loss: 0.7969 - acc: 0.6443 - val_loss: 0.8106 - val_acc: 0.6470\n",
      "Epoch 137/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.7993 - acc: 0.6436 - val_loss: 0.8072 - val_acc: 0.6470\n",
      "Epoch 138/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.7990 - acc: 0.6439 - val_loss: 0.8081 - val_acc: 0.6484\n",
      "Epoch 139/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.7973 - acc: 0.6444 - val_loss: 0.8061 - val_acc: 0.6463\n",
      "Epoch 140/1000\n",
      "5968/5968 [==============================] - 2s 299us/sample - loss: 0.8013 - acc: 0.6434 - val_loss: 0.8062 - val_acc: 0.6470\n",
      "Epoch 141/1000\n",
      "5968/5968 [==============================] - 2s 307us/sample - loss: 0.7983 - acc: 0.6436 - val_loss: 0.8026 - val_acc: 0.6484\n",
      "Epoch 142/1000\n",
      "5968/5968 [==============================] - 2s 299us/sample - loss: 0.7984 - acc: 0.6429 - val_loss: 0.8035 - val_acc: 0.6477\n",
      "Epoch 143/1000\n",
      "5968/5968 [==============================] - 2s 298us/sample - loss: 0.7984 - acc: 0.6433 - val_loss: 0.8085 - val_acc: 0.6477\n",
      "Epoch 144/1000\n",
      "5968/5968 [==============================] - 2s 298us/sample - loss: 0.7977 - acc: 0.6438 - val_loss: 0.8068 - val_acc: 0.6477\n",
      "Epoch 145/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.7975 - acc: 0.6449 - val_loss: 0.8073 - val_acc: 0.6477\n",
      "Epoch 146/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.7961 - acc: 0.6439 - val_loss: 0.8087 - val_acc: 0.6457\n",
      "Epoch 147/1000\n",
      "5968/5968 [==============================] - 2s 317us/sample - loss: 0.7985 - acc: 0.6446 - val_loss: 0.8090 - val_acc: 0.6477\n",
      "Epoch 148/1000\n",
      "5968/5968 [==============================] - 2s 305us/sample - loss: 0.7966 - acc: 0.6441 - val_loss: 0.8053 - val_acc: 0.6470\n",
      "Epoch 149/1000\n",
      "5968/5968 [==============================] - 2s 298us/sample - loss: 0.8031 - acc: 0.6436 - val_loss: 0.8128 - val_acc: 0.6470\n",
      "Epoch 150/1000\n",
      "5968/5968 [==============================] - 2s 316us/sample - loss: 0.8430 - acc: 0.6190 - val_loss: 0.9441 - val_acc: 0.5452\n",
      "Epoch 151/1000\n",
      "5968/5968 [==============================] - 2s 335us/sample - loss: 0.8662 - acc: 0.6307 - val_loss: 0.8433 - val_acc: 0.6450\n",
      "Epoch 152/1000\n",
      "5968/5968 [==============================] - 2s 319us/sample - loss: 0.8315 - acc: 0.6384 - val_loss: 0.8397 - val_acc: 0.6450\n",
      "Epoch 153/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8298 - acc: 0.6397 - val_loss: 0.8298 - val_acc: 0.6450\n",
      "Epoch 154/1000\n",
      "5968/5968 [==============================] - 2s 330us/sample - loss: 0.8244 - acc: 0.6401 - val_loss: 0.8328 - val_acc: 0.6450\n",
      "Epoch 155/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.8231 - acc: 0.6408 - val_loss: 0.8293 - val_acc: 0.6450\n",
      "Epoch 156/1000\n",
      "5968/5968 [==============================] - 2s 334us/sample - loss: 0.8223 - acc: 0.6406 - val_loss: 0.8322 - val_acc: 0.6450\n",
      "Epoch 157/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8248 - acc: 0.6404 - val_loss: 0.8344 - val_acc: 0.6450\n",
      "Epoch 158/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.8263 - acc: 0.6416 - val_loss: 0.8302 - val_acc: 0.6450\n",
      "Epoch 159/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.8228 - acc: 0.6414 - val_loss: 0.8312 - val_acc: 0.6457\n",
      "Epoch 160/1000\n",
      "5968/5968 [==============================] - 2s 328us/sample - loss: 0.8246 - acc: 0.6418 - val_loss: 0.8324 - val_acc: 0.6457\n",
      "Epoch 161/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8228 - acc: 0.6418 - val_loss: 0.8266 - val_acc: 0.6457\n",
      "Epoch 162/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8212 - acc: 0.6413 - val_loss: 0.8253 - val_acc: 0.6457\n",
      "Epoch 163/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8210 - acc: 0.6416 - val_loss: 0.8319 - val_acc: 0.6457\n",
      "Epoch 164/1000\n",
      "5968/5968 [==============================] - 2s 328us/sample - loss: 0.8224 - acc: 0.6416 - val_loss: 0.8365 - val_acc: 0.6463\n",
      "Epoch 165/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8254 - acc: 0.6421 - val_loss: 0.8273 - val_acc: 0.6463\n",
      "Epoch 166/1000\n",
      "5968/5968 [==============================] - 2s 332us/sample - loss: 0.8229 - acc: 0.6414 - val_loss: 0.8282 - val_acc: 0.6457\n",
      "Epoch 167/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.8218 - acc: 0.6416 - val_loss: 0.8370 - val_acc: 0.6463\n",
      "Epoch 168/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8229 - acc: 0.6423 - val_loss: 0.8320 - val_acc: 0.6457\n",
      "Epoch 169/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8219 - acc: 0.6419 - val_loss: 0.8320 - val_acc: 0.6463\n",
      "Epoch 170/1000\n",
      "5968/5968 [==============================] - 2s 322us/sample - loss: 0.8220 - acc: 0.6416 - val_loss: 0.8309 - val_acc: 0.6457\n",
      "Epoch 171/1000\n",
      "5968/5968 [==============================] - 2s 327us/sample - loss: 0.8225 - acc: 0.6419 - val_loss: 0.8262 - val_acc: 0.6463\n",
      "Epoch 172/1000\n",
      "5968/5968 [==============================] - 2s 322us/sample - loss: 0.8213 - acc: 0.6409 - val_loss: 0.8256 - val_acc: 0.6457\n",
      "Epoch 173/1000\n",
      "5968/5968 [==============================] - 2s 321us/sample - loss: 0.8224 - acc: 0.6413 - val_loss: 0.8355 - val_acc: 0.6450\n",
      "Epoch 174/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.8215 - acc: 0.6409 - val_loss: 0.8295 - val_acc: 0.6463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8209 - acc: 0.6423 - val_loss: 0.8272 - val_acc: 0.6457\n",
      "Epoch 176/1000\n",
      "5968/5968 [==============================] - 2s 320us/sample - loss: 0.8184 - acc: 0.6418 - val_loss: 0.8187 - val_acc: 0.6463\n",
      "Epoch 177/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8152 - acc: 0.6416 - val_loss: 0.8178 - val_acc: 0.6463\n",
      "Epoch 178/1000\n",
      "5968/5968 [==============================] - 2s 321us/sample - loss: 0.8148 - acc: 0.6411 - val_loss: 0.8184 - val_acc: 0.6463\n",
      "Epoch 179/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8118 - acc: 0.6416 - val_loss: 0.8158 - val_acc: 0.6463\n",
      "Epoch 180/1000\n",
      "5968/5968 [==============================] - 2s 322us/sample - loss: 0.8144 - acc: 0.6413 - val_loss: 0.8243 - val_acc: 0.6457\n",
      "Epoch 181/1000\n",
      "5968/5968 [==============================] - 2s 330us/sample - loss: 0.8128 - acc: 0.6418 - val_loss: 0.8119 - val_acc: 0.6463\n",
      "Epoch 182/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8099 - acc: 0.6401 - val_loss: 0.8091 - val_acc: 0.6457\n",
      "Epoch 183/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8121 - acc: 0.6414 - val_loss: 0.8256 - val_acc: 0.6470\n",
      "Epoch 184/1000\n",
      "5968/5968 [==============================] - 2s 318us/sample - loss: 0.8126 - acc: 0.6413 - val_loss: 0.8042 - val_acc: 0.6457\n",
      "Epoch 185/1000\n",
      "5968/5968 [==============================] - 2s 330us/sample - loss: 0.8080 - acc: 0.6413 - val_loss: 0.8028 - val_acc: 0.6463\n",
      "Epoch 186/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8089 - acc: 0.6409 - val_loss: 0.8151 - val_acc: 0.6463\n",
      "Epoch 187/1000\n",
      "5968/5968 [==============================] - 2s 319us/sample - loss: 0.8107 - acc: 0.6396 - val_loss: 0.8182 - val_acc: 0.6443\n",
      "Epoch 188/1000\n",
      "5968/5968 [==============================] - 2s 327us/sample - loss: 0.8089 - acc: 0.6421 - val_loss: 0.8069 - val_acc: 0.6484\n",
      "Epoch 189/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8080 - acc: 0.6419 - val_loss: 0.8067 - val_acc: 0.6470\n",
      "Epoch 189: early stopping\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding으로 fit\n",
    "multi_rnn_model_ohe = KerasClassifier(build_fn = stacked_vanilla_rnn_ohe\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])\n",
    "multi_rnn_model_ohe.fit(X3_train_ohe, y3_train_ohe)\n",
    "y3_pred_multi_rnn_ohe = multi_rnn_model_ohe.predict(X3_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba97e97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accouracy :  0.6371918542336549\n"
     ]
    }
   ],
   "source": [
    "print(\"accouracy : \", accuracy_score(y3_pred_multi_rnn_ohe,y3_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66525c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_vanilla_rnn_label():\n",
    "    model = Sequential()\n",
    "    # return_sequences parameter has to be set True to stack\n",
    "    model.add(SimpleRNN(units, input_shape = inputs, return_sequences = True))   \n",
    "    model.add(SimpleRNN(units, return_sequences = False))\n",
    "    model.add(Dense(outputs))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3120ffcc",
   "metadata": {},
   "source": [
    "### Mulit Layer RNN (label encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91a54e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5968 samples, validate on 1493 samples\n",
      "Epoch 1/1000\n",
      "5968/5968 [==============================] - 2s 387us/sample - loss: 0.8810 - acc: 0.6136 - val_loss: 0.8315 - val_acc: 0.6443\n",
      "Epoch 2/1000\n",
      "5968/5968 [==============================] - 2s 331us/sample - loss: 0.8264 - acc: 0.6391 - val_loss: 0.8185 - val_acc: 0.6437\n",
      "Epoch 3/1000\n",
      "5968/5968 [==============================] - 2s 328us/sample - loss: 0.8205 - acc: 0.6391 - val_loss: 0.8063 - val_acc: 0.6457\n",
      "Epoch 4/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.8177 - acc: 0.6389 - val_loss: 0.8071 - val_acc: 0.6457\n",
      "Epoch 5/1000\n",
      "5968/5968 [==============================] - 2s 330us/sample - loss: 0.8160 - acc: 0.6391 - val_loss: 0.8028 - val_acc: 0.6470\n",
      "Epoch 6/1000\n",
      "5968/5968 [==============================] - 2s 336us/sample - loss: 0.8127 - acc: 0.6386 - val_loss: 0.8052 - val_acc: 0.6470\n",
      "Epoch 7/1000\n",
      "5968/5968 [==============================] - 2s 328us/sample - loss: 0.8136 - acc: 0.6397 - val_loss: 0.8085 - val_acc: 0.6470\n",
      "Epoch 8/1000\n",
      "5968/5968 [==============================] - 2s 330us/sample - loss: 0.8184 - acc: 0.6401 - val_loss: 0.8093 - val_acc: 0.6477\n",
      "Epoch 9/1000\n",
      "5968/5968 [==============================] - 2s 327us/sample - loss: 0.8152 - acc: 0.6384 - val_loss: 0.8039 - val_acc: 0.6470\n",
      "Epoch 10/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8131 - acc: 0.6389 - val_loss: 0.8100 - val_acc: 0.6490\n",
      "Epoch 11/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.8131 - acc: 0.6397 - val_loss: 0.8020 - val_acc: 0.6463\n",
      "Epoch 12/1000\n",
      "5968/5968 [==============================] - 2s 327us/sample - loss: 0.8128 - acc: 0.6402 - val_loss: 0.8094 - val_acc: 0.6437\n",
      "Epoch 13/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8102 - acc: 0.6401 - val_loss: 0.7978 - val_acc: 0.6463\n",
      "Epoch 14/1000\n",
      "5968/5968 [==============================] - 2s 331us/sample - loss: 0.8093 - acc: 0.6402 - val_loss: 0.8011 - val_acc: 0.6477\n",
      "Epoch 15/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8123 - acc: 0.6399 - val_loss: 0.8035 - val_acc: 0.6470\n",
      "Epoch 16/1000\n",
      "5968/5968 [==============================] - 2s 327us/sample - loss: 0.8095 - acc: 0.6402 - val_loss: 0.8023 - val_acc: 0.6470\n",
      "Epoch 17/1000\n",
      "5968/5968 [==============================] - 2s 330us/sample - loss: 0.8087 - acc: 0.6404 - val_loss: 0.8064 - val_acc: 0.6470\n",
      "Epoch 18/1000\n",
      "5968/5968 [==============================] - 2s 334us/sample - loss: 0.8076 - acc: 0.6408 - val_loss: 0.8036 - val_acc: 0.6463\n",
      "Epoch 19/1000\n",
      "5968/5968 [==============================] - 2s 331us/sample - loss: 0.8078 - acc: 0.6399 - val_loss: 0.8029 - val_acc: 0.6463\n",
      "Epoch 20/1000\n",
      "5968/5968 [==============================] - 2s 319us/sample - loss: 0.8062 - acc: 0.6406 - val_loss: 0.8006 - val_acc: 0.6470\n",
      "Epoch 21/1000\n",
      "5968/5968 [==============================] - 2s 321us/sample - loss: 0.8053 - acc: 0.6397 - val_loss: 0.8015 - val_acc: 0.6470\n",
      "Epoch 22/1000\n",
      "5968/5968 [==============================] - 2s 333us/sample - loss: 0.8070 - acc: 0.6404 - val_loss: 0.8109 - val_acc: 0.6490\n",
      "Epoch 23/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8070 - acc: 0.6406 - val_loss: 0.8021 - val_acc: 0.6490\n",
      "Epoch 24/1000\n",
      "5968/5968 [==============================] - 2s 327us/sample - loss: 0.8063 - acc: 0.6409 - val_loss: 0.7971 - val_acc: 0.6463\n",
      "Epoch 25/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8054 - acc: 0.6419 - val_loss: 0.7999 - val_acc: 0.6477\n",
      "Epoch 26/1000\n",
      "5968/5968 [==============================] - 2s 322us/sample - loss: 0.8036 - acc: 0.6402 - val_loss: 0.8006 - val_acc: 0.6477\n",
      "Epoch 27/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.8056 - acc: 0.6404 - val_loss: 0.7990 - val_acc: 0.6490\n",
      "Epoch 28/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8044 - acc: 0.6418 - val_loss: 0.7995 - val_acc: 0.6477\n",
      "Epoch 29/1000\n",
      "5968/5968 [==============================] - 2s 330us/sample - loss: 0.8061 - acc: 0.6409 - val_loss: 0.8085 - val_acc: 0.6484\n",
      "Epoch 30/1000\n",
      "5968/5968 [==============================] - 2s 327us/sample - loss: 0.8045 - acc: 0.6416 - val_loss: 0.8022 - val_acc: 0.6490\n",
      "Epoch 31/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8051 - acc: 0.6409 - val_loss: 0.8044 - val_acc: 0.6484\n",
      "Epoch 32/1000\n",
      "5968/5968 [==============================] - 2s 332us/sample - loss: 0.8044 - acc: 0.6416 - val_loss: 0.8042 - val_acc: 0.6477\n",
      "Epoch 33/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8044 - acc: 0.6408 - val_loss: 0.8737 - val_acc: 0.5995\n",
      "Epoch 34/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8254 - acc: 0.6374 - val_loss: 0.8219 - val_acc: 0.6470\n",
      "Epoch 35/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.8213 - acc: 0.6401 - val_loss: 0.8252 - val_acc: 0.6490\n",
      "Epoch 36/1000\n",
      "5968/5968 [==============================] - 2s 327us/sample - loss: 0.8176 - acc: 0.6402 - val_loss: 0.8212 - val_acc: 0.6490\n",
      "Epoch 37/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.8171 - acc: 0.6408 - val_loss: 0.8325 - val_acc: 0.6504\n",
      "Epoch 38/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.8120 - acc: 0.6408 - val_loss: 0.8184 - val_acc: 0.6463\n",
      "Epoch 39/1000\n",
      "5968/5968 [==============================] - 2s 319us/sample - loss: 0.8086 - acc: 0.6411 - val_loss: 0.8145 - val_acc: 0.6477\n",
      "Epoch 40/1000\n",
      "5968/5968 [==============================] - 2s 322us/sample - loss: 0.8079 - acc: 0.6416 - val_loss: 0.8219 - val_acc: 0.6484\n",
      "Epoch 41/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8426 - acc: 0.6225 - val_loss: 0.8261 - val_acc: 0.6463\n",
      "Epoch 42/1000\n",
      "5968/5968 [==============================] - 2s 316us/sample - loss: 0.8201 - acc: 0.6379 - val_loss: 0.8177 - val_acc: 0.6457\n",
      "Epoch 43/1000\n",
      "5968/5968 [==============================] - 2s 327us/sample - loss: 0.8141 - acc: 0.6394 - val_loss: 0.8234 - val_acc: 0.6463\n",
      "Epoch 44/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.8162 - acc: 0.6409 - val_loss: 0.8261 - val_acc: 0.6450\n",
      "Epoch 45/1000\n",
      "5968/5968 [==============================] - 2s 330us/sample - loss: 0.8142 - acc: 0.6404 - val_loss: 0.8283 - val_acc: 0.6443\n",
      "Epoch 46/1000\n",
      "5968/5968 [==============================] - 2s 328us/sample - loss: 0.8136 - acc: 0.6409 - val_loss: 0.8295 - val_acc: 0.6457\n",
      "Epoch 47/1000\n",
      "5968/5968 [==============================] - 2s 330us/sample - loss: 0.8110 - acc: 0.6413 - val_loss: 0.8229 - val_acc: 0.6457\n",
      "Epoch 48/1000\n",
      "5968/5968 [==============================] - 2s 321us/sample - loss: 0.8102 - acc: 0.6408 - val_loss: 0.8205 - val_acc: 0.6463\n",
      "Epoch 49/1000\n",
      "5968/5968 [==============================] - 2s 322us/sample - loss: 0.8079 - acc: 0.6411 - val_loss: 0.8202 - val_acc: 0.6470\n",
      "Epoch 50/1000\n",
      "5968/5968 [==============================] - 2s 332us/sample - loss: 0.8101 - acc: 0.6416 - val_loss: 0.8218 - val_acc: 0.6457\n",
      "Epoch 51/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.8074 - acc: 0.6413 - val_loss: 0.8111 - val_acc: 0.6450\n",
      "Epoch 52/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.8086 - acc: 0.6421 - val_loss: 0.8212 - val_acc: 0.6457\n",
      "Epoch 53/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8078 - acc: 0.6406 - val_loss: 0.8165 - val_acc: 0.6450\n",
      "Epoch 54/1000\n",
      "5968/5968 [==============================] - 2s 319us/sample - loss: 0.8080 - acc: 0.6409 - val_loss: 0.8103 - val_acc: 0.6463\n",
      "Epoch 55/1000\n",
      "5968/5968 [==============================] - 2s 331us/sample - loss: 0.8046 - acc: 0.6404 - val_loss: 0.8084 - val_acc: 0.6470\n",
      "Epoch 56/1000\n",
      "5968/5968 [==============================] - 2s 350us/sample - loss: 0.8034 - acc: 0.6421 - val_loss: 0.8091 - val_acc: 0.6470\n",
      "Epoch 57/1000\n",
      "5968/5968 [==============================] - 2s 345us/sample - loss: 0.8023 - acc: 0.6411 - val_loss: 0.8081 - val_acc: 0.6463\n",
      "Epoch 58/1000\n",
      "5968/5968 [==============================] - 2s 359us/sample - loss: 0.8025 - acc: 0.6414 - val_loss: 0.8127 - val_acc: 0.6470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "5968/5968 [==============================] - 2s 337us/sample - loss: 0.8026 - acc: 0.6416 - val_loss: 0.8069 - val_acc: 0.6463\n",
      "Epoch 60/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.8020 - acc: 0.6416 - val_loss: 0.8096 - val_acc: 0.6470\n",
      "Epoch 61/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.8025 - acc: 0.6419 - val_loss: 0.8101 - val_acc: 0.6457\n",
      "Epoch 62/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.8034 - acc: 0.6419 - val_loss: 0.8066 - val_acc: 0.6484\n",
      "Epoch 63/1000\n",
      "5968/5968 [==============================] - 2s 328us/sample - loss: 0.8025 - acc: 0.6421 - val_loss: 0.8059 - val_acc: 0.6477\n",
      "Epoch 64/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8039 - acc: 0.6419 - val_loss: 0.8082 - val_acc: 0.6470\n",
      "Epoch 65/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8008 - acc: 0.6418 - val_loss: 0.8086 - val_acc: 0.6470\n",
      "Epoch 66/1000\n",
      "5968/5968 [==============================] - 2s 322us/sample - loss: 0.8025 - acc: 0.6419 - val_loss: 0.8069 - val_acc: 0.6484\n",
      "Epoch 67/1000\n",
      "5968/5968 [==============================] - 2s 331us/sample - loss: 0.8040 - acc: 0.6404 - val_loss: 0.8055 - val_acc: 0.6457\n",
      "Epoch 68/1000\n",
      "5968/5968 [==============================] - 2s 322us/sample - loss: 0.8007 - acc: 0.6419 - val_loss: 0.8044 - val_acc: 0.6497\n",
      "Epoch 69/1000\n",
      "5968/5968 [==============================] - 2s 327us/sample - loss: 0.8001 - acc: 0.6413 - val_loss: 0.8095 - val_acc: 0.6463\n",
      "Epoch 70/1000\n",
      "5968/5968 [==============================] - 2s 332us/sample - loss: 0.8017 - acc: 0.6426 - val_loss: 0.8060 - val_acc: 0.6450\n",
      "Epoch 71/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8016 - acc: 0.6418 - val_loss: 0.8072 - val_acc: 0.6470\n",
      "Epoch 72/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.8007 - acc: 0.6423 - val_loss: 0.8096 - val_acc: 0.6463\n",
      "Epoch 73/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8028 - acc: 0.6414 - val_loss: 0.8058 - val_acc: 0.6477\n",
      "Epoch 74/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.7996 - acc: 0.6418 - val_loss: 0.8054 - val_acc: 0.6497\n",
      "Epoch 75/1000\n",
      "5968/5968 [==============================] - 2s 313us/sample - loss: 0.8020 - acc: 0.6414 - val_loss: 0.8059 - val_acc: 0.6490\n",
      "Epoch 76/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8012 - acc: 0.6408 - val_loss: 0.8035 - val_acc: 0.6470\n",
      "Epoch 77/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8016 - acc: 0.6409 - val_loss: 0.8047 - val_acc: 0.6470\n",
      "Epoch 78/1000\n",
      "5968/5968 [==============================] - 2s 299us/sample - loss: 0.8009 - acc: 0.6419 - val_loss: 0.8110 - val_acc: 0.6517\n",
      "Epoch 79/1000\n",
      "5968/5968 [==============================] - 2s 300us/sample - loss: 0.8026 - acc: 0.6408 - val_loss: 0.8072 - val_acc: 0.6477\n",
      "Epoch 80/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8019 - acc: 0.6424 - val_loss: 0.8064 - val_acc: 0.6497\n",
      "Epoch 81/1000\n",
      "5968/5968 [==============================] - 2s 297us/sample - loss: 0.8000 - acc: 0.6421 - val_loss: 0.8106 - val_acc: 0.6497\n",
      "Epoch 82/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8014 - acc: 0.6413 - val_loss: 0.8085 - val_acc: 0.6497\n",
      "Epoch 83/1000\n",
      "5968/5968 [==============================] - 2s 298us/sample - loss: 0.8007 - acc: 0.6423 - val_loss: 0.8027 - val_acc: 0.6490\n",
      "Epoch 84/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.8010 - acc: 0.6419 - val_loss: 0.8067 - val_acc: 0.6484\n",
      "Epoch 85/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8016 - acc: 0.6418 - val_loss: 0.8053 - val_acc: 0.6484\n",
      "Epoch 86/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.7998 - acc: 0.6414 - val_loss: 0.8026 - val_acc: 0.6477\n",
      "Epoch 87/1000\n",
      "5968/5968 [==============================] - 2s 301us/sample - loss: 0.8009 - acc: 0.6421 - val_loss: 0.8030 - val_acc: 0.6484\n",
      "Epoch 88/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8030 - acc: 0.6414 - val_loss: 0.8066 - val_acc: 0.6484\n",
      "Epoch 89/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8013 - acc: 0.6421 - val_loss: 0.8071 - val_acc: 0.6477\n",
      "Epoch 90/1000\n",
      "5968/5968 [==============================] - 2s 304us/sample - loss: 0.7998 - acc: 0.6424 - val_loss: 0.8104 - val_acc: 0.6484\n",
      "Epoch 91/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8004 - acc: 0.6424 - val_loss: 0.8064 - val_acc: 0.6484\n",
      "Epoch 92/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.7989 - acc: 0.6426 - val_loss: 0.8068 - val_acc: 0.6484\n",
      "Epoch 93/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.7988 - acc: 0.6421 - val_loss: 0.8103 - val_acc: 0.6497\n",
      "Epoch 94/1000\n",
      "5968/5968 [==============================] - 2s 303us/sample - loss: 0.8010 - acc: 0.6418 - val_loss: 0.8053 - val_acc: 0.6490\n",
      "Epoch 95/1000\n",
      "5968/5968 [==============================] - 2s 302us/sample - loss: 0.7995 - acc: 0.6426 - val_loss: 0.8083 - val_acc: 0.6490\n",
      "Epoch 96/1000\n",
      "5968/5968 [==============================] - 2s 336us/sample - loss: 0.7996 - acc: 0.6419 - val_loss: 0.8027 - val_acc: 0.6504\n",
      "Epoch 97/1000\n",
      "5968/5968 [==============================] - 2s 350us/sample - loss: 0.8004 - acc: 0.6419 - val_loss: 0.8021 - val_acc: 0.6490\n",
      "Epoch 98/1000\n",
      "5968/5968 [==============================] - 2s 334us/sample - loss: 0.8001 - acc: 0.6421 - val_loss: 0.8039 - val_acc: 0.6497\n",
      "Epoch 99/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8018 - acc: 0.6421 - val_loss: 0.8104 - val_acc: 0.6497\n",
      "Epoch 100/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.8015 - acc: 0.6423 - val_loss: 0.8020 - val_acc: 0.6477\n",
      "Epoch 101/1000\n",
      "5968/5968 [==============================] - 2s 327us/sample - loss: 0.8047 - acc: 0.6419 - val_loss: 0.8042 - val_acc: 0.6497\n",
      "Epoch 102/1000\n",
      "5968/5968 [==============================] - 2s 324us/sample - loss: 0.8021 - acc: 0.6423 - val_loss: 0.8078 - val_acc: 0.6497\n",
      "Epoch 103/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.8004 - acc: 0.6431 - val_loss: 0.8057 - val_acc: 0.6504\n",
      "Epoch 104/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.7982 - acc: 0.6423 - val_loss: 0.8029 - val_acc: 0.6504\n",
      "Epoch 105/1000\n",
      "5968/5968 [==============================] - 2s 331us/sample - loss: 0.7998 - acc: 0.6421 - val_loss: 0.8130 - val_acc: 0.6490\n",
      "Epoch 106/1000\n",
      "5968/5968 [==============================] - 2s 331us/sample - loss: 0.7985 - acc: 0.6426 - val_loss: 0.8078 - val_acc: 0.6484\n",
      "Epoch 107/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.7996 - acc: 0.6444 - val_loss: 0.8048 - val_acc: 0.6490\n",
      "Epoch 108/1000\n",
      "5968/5968 [==============================] - 2s 323us/sample - loss: 0.7989 - acc: 0.6441 - val_loss: 0.8090 - val_acc: 0.6470\n",
      "Epoch 109/1000\n",
      "5968/5968 [==============================] - 2s 330us/sample - loss: 0.8008 - acc: 0.6424 - val_loss: 0.8038 - val_acc: 0.6484\n",
      "Epoch 110/1000\n",
      "5968/5968 [==============================] - 2s 333us/sample - loss: 0.7997 - acc: 0.6443 - val_loss: 0.8058 - val_acc: 0.6484\n",
      "Epoch 111/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.7987 - acc: 0.6433 - val_loss: 0.8044 - val_acc: 0.6477\n",
      "Epoch 112/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.7987 - acc: 0.6423 - val_loss: 0.8057 - val_acc: 0.6490\n",
      "Epoch 113/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.7994 - acc: 0.6438 - val_loss: 0.8168 - val_acc: 0.6484\n",
      "Epoch 114/1000\n",
      "5968/5968 [==============================] - 2s 325us/sample - loss: 0.8012 - acc: 0.6439 - val_loss: 0.8051 - val_acc: 0.6463\n",
      "Epoch 115/1000\n",
      "5968/5968 [==============================] - 2s 328us/sample - loss: 0.8015 - acc: 0.6429 - val_loss: 0.8112 - val_acc: 0.6463\n",
      "Epoch 116/1000\n",
      "5968/5968 [==============================] - 2s 328us/sample - loss: 0.7995 - acc: 0.6434 - val_loss: 0.8039 - val_acc: 0.6470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/1000\n",
      "5968/5968 [==============================] - 2s 335us/sample - loss: 0.7979 - acc: 0.6453 - val_loss: 0.8033 - val_acc: 0.6490\n",
      "Epoch 118/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.7961 - acc: 0.6448 - val_loss: 0.8027 - val_acc: 0.6477\n",
      "Epoch 119/1000\n",
      "5968/5968 [==============================] - 2s 329us/sample - loss: 0.7980 - acc: 0.6431 - val_loss: 0.8033 - val_acc: 0.6463\n",
      "Epoch 120/1000\n",
      "5968/5968 [==============================] - 2s 326us/sample - loss: 0.7989 - acc: 0.6441 - val_loss: 0.8004 - val_acc: 0.6490\n",
      "Epoch 121/1000\n",
      "5968/5968 [==============================] - 2s 328us/sample - loss: 0.7973 - acc: 0.6434 - val_loss: 0.8059 - val_acc: 0.6490\n",
      "Epoch 122/1000\n",
      "5968/5968 [==============================] - 2s 328us/sample - loss: 0.7966 - acc: 0.6446 - val_loss: 0.8061 - val_acc: 0.6490\n",
      "Epoch 123/1000\n",
      "5968/5968 [==============================] - 2s 328us/sample - loss: 0.7965 - acc: 0.6444 - val_loss: 0.8047 - val_acc: 0.6477\n",
      "Epoch 124/1000\n",
      "5968/5968 [==============================] - 2s 335us/sample - loss: 0.7972 - acc: 0.6443 - val_loss: 0.8066 - val_acc: 0.6490\n",
      "Epoch 124: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Label encoding으로 fit\n",
    "multi_rnn_model_label = KerasClassifier(build_fn = stacked_vanilla_rnn_label\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])\n",
    "multi_rnn_model_label.fit(X3_train_label, y3_train_label)\n",
    "y3_pred_multi_rnn_label = multi_rnn_model_label.predict(X3_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d730bc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accouracy :  0.6361200428724545\n"
     ]
    }
   ],
   "source": [
    "print(\"accouracy : \", accuracy_score(y3_pred_multi_rnn_label,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2a83a",
   "metadata": {},
   "source": [
    "### Single Layer LSTM (One-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64543d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_ohe():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape = inputs, return_sequences = False))\n",
    "    model.add(Dense(outputs))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b095fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5968 samples, validate on 1493 samples\n",
      "Epoch 1/1000\n",
      "5968/5968 [==============================] - 3s 524us/sample - loss: 0.9949 - acc: 0.6220 - val_loss: 0.8427 - val_acc: 0.6450\n",
      "Epoch 2/1000\n",
      "5968/5968 [==============================] - 3s 514us/sample - loss: 0.8234 - acc: 0.6379 - val_loss: 0.8064 - val_acc: 0.6463\n",
      "Epoch 3/1000\n",
      "5968/5968 [==============================] - 3s 494us/sample - loss: 0.8162 - acc: 0.6369 - val_loss: 0.8100 - val_acc: 0.6457\n",
      "Epoch 4/1000\n",
      "5968/5968 [==============================] - 3s 499us/sample - loss: 0.8143 - acc: 0.6391 - val_loss: 0.8019 - val_acc: 0.6457\n",
      "Epoch 5/1000\n",
      "5968/5968 [==============================] - 3s 498us/sample - loss: 0.8141 - acc: 0.6376 - val_loss: 0.8065 - val_acc: 0.6450\n",
      "Epoch 6/1000\n",
      "5968/5968 [==============================] - 3s 504us/sample - loss: 0.8154 - acc: 0.6379 - val_loss: 0.8022 - val_acc: 0.6450\n",
      "Epoch 7/1000\n",
      "5968/5968 [==============================] - 3s 498us/sample - loss: 0.8120 - acc: 0.6382 - val_loss: 0.8023 - val_acc: 0.6450\n",
      "Epoch 8/1000\n",
      "5968/5968 [==============================] - 3s 495us/sample - loss: 0.8108 - acc: 0.6381 - val_loss: 0.7983 - val_acc: 0.6457\n",
      "Epoch 9/1000\n",
      "5968/5968 [==============================] - 3s 497us/sample - loss: 0.8114 - acc: 0.6382 - val_loss: 0.8047 - val_acc: 0.6443\n",
      "Epoch 10/1000\n",
      "5968/5968 [==============================] - 3s 490us/sample - loss: 0.8103 - acc: 0.6382 - val_loss: 0.8034 - val_acc: 0.6450\n",
      "Epoch 11/1000\n",
      "5968/5968 [==============================] - 3s 494us/sample - loss: 0.8105 - acc: 0.6389 - val_loss: 0.7954 - val_acc: 0.6457\n",
      "Epoch 12/1000\n",
      "5968/5968 [==============================] - 3s 497us/sample - loss: 0.8069 - acc: 0.6389 - val_loss: 0.7924 - val_acc: 0.6450\n",
      "Epoch 13/1000\n",
      "5968/5968 [==============================] - 3s 500us/sample - loss: 0.8086 - acc: 0.6381 - val_loss: 0.7993 - val_acc: 0.6450\n",
      "Epoch 14/1000\n",
      "5968/5968 [==============================] - 3s 494us/sample - loss: 0.8076 - acc: 0.6386 - val_loss: 0.7926 - val_acc: 0.6457\n",
      "Epoch 15/1000\n",
      "5968/5968 [==============================] - 3s 496us/sample - loss: 0.8068 - acc: 0.6371 - val_loss: 0.8037 - val_acc: 0.6457\n",
      "Epoch 16/1000\n",
      "5968/5968 [==============================] - 3s 531us/sample - loss: 0.8111 - acc: 0.6394 - val_loss: 0.7983 - val_acc: 0.6470\n",
      "Epoch 17/1000\n",
      "5968/5968 [==============================] - 3s 585us/sample - loss: 0.8066 - acc: 0.6389 - val_loss: 0.7940 - val_acc: 0.6470\n",
      "Epoch 18/1000\n",
      "5968/5968 [==============================] - 4s 590us/sample - loss: 0.8037 - acc: 0.6387 - val_loss: 0.8067 - val_acc: 0.6450\n",
      "Epoch 19/1000\n",
      "5968/5968 [==============================] - 4s 632us/sample - loss: 0.8054 - acc: 0.6392 - val_loss: 0.7928 - val_acc: 0.6470\n",
      "Epoch 20/1000\n",
      "5968/5968 [==============================] - 3s 523us/sample - loss: 0.8037 - acc: 0.6397 - val_loss: 0.7910 - val_acc: 0.6477\n",
      "Epoch 21/1000\n",
      "5968/5968 [==============================] - 3s 493us/sample - loss: 0.8041 - acc: 0.6396 - val_loss: 0.8010 - val_acc: 0.6470\n",
      "Epoch 22/1000\n",
      "5968/5968 [==============================] - 3s 513us/sample - loss: 0.8073 - acc: 0.6394 - val_loss: 0.7920 - val_acc: 0.6470\n",
      "Epoch 23/1000\n",
      "5968/5968 [==============================] - 3s 522us/sample - loss: 0.8037 - acc: 0.6396 - val_loss: 0.7932 - val_acc: 0.6470\n",
      "Epoch 24/1000\n",
      "5968/5968 [==============================] - 3s 496us/sample - loss: 0.8030 - acc: 0.6394 - val_loss: 0.7974 - val_acc: 0.6463\n",
      "Epoch 25/1000\n",
      "5968/5968 [==============================] - 3s 451us/sample - loss: 0.8019 - acc: 0.6402 - val_loss: 0.7906 - val_acc: 0.6463\n",
      "Epoch 26/1000\n",
      "5968/5968 [==============================] - 3s 453us/sample - loss: 0.8021 - acc: 0.6397 - val_loss: 0.7893 - val_acc: 0.6470\n",
      "Epoch 27/1000\n",
      "5968/5968 [==============================] - 3s 456us/sample - loss: 0.8021 - acc: 0.6392 - val_loss: 0.7896 - val_acc: 0.6484\n",
      "Epoch 28/1000\n",
      "5968/5968 [==============================] - 3s 458us/sample - loss: 0.8027 - acc: 0.6404 - val_loss: 0.7900 - val_acc: 0.6470\n",
      "Epoch 29/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.8026 - acc: 0.6397 - val_loss: 0.7898 - val_acc: 0.6470\n",
      "Epoch 30/1000\n",
      "5968/5968 [==============================] - 3s 451us/sample - loss: 0.7999 - acc: 0.6402 - val_loss: 0.7931 - val_acc: 0.6450\n",
      "Epoch 31/1000\n",
      "5968/5968 [==============================] - 3s 450us/sample - loss: 0.8008 - acc: 0.6408 - val_loss: 0.7937 - val_acc: 0.6477\n",
      "Epoch 32/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.8009 - acc: 0.6406 - val_loss: 0.7998 - val_acc: 0.6470\n",
      "Epoch 33/1000\n",
      "5968/5968 [==============================] - 3s 454us/sample - loss: 0.8020 - acc: 0.6402 - val_loss: 0.7899 - val_acc: 0.6477\n",
      "Epoch 34/1000\n",
      "5968/5968 [==============================] - 3s 451us/sample - loss: 0.8006 - acc: 0.6394 - val_loss: 0.7930 - val_acc: 0.6477\n",
      "Epoch 35/1000\n",
      "5968/5968 [==============================] - 3s 449us/sample - loss: 0.8007 - acc: 0.6409 - val_loss: 0.7885 - val_acc: 0.6477\n",
      "Epoch 36/1000\n",
      "5968/5968 [==============================] - 3s 452us/sample - loss: 0.8017 - acc: 0.6419 - val_loss: 0.7930 - val_acc: 0.6470\n",
      "Epoch 37/1000\n",
      "5968/5968 [==============================] - 3s 456us/sample - loss: 0.8016 - acc: 0.6401 - val_loss: 0.7902 - val_acc: 0.6457\n",
      "Epoch 38/1000\n",
      "5968/5968 [==============================] - 3s 505us/sample - loss: 0.7995 - acc: 0.6408 - val_loss: 0.7873 - val_acc: 0.6477\n",
      "Epoch 39/1000\n",
      "5968/5968 [==============================] - 3s 528us/sample - loss: 0.8003 - acc: 0.6409 - val_loss: 0.7879 - val_acc: 0.6484\n",
      "Epoch 40/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.8006 - acc: 0.6408 - val_loss: 0.7946 - val_acc: 0.6484\n",
      "Epoch 41/1000\n",
      "5968/5968 [==============================] - 3s 500us/sample - loss: 0.7976 - acc: 0.6419 - val_loss: 0.7930 - val_acc: 0.6490\n",
      "Epoch 42/1000\n",
      "5968/5968 [==============================] - 3s 481us/sample - loss: 0.7960 - acc: 0.6411 - val_loss: 0.7879 - val_acc: 0.6470\n",
      "Epoch 43/1000\n",
      "5968/5968 [==============================] - 3s 507us/sample - loss: 0.7974 - acc: 0.6396 - val_loss: 0.7917 - val_acc: 0.6490\n",
      "Epoch 44/1000\n",
      "5968/5968 [==============================] - 3s 502us/sample - loss: 0.7970 - acc: 0.6419 - val_loss: 0.7879 - val_acc: 0.6470\n",
      "Epoch 45/1000\n",
      "5968/5968 [==============================] - 3s 510us/sample - loss: 0.7964 - acc: 0.6424 - val_loss: 0.7935 - val_acc: 0.6490\n",
      "Epoch 46/1000\n",
      "5968/5968 [==============================] - 3s 473us/sample - loss: 0.7981 - acc: 0.6413 - val_loss: 0.7930 - val_acc: 0.6490\n",
      "Epoch 47/1000\n",
      "5968/5968 [==============================] - 3s 522us/sample - loss: 0.7960 - acc: 0.6416 - val_loss: 0.7937 - val_acc: 0.6497\n",
      "Epoch 48/1000\n",
      "5968/5968 [==============================] - 3s 536us/sample - loss: 0.7986 - acc: 0.6402 - val_loss: 0.7911 - val_acc: 0.6490\n",
      "Epoch 49/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7977 - acc: 0.6413 - val_loss: 0.7900 - val_acc: 0.6477\n",
      "Epoch 50/1000\n",
      "5968/5968 [==============================] - 3s 459us/sample - loss: 0.7969 - acc: 0.6428 - val_loss: 0.7916 - val_acc: 0.6490\n",
      "Epoch 51/1000\n",
      "5968/5968 [==============================] - 3s 503us/sample - loss: 0.7958 - acc: 0.6421 - val_loss: 0.7897 - val_acc: 0.6490\n",
      "Epoch 52/1000\n",
      "5968/5968 [==============================] - 3s 551us/sample - loss: 0.7957 - acc: 0.6421 - val_loss: 0.7909 - val_acc: 0.6477\n",
      "Epoch 53/1000\n",
      "5968/5968 [==============================] - 3s 501us/sample - loss: 0.7936 - acc: 0.6413 - val_loss: 0.7906 - val_acc: 0.6490\n",
      "Epoch 54/1000\n",
      "5968/5968 [==============================] - 3s 507us/sample - loss: 0.7938 - acc: 0.6438 - val_loss: 0.7871 - val_acc: 0.6470\n",
      "Epoch 55/1000\n",
      "5968/5968 [==============================] - 3s 493us/sample - loss: 0.7930 - acc: 0.6428 - val_loss: 0.7919 - val_acc: 0.6490\n",
      "Epoch 56/1000\n",
      "5968/5968 [==============================] - 3s 505us/sample - loss: 0.7959 - acc: 0.6431 - val_loss: 0.7924 - val_acc: 0.6510\n",
      "Epoch 57/1000\n",
      "5968/5968 [==============================] - 3s 501us/sample - loss: 0.7947 - acc: 0.6424 - val_loss: 0.7921 - val_acc: 0.6497\n",
      "Epoch 58/1000\n",
      "5968/5968 [==============================] - 3s 505us/sample - loss: 0.7938 - acc: 0.6424 - val_loss: 0.7888 - val_acc: 0.6504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "5968/5968 [==============================] - 3s 522us/sample - loss: 0.7922 - acc: 0.6418 - val_loss: 0.7956 - val_acc: 0.6490\n",
      "Epoch 60/1000\n",
      "5968/5968 [==============================] - 4s 601us/sample - loss: 0.7981 - acc: 0.6406 - val_loss: 0.7884 - val_acc: 0.6497\n",
      "Epoch 61/1000\n",
      "5968/5968 [==============================] - 3s 516us/sample - loss: 0.7927 - acc: 0.6421 - val_loss: 0.7942 - val_acc: 0.6490\n",
      "Epoch 62/1000\n",
      "5968/5968 [==============================] - 3s 497us/sample - loss: 0.7951 - acc: 0.6406 - val_loss: 0.7897 - val_acc: 0.6477\n",
      "Epoch 63/1000\n",
      "5968/5968 [==============================] - 3s 519us/sample - loss: 0.7951 - acc: 0.6419 - val_loss: 0.7901 - val_acc: 0.6490\n",
      "Epoch 64/1000\n",
      "5968/5968 [==============================] - 3s 526us/sample - loss: 0.7928 - acc: 0.6416 - val_loss: 0.7939 - val_acc: 0.6477\n",
      "Epoch 65/1000\n",
      "5968/5968 [==============================] - 3s 534us/sample - loss: 0.7901 - acc: 0.6423 - val_loss: 0.7892 - val_acc: 0.6504\n",
      "Epoch 66/1000\n",
      "5968/5968 [==============================] - 3s 529us/sample - loss: 0.7952 - acc: 0.6434 - val_loss: 0.7876 - val_acc: 0.6497\n",
      "Epoch 67/1000\n",
      "5968/5968 [==============================] - 3s 530us/sample - loss: 0.7905 - acc: 0.6433 - val_loss: 0.7880 - val_acc: 0.6510\n",
      "Epoch 68/1000\n",
      "5968/5968 [==============================] - 3s 521us/sample - loss: 0.7887 - acc: 0.6441 - val_loss: 0.7879 - val_acc: 0.6490\n",
      "Epoch 69/1000\n",
      "5968/5968 [==============================] - 3s 537us/sample - loss: 0.7948 - acc: 0.6423 - val_loss: 0.7855 - val_acc: 0.6517\n",
      "Epoch 70/1000\n",
      "5968/5968 [==============================] - 3s 534us/sample - loss: 0.7926 - acc: 0.6423 - val_loss: 0.7928 - val_acc: 0.6477\n",
      "Epoch 71/1000\n",
      "5968/5968 [==============================] - 3s 522us/sample - loss: 0.7898 - acc: 0.6443 - val_loss: 0.7860 - val_acc: 0.6504\n",
      "Epoch 72/1000\n",
      "5968/5968 [==============================] - 3s 538us/sample - loss: 0.7908 - acc: 0.6428 - val_loss: 0.7890 - val_acc: 0.6484\n",
      "Epoch 73/1000\n",
      "5968/5968 [==============================] - 3s 521us/sample - loss: 0.7896 - acc: 0.6426 - val_loss: 0.7975 - val_acc: 0.6490\n",
      "Epoch 74/1000\n",
      "5968/5968 [==============================] - 3s 528us/sample - loss: 0.7915 - acc: 0.6444 - val_loss: 0.7908 - val_acc: 0.6497\n",
      "Epoch 75/1000\n",
      "5968/5968 [==============================] - 3s 538us/sample - loss: 0.7888 - acc: 0.6444 - val_loss: 0.7859 - val_acc: 0.6530\n",
      "Epoch 76/1000\n",
      "5968/5968 [==============================] - 3s 527us/sample - loss: 0.7908 - acc: 0.6449 - val_loss: 0.7904 - val_acc: 0.6504\n",
      "Epoch 77/1000\n",
      "5968/5968 [==============================] - 3s 532us/sample - loss: 0.7902 - acc: 0.6443 - val_loss: 0.7870 - val_acc: 0.6497\n",
      "Epoch 78/1000\n",
      "5968/5968 [==============================] - 3s 525us/sample - loss: 0.7884 - acc: 0.6438 - val_loss: 0.7873 - val_acc: 0.6497\n",
      "Epoch 79/1000\n",
      "5968/5968 [==============================] - 3s 520us/sample - loss: 0.7873 - acc: 0.6433 - val_loss: 0.7846 - val_acc: 0.6504\n",
      "Epoch 80/1000\n",
      "5968/5968 [==============================] - 3s 535us/sample - loss: 0.7876 - acc: 0.6451 - val_loss: 0.7923 - val_acc: 0.6497\n",
      "Epoch 81/1000\n",
      "5968/5968 [==============================] - 3s 532us/sample - loss: 0.7893 - acc: 0.6431 - val_loss: 0.7871 - val_acc: 0.6510\n",
      "Epoch 82/1000\n",
      "5968/5968 [==============================] - 3s 517us/sample - loss: 0.7865 - acc: 0.6444 - val_loss: 0.7919 - val_acc: 0.6504\n",
      "Epoch 83/1000\n",
      "5968/5968 [==============================] - 3s 526us/sample - loss: 0.7885 - acc: 0.6443 - val_loss: 0.8021 - val_acc: 0.6470\n",
      "Epoch 84/1000\n",
      "5968/5968 [==============================] - 3s 526us/sample - loss: 0.7899 - acc: 0.6434 - val_loss: 0.7828 - val_acc: 0.6517\n",
      "Epoch 85/1000\n",
      "5968/5968 [==============================] - 3s 533us/sample - loss: 0.7875 - acc: 0.6443 - val_loss: 0.7833 - val_acc: 0.6510\n",
      "Epoch 86/1000\n",
      "5968/5968 [==============================] - 3s 530us/sample - loss: 0.7894 - acc: 0.6438 - val_loss: 0.7830 - val_acc: 0.6510\n",
      "Epoch 87/1000\n",
      "5968/5968 [==============================] - 3s 525us/sample - loss: 0.7890 - acc: 0.6453 - val_loss: 0.7853 - val_acc: 0.6497\n",
      "Epoch 88/1000\n",
      "5968/5968 [==============================] - 3s 526us/sample - loss: 0.7852 - acc: 0.6464 - val_loss: 0.7898 - val_acc: 0.6497\n",
      "Epoch 89/1000\n",
      "5968/5968 [==============================] - 3s 533us/sample - loss: 0.7871 - acc: 0.6456 - val_loss: 0.7825 - val_acc: 0.6504\n",
      "Epoch 90/1000\n",
      "5968/5968 [==============================] - 3s 528us/sample - loss: 0.7856 - acc: 0.6446 - val_loss: 0.7981 - val_acc: 0.6497\n",
      "Epoch 91/1000\n",
      "5968/5968 [==============================] - 3s 533us/sample - loss: 0.7947 - acc: 0.6434 - val_loss: 0.7892 - val_acc: 0.6504\n",
      "Epoch 92/1000\n",
      "5968/5968 [==============================] - 3s 523us/sample - loss: 0.7903 - acc: 0.6414 - val_loss: 0.7875 - val_acc: 0.6504\n",
      "Epoch 93/1000\n",
      "5968/5968 [==============================] - 3s 526us/sample - loss: 0.7893 - acc: 0.6444 - val_loss: 0.7824 - val_acc: 0.6551\n",
      "Epoch 94/1000\n",
      "5968/5968 [==============================] - 3s 534us/sample - loss: 0.7833 - acc: 0.6446 - val_loss: 0.7911 - val_acc: 0.6510\n",
      "Epoch 95/1000\n",
      "5968/5968 [==============================] - 3s 519us/sample - loss: 0.7909 - acc: 0.6438 - val_loss: 0.7872 - val_acc: 0.6537\n",
      "Epoch 96/1000\n",
      "5968/5968 [==============================] - 3s 543us/sample - loss: 0.7869 - acc: 0.6448 - val_loss: 0.7850 - val_acc: 0.6544\n",
      "Epoch 97/1000\n",
      "5968/5968 [==============================] - 3s 529us/sample - loss: 0.7886 - acc: 0.6446 - val_loss: 0.7855 - val_acc: 0.6510\n",
      "Epoch 98/1000\n",
      "5968/5968 [==============================] - 3s 533us/sample - loss: 0.7825 - acc: 0.6443 - val_loss: 0.7863 - val_acc: 0.6510\n",
      "Epoch 99/1000\n",
      "5968/5968 [==============================] - 3s 525us/sample - loss: 0.7835 - acc: 0.6434 - val_loss: 0.7811 - val_acc: 0.6530\n",
      "Epoch 100/1000\n",
      "5968/5968 [==============================] - 3s 522us/sample - loss: 0.7800 - acc: 0.6438 - val_loss: 0.7801 - val_acc: 0.6504\n",
      "Epoch 101/1000\n",
      "5968/5968 [==============================] - 3s 532us/sample - loss: 0.7798 - acc: 0.6466 - val_loss: 0.7784 - val_acc: 0.6524\n",
      "Epoch 102/1000\n",
      "5968/5968 [==============================] - 3s 530us/sample - loss: 0.7788 - acc: 0.6456 - val_loss: 0.7830 - val_acc: 0.6524\n",
      "Epoch 103/1000\n",
      "5968/5968 [==============================] - 3s 521us/sample - loss: 0.7771 - acc: 0.6476 - val_loss: 0.7852 - val_acc: 0.6497\n",
      "Epoch 104/1000\n",
      "5968/5968 [==============================] - 3s 522us/sample - loss: 0.7815 - acc: 0.6470 - val_loss: 0.7922 - val_acc: 0.6510\n",
      "Epoch 105/1000\n",
      "5968/5968 [==============================] - 3s 525us/sample - loss: 0.7794 - acc: 0.6443 - val_loss: 0.7806 - val_acc: 0.6504\n",
      "Epoch 106/1000\n",
      "5968/5968 [==============================] - 3s 522us/sample - loss: 0.7751 - acc: 0.6470 - val_loss: 0.7858 - val_acc: 0.6544\n",
      "Epoch 107/1000\n",
      "5968/5968 [==============================] - 3s 533us/sample - loss: 0.7757 - acc: 0.6473 - val_loss: 0.7990 - val_acc: 0.6504\n",
      "Epoch 108/1000\n",
      "5968/5968 [==============================] - 3s 524us/sample - loss: 0.7792 - acc: 0.6473 - val_loss: 0.7797 - val_acc: 0.6544\n",
      "Epoch 109/1000\n",
      "5968/5968 [==============================] - 3s 522us/sample - loss: 0.7822 - acc: 0.6439 - val_loss: 0.7815 - val_acc: 0.6490\n",
      "Epoch 110/1000\n",
      "5968/5968 [==============================] - 3s 530us/sample - loss: 0.7810 - acc: 0.6464 - val_loss: 0.7841 - val_acc: 0.6551\n",
      "Epoch 111/1000\n",
      "5968/5968 [==============================] - 3s 524us/sample - loss: 0.7770 - acc: 0.6486 - val_loss: 0.7817 - val_acc: 0.6484\n",
      "Epoch 112/1000\n",
      "5968/5968 [==============================] - 3s 529us/sample - loss: 0.7743 - acc: 0.6483 - val_loss: 0.7899 - val_acc: 0.6551\n",
      "Epoch 113/1000\n",
      "5968/5968 [==============================] - 3s 530us/sample - loss: 0.7867 - acc: 0.6448 - val_loss: 0.7824 - val_acc: 0.6510\n",
      "Epoch 114/1000\n",
      "5968/5968 [==============================] - 3s 518us/sample - loss: 0.7731 - acc: 0.6481 - val_loss: 0.7747 - val_acc: 0.6564\n",
      "Epoch 115/1000\n",
      "5968/5968 [==============================] - 3s 528us/sample - loss: 0.7955 - acc: 0.6423 - val_loss: 0.7953 - val_acc: 0.6490\n",
      "Epoch 116/1000\n",
      "5968/5968 [==============================] - 3s 534us/sample - loss: 0.7820 - acc: 0.6459 - val_loss: 0.7721 - val_acc: 0.6517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/1000\n",
      "5968/5968 [==============================] - 3s 533us/sample - loss: 0.7688 - acc: 0.6481 - val_loss: 0.7915 - val_acc: 0.6544\n",
      "Epoch 118/1000\n",
      "5968/5968 [==============================] - 3s 535us/sample - loss: 0.7654 - acc: 0.6493 - val_loss: 0.7693 - val_acc: 0.6537\n",
      "Epoch 119/1000\n",
      "5968/5968 [==============================] - 3s 530us/sample - loss: 0.7672 - acc: 0.6480 - val_loss: 0.7808 - val_acc: 0.6490\n",
      "Epoch 120/1000\n",
      "5968/5968 [==============================] - 3s 530us/sample - loss: 0.7808 - acc: 0.6463 - val_loss: 0.7732 - val_acc: 0.6551\n",
      "Epoch 121/1000\n",
      "5968/5968 [==============================] - 3s 532us/sample - loss: 0.7811 - acc: 0.6446 - val_loss: 0.7808 - val_acc: 0.6517\n",
      "Epoch 122/1000\n",
      "5968/5968 [==============================] - 3s 532us/sample - loss: 0.7827 - acc: 0.6468 - val_loss: 0.8108 - val_acc: 0.6510\n",
      "Epoch 123/1000\n",
      "5968/5968 [==============================] - 3s 536us/sample - loss: 0.8651 - acc: 0.6399 - val_loss: 0.8368 - val_acc: 0.6450\n",
      "Epoch 124/1000\n",
      "5968/5968 [==============================] - 3s 539us/sample - loss: 0.8301 - acc: 0.6387 - val_loss: 0.8277 - val_acc: 0.6450\n",
      "Epoch 125/1000\n",
      "5968/5968 [==============================] - 3s 524us/sample - loss: 0.8227 - acc: 0.6389 - val_loss: 0.8167 - val_acc: 0.6450\n",
      "Epoch 126/1000\n",
      "5968/5968 [==============================] - 3s 536us/sample - loss: 0.8160 - acc: 0.6392 - val_loss: 0.8092 - val_acc: 0.6450\n",
      "Epoch 127/1000\n",
      "5968/5968 [==============================] - 3s 529us/sample - loss: 0.8121 - acc: 0.6394 - val_loss: 0.8077 - val_acc: 0.6463\n",
      "Epoch 128/1000\n",
      "5968/5968 [==============================] - 3s 535us/sample - loss: 0.8133 - acc: 0.6394 - val_loss: 0.8041 - val_acc: 0.6470\n",
      "Epoch 129/1000\n",
      "5968/5968 [==============================] - 3s 533us/sample - loss: 0.8112 - acc: 0.6389 - val_loss: 0.8001 - val_acc: 0.6463\n",
      "Epoch 130/1000\n",
      "5968/5968 [==============================] - 3s 524us/sample - loss: 0.8118 - acc: 0.6392 - val_loss: 0.7990 - val_acc: 0.6463\n",
      "Epoch 131/1000\n",
      "5968/5968 [==============================] - 3s 522us/sample - loss: 0.8106 - acc: 0.6389 - val_loss: 0.8000 - val_acc: 0.6470\n",
      "Epoch 132/1000\n",
      "5968/5968 [==============================] - 3s 531us/sample - loss: 0.8090 - acc: 0.6396 - val_loss: 0.7998 - val_acc: 0.6470\n",
      "Epoch 133/1000\n",
      "5968/5968 [==============================] - 3s 534us/sample - loss: 0.8097 - acc: 0.6392 - val_loss: 0.7964 - val_acc: 0.6470\n",
      "Epoch 134/1000\n",
      "5968/5968 [==============================] - 3s 530us/sample - loss: 0.8083 - acc: 0.6392 - val_loss: 0.7977 - val_acc: 0.6463\n",
      "Epoch 135/1000\n",
      "5968/5968 [==============================] - 3s 524us/sample - loss: 0.8099 - acc: 0.6396 - val_loss: 0.7962 - val_acc: 0.6463\n",
      "Epoch 136/1000\n",
      "5968/5968 [==============================] - 3s 547us/sample - loss: 0.8080 - acc: 0.6394 - val_loss: 0.7970 - val_acc: 0.6463\n",
      "Epoch 137/1000\n",
      "5968/5968 [==============================] - 3s 571us/sample - loss: 0.8072 - acc: 0.6394 - val_loss: 0.7947 - val_acc: 0.6463\n",
      "Epoch 138/1000\n",
      "5968/5968 [==============================] - 3s 558us/sample - loss: 0.8064 - acc: 0.6389 - val_loss: 0.7957 - val_acc: 0.6463\n",
      "Epoch 139/1000\n",
      "5968/5968 [==============================] - 3s 546us/sample - loss: 0.8071 - acc: 0.6402 - val_loss: 0.7950 - val_acc: 0.6463\n",
      "Epoch 140/1000\n",
      "5968/5968 [==============================] - 3s 532us/sample - loss: 0.8074 - acc: 0.6401 - val_loss: 0.7958 - val_acc: 0.6463\n",
      "Epoch 141/1000\n",
      "5968/5968 [==============================] - 3s 531us/sample - loss: 0.8066 - acc: 0.6399 - val_loss: 0.7939 - val_acc: 0.6463\n",
      "Epoch 142/1000\n",
      "5968/5968 [==============================] - 3s 534us/sample - loss: 0.8075 - acc: 0.6409 - val_loss: 0.7961 - val_acc: 0.6463\n",
      "Epoch 143/1000\n",
      "5968/5968 [==============================] - 3s 560us/sample - loss: 0.8057 - acc: 0.6399 - val_loss: 0.7946 - val_acc: 0.6463\n",
      "Epoch 144/1000\n",
      "5968/5968 [==============================] - 3s 537us/sample - loss: 0.8062 - acc: 0.6399 - val_loss: 0.7939 - val_acc: 0.6463\n",
      "Epoch 145/1000\n",
      "5968/5968 [==============================] - 3s 536us/sample - loss: 0.8049 - acc: 0.6397 - val_loss: 0.7957 - val_acc: 0.6457\n",
      "Epoch 146/1000\n",
      "5968/5968 [==============================] - 3s 524us/sample - loss: 0.8066 - acc: 0.6404 - val_loss: 0.7926 - val_acc: 0.6463\n",
      "Epoch 147/1000\n",
      "5968/5968 [==============================] - 3s 525us/sample - loss: 0.8042 - acc: 0.6394 - val_loss: 0.7966 - val_acc: 0.6457\n",
      "Epoch 148/1000\n",
      "5968/5968 [==============================] - 3s 558us/sample - loss: 0.8035 - acc: 0.6402 - val_loss: 0.7939 - val_acc: 0.6457\n",
      "Epoch 149/1000\n",
      "5968/5968 [==============================] - 3s 542us/sample - loss: 0.8038 - acc: 0.6406 - val_loss: 0.7941 - val_acc: 0.6457\n",
      "Epoch 150/1000\n",
      "5968/5968 [==============================] - 3s 529us/sample - loss: 0.8076 - acc: 0.6401 - val_loss: 0.7984 - val_acc: 0.6463\n",
      "Epoch 151/1000\n",
      "5968/5968 [==============================] - 3s 528us/sample - loss: 0.8048 - acc: 0.6394 - val_loss: 0.7935 - val_acc: 0.6470\n",
      "Epoch 152/1000\n",
      "5968/5968 [==============================] - 3s 532us/sample - loss: 0.8033 - acc: 0.6402 - val_loss: 0.7948 - val_acc: 0.6470\n",
      "Epoch 153/1000\n",
      "5968/5968 [==============================] - 3s 522us/sample - loss: 0.8020 - acc: 0.6402 - val_loss: 0.7962 - val_acc: 0.6463\n",
      "Epoch 154/1000\n",
      "5968/5968 [==============================] - 3s 537us/sample - loss: 0.8032 - acc: 0.6404 - val_loss: 0.7990 - val_acc: 0.6457\n",
      "Epoch 155/1000\n",
      "5968/5968 [==============================] - 3s 534us/sample - loss: 0.8026 - acc: 0.6408 - val_loss: 0.7927 - val_acc: 0.6463\n",
      "Epoch 156/1000\n",
      "5968/5968 [==============================] - 3s 544us/sample - loss: 0.7998 - acc: 0.6402 - val_loss: 0.7943 - val_acc: 0.6463\n",
      "Epoch 157/1000\n",
      "5968/5968 [==============================] - 3s 536us/sample - loss: 0.8055 - acc: 0.6408 - val_loss: 0.8005 - val_acc: 0.6470\n",
      "Epoch 158/1000\n",
      "5968/5968 [==============================] - 3s 545us/sample - loss: 0.8025 - acc: 0.6408 - val_loss: 0.7959 - val_acc: 0.6470\n",
      "Epoch 159/1000\n",
      "5968/5968 [==============================] - 3s 523us/sample - loss: 0.8020 - acc: 0.6413 - val_loss: 0.7940 - val_acc: 0.6463\n",
      "Epoch 160/1000\n",
      "5968/5968 [==============================] - 3s 535us/sample - loss: 0.8007 - acc: 0.6408 - val_loss: 0.7972 - val_acc: 0.6470\n",
      "Epoch 161/1000\n",
      "5968/5968 [==============================] - 3s 537us/sample - loss: 0.7980 - acc: 0.6413 - val_loss: 0.7953 - val_acc: 0.6470\n",
      "Epoch 162/1000\n",
      "5968/5968 [==============================] - 3s 524us/sample - loss: 0.7973 - acc: 0.6402 - val_loss: 0.7937 - val_acc: 0.6470\n",
      "Epoch 163/1000\n",
      "5968/5968 [==============================] - 3s 532us/sample - loss: 0.7976 - acc: 0.6414 - val_loss: 0.7968 - val_acc: 0.6470\n",
      "Epoch 164/1000\n",
      "5968/5968 [==============================] - 3s 523us/sample - loss: 0.7968 - acc: 0.6408 - val_loss: 0.7924 - val_acc: 0.6470\n",
      "Epoch 165/1000\n",
      "5968/5968 [==============================] - 3s 523us/sample - loss: 0.7967 - acc: 0.6406 - val_loss: 0.7984 - val_acc: 0.6463\n",
      "Epoch 166/1000\n",
      "5968/5968 [==============================] - 3s 528us/sample - loss: 0.7971 - acc: 0.6418 - val_loss: 0.7919 - val_acc: 0.6477\n",
      "Epoch 167/1000\n",
      "5968/5968 [==============================] - 3s 526us/sample - loss: 0.7997 - acc: 0.6414 - val_loss: 0.7911 - val_acc: 0.6484\n",
      "Epoch 168/1000\n",
      "5968/5968 [==============================] - 3s 525us/sample - loss: 0.7953 - acc: 0.6416 - val_loss: 0.7943 - val_acc: 0.6477\n",
      "Epoch 169/1000\n",
      "5968/5968 [==============================] - 3s 521us/sample - loss: 0.7947 - acc: 0.6421 - val_loss: 0.7891 - val_acc: 0.6484\n",
      "Epoch 170/1000\n",
      "5968/5968 [==============================] - 3s 526us/sample - loss: 0.7950 - acc: 0.6418 - val_loss: 0.7889 - val_acc: 0.6470\n",
      "Epoch 171/1000\n",
      "5968/5968 [==============================] - 3s 529us/sample - loss: 0.7929 - acc: 0.6424 - val_loss: 0.7910 - val_acc: 0.6470\n",
      "Epoch 172/1000\n",
      "5968/5968 [==============================] - 3s 529us/sample - loss: 0.7915 - acc: 0.6423 - val_loss: 0.7987 - val_acc: 0.6463\n",
      "Epoch 173/1000\n",
      "5968/5968 [==============================] - 3s 520us/sample - loss: 0.7906 - acc: 0.6434 - val_loss: 0.7990 - val_acc: 0.6470\n",
      "Epoch 174/1000\n",
      "5968/5968 [==============================] - 3s 521us/sample - loss: 0.7884 - acc: 0.6436 - val_loss: 0.7871 - val_acc: 0.6477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/1000\n",
      "5968/5968 [==============================] - 3s 518us/sample - loss: 0.7936 - acc: 0.6423 - val_loss: 0.8012 - val_acc: 0.6470\n",
      "Epoch 176/1000\n",
      "5968/5968 [==============================] - 3s 527us/sample - loss: 0.7969 - acc: 0.6419 - val_loss: 0.7989 - val_acc: 0.6470\n",
      "Epoch 177/1000\n",
      "5968/5968 [==============================] - 3s 530us/sample - loss: 0.7939 - acc: 0.6421 - val_loss: 0.7894 - val_acc: 0.6463\n",
      "Epoch 178/1000\n",
      "5968/5968 [==============================] - 3s 522us/sample - loss: 0.7912 - acc: 0.6444 - val_loss: 0.7885 - val_acc: 0.6490\n",
      "Epoch 179/1000\n",
      "5968/5968 [==============================] - 3s 521us/sample - loss: 0.7888 - acc: 0.6429 - val_loss: 0.7855 - val_acc: 0.6463\n",
      "Epoch 180/1000\n",
      "5968/5968 [==============================] - 3s 531us/sample - loss: 0.7857 - acc: 0.6443 - val_loss: 0.7925 - val_acc: 0.6497\n",
      "Epoch 181/1000\n",
      "5968/5968 [==============================] - 3s 539us/sample - loss: 0.7917 - acc: 0.6414 - val_loss: 0.7845 - val_acc: 0.6490\n",
      "Epoch 182/1000\n",
      "5968/5968 [==============================] - 3s 530us/sample - loss: 0.7851 - acc: 0.6444 - val_loss: 0.7944 - val_acc: 0.6450\n",
      "Epoch 183/1000\n",
      "5968/5968 [==============================] - 3s 523us/sample - loss: 0.7845 - acc: 0.6433 - val_loss: 0.7936 - val_acc: 0.6504\n",
      "Epoch 184/1000\n",
      "5968/5968 [==============================] - 3s 536us/sample - loss: 0.8062 - acc: 0.6392 - val_loss: 0.8009 - val_acc: 0.6457\n",
      "Epoch 185/1000\n",
      "5968/5968 [==============================] - 3s 476us/sample - loss: 0.8000 - acc: 0.6411 - val_loss: 0.7892 - val_acc: 0.6470\n",
      "Epoch 186/1000\n",
      "5968/5968 [==============================] - 3s 483us/sample - loss: 0.7951 - acc: 0.6413 - val_loss: 0.7924 - val_acc: 0.6443\n",
      "Epoch 187/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7936 - acc: 0.6439 - val_loss: 0.7923 - val_acc: 0.6490\n",
      "Epoch 188/1000\n",
      "5968/5968 [==============================] - 3s 485us/sample - loss: 0.7921 - acc: 0.6449 - val_loss: 0.7937 - val_acc: 0.6470\n",
      "Epoch 189/1000\n",
      "5968/5968 [==============================] - 3s 474us/sample - loss: 0.7932 - acc: 0.6423 - val_loss: 0.7917 - val_acc: 0.6477\n",
      "Epoch 190/1000\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.7917 - acc: 0.6449 - val_loss: 0.7918 - val_acc: 0.6510\n",
      "Epoch 191/1000\n",
      "5968/5968 [==============================] - 3s 474us/sample - loss: 0.7915 - acc: 0.6436 - val_loss: 0.7964 - val_acc: 0.6470\n",
      "Epoch 192/1000\n",
      "5968/5968 [==============================] - 3s 474us/sample - loss: 0.7901 - acc: 0.6443 - val_loss: 0.7916 - val_acc: 0.6457\n",
      "Epoch 193/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7890 - acc: 0.6443 - val_loss: 0.7886 - val_acc: 0.6470\n",
      "Epoch 194/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7881 - acc: 0.6434 - val_loss: 0.7871 - val_acc: 0.6484\n",
      "Epoch 195/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7846 - acc: 0.6458 - val_loss: 0.7885 - val_acc: 0.6477\n",
      "Epoch 196/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7851 - acc: 0.6464 - val_loss: 0.7880 - val_acc: 0.6497\n",
      "Epoch 197/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7849 - acc: 0.6456 - val_loss: 0.7866 - val_acc: 0.6463\n",
      "Epoch 198/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7846 - acc: 0.6448 - val_loss: 0.7855 - val_acc: 0.6470\n",
      "Epoch 199/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7844 - acc: 0.6443 - val_loss: 0.7854 - val_acc: 0.6497\n",
      "Epoch 200/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7834 - acc: 0.6458 - val_loss: 0.7925 - val_acc: 0.6484\n",
      "Epoch 201/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7834 - acc: 0.6470 - val_loss: 0.7897 - val_acc: 0.6463\n",
      "Epoch 202/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7829 - acc: 0.6451 - val_loss: 0.7822 - val_acc: 0.6504\n",
      "Epoch 203/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7811 - acc: 0.6453 - val_loss: 0.7803 - val_acc: 0.6497\n",
      "Epoch 204/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.8203 - acc: 0.6404 - val_loss: 0.8066 - val_acc: 0.6484\n",
      "Epoch 205/1000\n",
      "5968/5968 [==============================] - 3s 456us/sample - loss: 0.8007 - acc: 0.6413 - val_loss: 0.7909 - val_acc: 0.6477\n",
      "Epoch 206/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7854 - acc: 0.6451 - val_loss: 0.7898 - val_acc: 0.6470\n",
      "Epoch 207/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7886 - acc: 0.6449 - val_loss: 0.7928 - val_acc: 0.6484\n",
      "Epoch 208/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7886 - acc: 0.6451 - val_loss: 0.7944 - val_acc: 0.6484\n",
      "Epoch 209/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7876 - acc: 0.6441 - val_loss: 0.7989 - val_acc: 0.6463\n",
      "Epoch 210/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7915 - acc: 0.6429 - val_loss: 0.7869 - val_acc: 0.6484\n",
      "Epoch 211/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7820 - acc: 0.6468 - val_loss: 0.7835 - val_acc: 0.6457\n",
      "Epoch 212/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7818 - acc: 0.6459 - val_loss: 0.7841 - val_acc: 0.6484\n",
      "Epoch 213/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7857 - acc: 0.6486 - val_loss: 0.7776 - val_acc: 0.6497\n",
      "Epoch 214/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7809 - acc: 0.6458 - val_loss: 0.8043 - val_acc: 0.6457\n",
      "Epoch 215/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7830 - acc: 0.6468 - val_loss: 0.7951 - val_acc: 0.6463\n",
      "Epoch 216/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7851 - acc: 0.6441 - val_loss: 0.7885 - val_acc: 0.6524\n",
      "Epoch 217/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7812 - acc: 0.6433 - val_loss: 0.7951 - val_acc: 0.6510\n",
      "Epoch 218/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7812 - acc: 0.6466 - val_loss: 0.7914 - val_acc: 0.6504\n",
      "Epoch 218: early stopping\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding으로 fit\n",
    "single_lstm_model_ohe = KerasClassifier(build_fn = lstm_ohe\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])\n",
    "single_lstm_model_ohe.fit(X3_train_ohe, y3_train_ohe)\n",
    "y3_pred_single_lstm = single_lstm_model_ohe.predict(X3_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f79c271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accouracy :  0.6382636655948553\n"
     ]
    }
   ],
   "source": [
    "print(\"accouracy : \", accuracy_score(y3_pred_single_lstm,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27440f68",
   "metadata": {},
   "source": [
    "### Single Layer LSTM (Label encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "138eaaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_label():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape = inputs, return_sequences = False))\n",
    "    model.add(Dense(outputs))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17b23630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5968 samples, validate on 1493 samples\n",
      "Epoch 1/1000\n",
      "5968/5968 [==============================] - 3s 484us/sample - loss: 0.9862 - acc: 0.6379 - val_loss: 0.8349 - val_acc: 0.6450\n",
      "Epoch 2/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.8302 - acc: 0.6381 - val_loss: 0.8124 - val_acc: 0.6457\n",
      "Epoch 3/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.8171 - acc: 0.6361 - val_loss: 0.8071 - val_acc: 0.6457\n",
      "Epoch 4/1000\n",
      "5968/5968 [==============================] - 3s 473us/sample - loss: 0.8157 - acc: 0.6362 - val_loss: 0.8068 - val_acc: 0.6463\n",
      "Epoch 5/1000\n",
      "5968/5968 [==============================] - 3s 482us/sample - loss: 0.8156 - acc: 0.6374 - val_loss: 0.8045 - val_acc: 0.6463\n",
      "Epoch 6/1000\n",
      "5968/5968 [==============================] - 3s 492us/sample - loss: 0.8137 - acc: 0.6371 - val_loss: 0.8030 - val_acc: 0.6477\n",
      "Epoch 7/1000\n",
      "5968/5968 [==============================] - 3s 481us/sample - loss: 0.8136 - acc: 0.6387 - val_loss: 0.8055 - val_acc: 0.6470\n",
      "Epoch 8/1000\n",
      "5968/5968 [==============================] - 3s 482us/sample - loss: 0.8115 - acc: 0.6379 - val_loss: 0.8018 - val_acc: 0.6463\n",
      "Epoch 9/1000\n",
      "5968/5968 [==============================] - 3s 483us/sample - loss: 0.8107 - acc: 0.6382 - val_loss: 0.7976 - val_acc: 0.6470\n",
      "Epoch 10/1000\n",
      "5968/5968 [==============================] - 3s 479us/sample - loss: 0.8090 - acc: 0.6391 - val_loss: 0.7980 - val_acc: 0.6443\n",
      "Epoch 11/1000\n",
      "5968/5968 [==============================] - 3s 478us/sample - loss: 0.8100 - acc: 0.6379 - val_loss: 0.8166 - val_acc: 0.6450\n",
      "Epoch 12/1000\n",
      "5968/5968 [==============================] - 3s 473us/sample - loss: 0.8148 - acc: 0.6387 - val_loss: 0.7956 - val_acc: 0.6463\n",
      "Epoch 13/1000\n",
      "5968/5968 [==============================] - 3s 478us/sample - loss: 0.8070 - acc: 0.6372 - val_loss: 0.7937 - val_acc: 0.6443\n",
      "Epoch 14/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.8073 - acc: 0.6384 - val_loss: 0.8025 - val_acc: 0.6450\n",
      "Epoch 15/1000\n",
      "5968/5968 [==============================] - 3s 474us/sample - loss: 0.8077 - acc: 0.6387 - val_loss: 0.7951 - val_acc: 0.6450\n",
      "Epoch 16/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.8041 - acc: 0.6382 - val_loss: 0.7916 - val_acc: 0.6450\n",
      "Epoch 17/1000\n",
      "5968/5968 [==============================] - 3s 472us/sample - loss: 0.8051 - acc: 0.6382 - val_loss: 0.7972 - val_acc: 0.6450\n",
      "Epoch 18/1000\n",
      "5968/5968 [==============================] - 3s 480us/sample - loss: 0.8069 - acc: 0.6394 - val_loss: 0.8029 - val_acc: 0.6457\n",
      "Epoch 19/1000\n",
      "5968/5968 [==============================] - 3s 477us/sample - loss: 0.8116 - acc: 0.6372 - val_loss: 0.8004 - val_acc: 0.6450\n",
      "Epoch 20/1000\n",
      "5968/5968 [==============================] - 3s 472us/sample - loss: 0.8050 - acc: 0.6386 - val_loss: 0.7920 - val_acc: 0.6457\n",
      "Epoch 21/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.8084 - acc: 0.6387 - val_loss: 0.7951 - val_acc: 0.6450\n",
      "Epoch 22/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.8043 - acc: 0.6374 - val_loss: 0.8035 - val_acc: 0.6450\n",
      "Epoch 23/1000\n",
      "5968/5968 [==============================] - 3s 474us/sample - loss: 0.8050 - acc: 0.6384 - val_loss: 0.7948 - val_acc: 0.6457\n",
      "Epoch 24/1000\n",
      "5968/5968 [==============================] - 3s 475us/sample - loss: 0.8031 - acc: 0.6382 - val_loss: 0.7937 - val_acc: 0.6450\n",
      "Epoch 25/1000\n",
      "5968/5968 [==============================] - 3s 476us/sample - loss: 0.8037 - acc: 0.6379 - val_loss: 0.7951 - val_acc: 0.6443\n",
      "Epoch 26/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.8032 - acc: 0.6386 - val_loss: 0.7928 - val_acc: 0.6450\n",
      "Epoch 27/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.8021 - acc: 0.6392 - val_loss: 0.8005 - val_acc: 0.6443\n",
      "Epoch 28/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.8045 - acc: 0.6391 - val_loss: 0.7933 - val_acc: 0.6457\n",
      "Epoch 29/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.8018 - acc: 0.6387 - val_loss: 0.7933 - val_acc: 0.6457\n",
      "Epoch 30/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.8020 - acc: 0.6387 - val_loss: 0.7904 - val_acc: 0.6450\n",
      "Epoch 31/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.8025 - acc: 0.6386 - val_loss: 0.8011 - val_acc: 0.6450\n",
      "Epoch 32/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.8029 - acc: 0.6387 - val_loss: 0.7921 - val_acc: 0.6463\n",
      "Epoch 33/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.8045 - acc: 0.6386 - val_loss: 0.7979 - val_acc: 0.6457\n",
      "Epoch 34/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.8042 - acc: 0.6396 - val_loss: 0.8033 - val_acc: 0.6463\n",
      "Epoch 35/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.8016 - acc: 0.6404 - val_loss: 0.7938 - val_acc: 0.6463\n",
      "Epoch 36/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7992 - acc: 0.6402 - val_loss: 0.7935 - val_acc: 0.6443\n",
      "Epoch 37/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7986 - acc: 0.6379 - val_loss: 0.7940 - val_acc: 0.6463\n",
      "Epoch 38/1000\n",
      "5968/5968 [==============================] - 3s 479us/sample - loss: 0.7988 - acc: 0.6387 - val_loss: 0.7957 - val_acc: 0.6477\n",
      "Epoch 39/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7963 - acc: 0.6406 - val_loss: 0.8072 - val_acc: 0.6443\n",
      "Epoch 40/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.8026 - acc: 0.6404 - val_loss: 0.7929 - val_acc: 0.6463\n",
      "Epoch 41/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7978 - acc: 0.6406 - val_loss: 0.7946 - val_acc: 0.6463\n",
      "Epoch 42/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7964 - acc: 0.6402 - val_loss: 0.7928 - val_acc: 0.6484\n",
      "Epoch 43/1000\n",
      "5968/5968 [==============================] - 3s 472us/sample - loss: 0.7988 - acc: 0.6397 - val_loss: 0.7916 - val_acc: 0.6457\n",
      "Epoch 44/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7947 - acc: 0.6414 - val_loss: 0.8005 - val_acc: 0.6463\n",
      "Epoch 45/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7951 - acc: 0.6402 - val_loss: 0.8032 - val_acc: 0.6450\n",
      "Epoch 46/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7960 - acc: 0.6406 - val_loss: 0.8004 - val_acc: 0.6477\n",
      "Epoch 47/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7982 - acc: 0.6411 - val_loss: 0.7929 - val_acc: 0.6457\n",
      "Epoch 48/1000\n",
      "5968/5968 [==============================] - 3s 475us/sample - loss: 0.7924 - acc: 0.6411 - val_loss: 0.8032 - val_acc: 0.6437\n",
      "Epoch 49/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7951 - acc: 0.6399 - val_loss: 0.7924 - val_acc: 0.6450\n",
      "Epoch 50/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7914 - acc: 0.6408 - val_loss: 0.7983 - val_acc: 0.6443\n",
      "Epoch 51/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7944 - acc: 0.6401 - val_loss: 0.7899 - val_acc: 0.6437\n",
      "Epoch 52/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7898 - acc: 0.6399 - val_loss: 0.7908 - val_acc: 0.6470\n",
      "Epoch 53/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7948 - acc: 0.6408 - val_loss: 0.7941 - val_acc: 0.6490\n",
      "Epoch 54/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7932 - acc: 0.6406 - val_loss: 0.7938 - val_acc: 0.6457\n",
      "Epoch 55/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7909 - acc: 0.6416 - val_loss: 0.7924 - val_acc: 0.6477\n",
      "Epoch 56/1000\n",
      "5968/5968 [==============================] - 3s 472us/sample - loss: 0.7888 - acc: 0.6409 - val_loss: 0.7925 - val_acc: 0.6470\n",
      "Epoch 57/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7879 - acc: 0.6421 - val_loss: 0.7888 - val_acc: 0.6497\n",
      "Epoch 58/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7933 - acc: 0.6399 - val_loss: 0.7915 - val_acc: 0.6484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7951 - acc: 0.6409 - val_loss: 0.7881 - val_acc: 0.6484\n",
      "Epoch 60/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7907 - acc: 0.6416 - val_loss: 0.7915 - val_acc: 0.6477\n",
      "Epoch 61/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7897 - acc: 0.6419 - val_loss: 0.7899 - val_acc: 0.6477\n",
      "Epoch 62/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7897 - acc: 0.6416 - val_loss: 0.7898 - val_acc: 0.6477\n",
      "Epoch 63/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7882 - acc: 0.6418 - val_loss: 0.7905 - val_acc: 0.6537\n",
      "Epoch 64/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7934 - acc: 0.6404 - val_loss: 0.7937 - val_acc: 0.6517\n",
      "Epoch 65/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7884 - acc: 0.6426 - val_loss: 0.7887 - val_acc: 0.6504\n",
      "Epoch 66/1000\n",
      "5968/5968 [==============================] - 3s 474us/sample - loss: 0.7863 - acc: 0.6416 - val_loss: 0.8021 - val_acc: 0.6450\n",
      "Epoch 67/1000\n",
      "5968/5968 [==============================] - 3s 473us/sample - loss: 0.7944 - acc: 0.6418 - val_loss: 0.7966 - val_acc: 0.6477\n",
      "Epoch 68/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7891 - acc: 0.6419 - val_loss: 0.7900 - val_acc: 0.6490\n",
      "Epoch 69/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7919 - acc: 0.6431 - val_loss: 0.8182 - val_acc: 0.6309\n",
      "Epoch 70/1000\n",
      "5968/5968 [==============================] - 3s 459us/sample - loss: 0.7990 - acc: 0.6371 - val_loss: 0.7934 - val_acc: 0.6497\n",
      "Epoch 71/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7929 - acc: 0.6413 - val_loss: 0.7977 - val_acc: 0.6470\n",
      "Epoch 72/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7911 - acc: 0.6414 - val_loss: 0.7923 - val_acc: 0.6504\n",
      "Epoch 73/1000\n",
      "5968/5968 [==============================] - 3s 455us/sample - loss: 0.7871 - acc: 0.6424 - val_loss: 0.7858 - val_acc: 0.6504\n",
      "Epoch 74/1000\n",
      "5968/5968 [==============================] - 3s 456us/sample - loss: 0.7878 - acc: 0.6413 - val_loss: 0.7891 - val_acc: 0.6463\n",
      "Epoch 75/1000\n",
      "5968/5968 [==============================] - 3s 451us/sample - loss: 0.7835 - acc: 0.6426 - val_loss: 0.7898 - val_acc: 0.6497\n",
      "Epoch 76/1000\n",
      "5968/5968 [==============================] - 3s 453us/sample - loss: 0.7823 - acc: 0.6443 - val_loss: 0.7899 - val_acc: 0.6463\n",
      "Epoch 77/1000\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.7797 - acc: 0.6426 - val_loss: 0.7847 - val_acc: 0.6477\n",
      "Epoch 78/1000\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.7848 - acc: 0.6416 - val_loss: 0.7812 - val_acc: 0.6510\n",
      "Epoch 79/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7854 - acc: 0.6429 - val_loss: 0.7845 - val_acc: 0.6490\n",
      "Epoch 80/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7801 - acc: 0.6429 - val_loss: 0.7844 - val_acc: 0.6470\n",
      "Epoch 81/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7796 - acc: 0.6428 - val_loss: 0.7791 - val_acc: 0.6497\n",
      "Epoch 82/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7849 - acc: 0.6424 - val_loss: 0.7814 - val_acc: 0.6497\n",
      "Epoch 83/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7793 - acc: 0.6421 - val_loss: 0.7793 - val_acc: 0.6504\n",
      "Epoch 84/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7825 - acc: 0.6431 - val_loss: 0.7803 - val_acc: 0.6510\n",
      "Epoch 85/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7810 - acc: 0.6441 - val_loss: 0.7768 - val_acc: 0.6497\n",
      "Epoch 86/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7992 - acc: 0.6381 - val_loss: 0.8063 - val_acc: 0.6504\n",
      "Epoch 87/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.8090 - acc: 0.6404 - val_loss: 0.7935 - val_acc: 0.6497\n",
      "Epoch 88/1000\n",
      "5968/5968 [==============================] - 3s 458us/sample - loss: 0.8019 - acc: 0.6418 - val_loss: 0.7909 - val_acc: 0.6497\n",
      "Epoch 89/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7954 - acc: 0.6428 - val_loss: 0.7818 - val_acc: 0.6504\n",
      "Epoch 90/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7872 - acc: 0.6409 - val_loss: 0.7858 - val_acc: 0.6517\n",
      "Epoch 91/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7816 - acc: 0.6426 - val_loss: 0.7787 - val_acc: 0.6530\n",
      "Epoch 92/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7911 - acc: 0.6406 - val_loss: 0.7963 - val_acc: 0.6510\n",
      "Epoch 93/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7851 - acc: 0.6421 - val_loss: 0.7813 - val_acc: 0.6484\n",
      "Epoch 94/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7833 - acc: 0.6429 - val_loss: 0.7858 - val_acc: 0.6524\n",
      "Epoch 95/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7824 - acc: 0.6426 - val_loss: 0.7798 - val_acc: 0.6530\n",
      "Epoch 96/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7762 - acc: 0.6441 - val_loss: 0.7750 - val_acc: 0.6490\n",
      "Epoch 97/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7865 - acc: 0.6428 - val_loss: 0.7823 - val_acc: 0.6504\n",
      "Epoch 98/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7793 - acc: 0.6419 - val_loss: 0.7794 - val_acc: 0.6484\n",
      "Epoch 99/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7917 - acc: 0.6416 - val_loss: 0.7912 - val_acc: 0.6504\n",
      "Epoch 100/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7930 - acc: 0.6404 - val_loss: 0.7904 - val_acc: 0.6497\n",
      "Epoch 101/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7885 - acc: 0.6411 - val_loss: 0.7871 - val_acc: 0.6504\n",
      "Epoch 102/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7804 - acc: 0.6449 - val_loss: 0.8335 - val_acc: 0.6463\n",
      "Epoch 103/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.8009 - acc: 0.6391 - val_loss: 0.7892 - val_acc: 0.6470\n",
      "Epoch 104/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7946 - acc: 0.6413 - val_loss: 0.7841 - val_acc: 0.6504\n",
      "Epoch 105/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7875 - acc: 0.6421 - val_loss: 0.7866 - val_acc: 0.6510\n",
      "Epoch 106/1000\n",
      "5968/5968 [==============================] - 3s 473us/sample - loss: 0.7896 - acc: 0.6411 - val_loss: 0.7845 - val_acc: 0.6510\n",
      "Epoch 107/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7867 - acc: 0.6429 - val_loss: 0.7876 - val_acc: 0.6517\n",
      "Epoch 108/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7834 - acc: 0.6421 - val_loss: 0.7867 - val_acc: 0.6490\n",
      "Epoch 109/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7847 - acc: 0.6429 - val_loss: 0.7857 - val_acc: 0.6504\n",
      "Epoch 110/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7852 - acc: 0.6428 - val_loss: 0.7823 - val_acc: 0.6510\n",
      "Epoch 111/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7851 - acc: 0.6428 - val_loss: 0.7869 - val_acc: 0.6517\n",
      "Epoch 112/1000\n",
      "5968/5968 [==============================] - 3s 473us/sample - loss: 0.7836 - acc: 0.6416 - val_loss: 0.7836 - val_acc: 0.6504\n",
      "Epoch 113/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7812 - acc: 0.6426 - val_loss: 0.7904 - val_acc: 0.6477\n",
      "Epoch 114/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7819 - acc: 0.6426 - val_loss: 0.7827 - val_acc: 0.6504\n",
      "Epoch 115/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7785 - acc: 0.6441 - val_loss: 0.7790 - val_acc: 0.6517\n",
      "Epoch 116/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7750 - acc: 0.6438 - val_loss: 0.7826 - val_acc: 0.6504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7797 - acc: 0.6434 - val_loss: 0.7813 - val_acc: 0.6490\n",
      "Epoch 118/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7770 - acc: 0.6446 - val_loss: 0.7816 - val_acc: 0.6510\n",
      "Epoch 119/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7765 - acc: 0.6431 - val_loss: 0.7803 - val_acc: 0.6497\n",
      "Epoch 120/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7737 - acc: 0.6459 - val_loss: 0.7779 - val_acc: 0.6477\n",
      "Epoch 121/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7707 - acc: 0.6471 - val_loss: 0.7805 - val_acc: 0.6370\n",
      "Epoch 122/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7702 - acc: 0.6439 - val_loss: 0.7793 - val_acc: 0.6470\n",
      "Epoch 123/1000\n",
      "5968/5968 [==============================] - 3s 458us/sample - loss: 0.7772 - acc: 0.6456 - val_loss: 0.7775 - val_acc: 0.6517\n",
      "Epoch 124/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7735 - acc: 0.6446 - val_loss: 0.7807 - val_acc: 0.6510\n",
      "Epoch 125/1000\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.7684 - acc: 0.6434 - val_loss: 0.7719 - val_acc: 0.6530\n",
      "Epoch 126/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7738 - acc: 0.6434 - val_loss: 0.7797 - val_acc: 0.6517\n",
      "Epoch 127/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7648 - acc: 0.6464 - val_loss: 0.7756 - val_acc: 0.6530\n",
      "Epoch 128/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7730 - acc: 0.6351 - val_loss: 0.7738 - val_acc: 0.6510\n",
      "Epoch 129/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7709 - acc: 0.6456 - val_loss: 0.7758 - val_acc: 0.6524\n",
      "Epoch 130/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7611 - acc: 0.6470 - val_loss: 0.7720 - val_acc: 0.6524\n",
      "Epoch 131/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7690 - acc: 0.6438 - val_loss: 0.7785 - val_acc: 0.6497\n",
      "Epoch 132/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7663 - acc: 0.6463 - val_loss: 0.7779 - val_acc: 0.6504\n",
      "Epoch 133/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7724 - acc: 0.6475 - val_loss: 0.7765 - val_acc: 0.6490\n",
      "Epoch 134/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7675 - acc: 0.6456 - val_loss: 0.7735 - val_acc: 0.6517\n",
      "Epoch 135/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7691 - acc: 0.6441 - val_loss: 0.7807 - val_acc: 0.6437\n",
      "Epoch 136/1000\n",
      "5968/5968 [==============================] - 3s 476us/sample - loss: 0.7654 - acc: 0.6446 - val_loss: 0.7692 - val_acc: 0.6537\n",
      "Epoch 137/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7609 - acc: 0.6473 - val_loss: 0.7611 - val_acc: 0.6544\n",
      "Epoch 138/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7677 - acc: 0.6424 - val_loss: 0.7820 - val_acc: 0.6497\n",
      "Epoch 139/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7695 - acc: 0.6439 - val_loss: 0.7646 - val_acc: 0.6537\n",
      "Epoch 140/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7703 - acc: 0.6453 - val_loss: 0.7821 - val_acc: 0.6517\n",
      "Epoch 141/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7666 - acc: 0.6431 - val_loss: 0.7686 - val_acc: 0.6571\n",
      "Epoch 142/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7727 - acc: 0.6471 - val_loss: 0.7740 - val_acc: 0.6510\n",
      "Epoch 143/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7602 - acc: 0.6374 - val_loss: 0.7665 - val_acc: 0.6504\n",
      "Epoch 144/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7631 - acc: 0.6436 - val_loss: 0.7687 - val_acc: 0.6490\n",
      "Epoch 145/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7659 - acc: 0.6436 - val_loss: 0.7679 - val_acc: 0.6537\n",
      "Epoch 146/1000\n",
      "5968/5968 [==============================] - 3s 475us/sample - loss: 0.7594 - acc: 0.6454 - val_loss: 0.7636 - val_acc: 0.6524\n",
      "Epoch 147/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7558 - acc: 0.6476 - val_loss: 0.7673 - val_acc: 0.6571\n",
      "Epoch 148/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7620 - acc: 0.6433 - val_loss: 0.7863 - val_acc: 0.6510\n",
      "Epoch 149/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7576 - acc: 0.6459 - val_loss: 0.7699 - val_acc: 0.6524\n",
      "Epoch 150/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7520 - acc: 0.6486 - val_loss: 0.7680 - val_acc: 0.6530\n",
      "Epoch 151/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7534 - acc: 0.6463 - val_loss: 0.7714 - val_acc: 0.6530\n",
      "Epoch 152/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7546 - acc: 0.6476 - val_loss: 0.7838 - val_acc: 0.6517\n",
      "Epoch 153/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7712 - acc: 0.6449 - val_loss: 0.7807 - val_acc: 0.6504\n",
      "Epoch 154/1000\n",
      "5968/5968 [==============================] - 3s 458us/sample - loss: 0.7655 - acc: 0.6458 - val_loss: 0.7634 - val_acc: 0.6530\n",
      "Epoch 155/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7521 - acc: 0.6495 - val_loss: 0.7825 - val_acc: 0.6450\n",
      "Epoch 156/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7579 - acc: 0.6463 - val_loss: 0.7699 - val_acc: 0.6551\n",
      "Epoch 157/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7648 - acc: 0.6468 - val_loss: 0.7660 - val_acc: 0.6530\n",
      "Epoch 158/1000\n",
      "5968/5968 [==============================] - 3s 458us/sample - loss: 0.7566 - acc: 0.6486 - val_loss: 0.7648 - val_acc: 0.6510\n",
      "Epoch 159/1000\n",
      "5968/5968 [==============================] - 3s 455us/sample - loss: 0.7594 - acc: 0.6463 - val_loss: 0.7596 - val_acc: 0.6537\n",
      "Epoch 160/1000\n",
      "5968/5968 [==============================] - 3s 458us/sample - loss: 0.7543 - acc: 0.6470 - val_loss: 0.7670 - val_acc: 0.6497\n",
      "Epoch 161/1000\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.7516 - acc: 0.6480 - val_loss: 0.7635 - val_acc: 0.6504\n",
      "Epoch 162/1000\n",
      "5968/5968 [==============================] - 3s 458us/sample - loss: 0.7521 - acc: 0.6478 - val_loss: 0.7642 - val_acc: 0.6504\n",
      "Epoch 163/1000\n",
      "5968/5968 [==============================] - 3s 456us/sample - loss: 0.7546 - acc: 0.6456 - val_loss: 0.7612 - val_acc: 0.6510\n",
      "Epoch 164/1000\n",
      "5968/5968 [==============================] - 3s 453us/sample - loss: 0.7449 - acc: 0.6508 - val_loss: 0.7609 - val_acc: 0.6544\n",
      "Epoch 165/1000\n",
      "5968/5968 [==============================] - 3s 459us/sample - loss: 0.7494 - acc: 0.6501 - val_loss: 0.7779 - val_acc: 0.6510\n",
      "Epoch 166/1000\n",
      "5968/5968 [==============================] - 3s 501us/sample - loss: 0.7529 - acc: 0.6496 - val_loss: 0.7710 - val_acc: 0.6484\n",
      "Epoch 167/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7538 - acc: 0.6483 - val_loss: 0.7719 - val_acc: 0.6510\n",
      "Epoch 168/1000\n",
      "5968/5968 [==============================] - 3s 456us/sample - loss: 0.7552 - acc: 0.6503 - val_loss: 0.7700 - val_acc: 0.6410\n",
      "Epoch 169/1000\n",
      "5968/5968 [==============================] - 3s 454us/sample - loss: 0.7516 - acc: 0.6471 - val_loss: 0.7605 - val_acc: 0.6544\n",
      "Epoch 170/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7491 - acc: 0.6490 - val_loss: 0.7653 - val_acc: 0.6510\n",
      "Epoch 171/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7579 - acc: 0.6448 - val_loss: 0.7621 - val_acc: 0.6497\n",
      "Epoch 172/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7533 - acc: 0.6454 - val_loss: 0.7686 - val_acc: 0.6530\n",
      "Epoch 173/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7528 - acc: 0.6500 - val_loss: 0.7634 - val_acc: 0.6537\n",
      "Epoch 174/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7453 - acc: 0.6475 - val_loss: 0.7699 - val_acc: 0.6490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7617 - acc: 0.6476 - val_loss: 0.7675 - val_acc: 0.6510\n",
      "Epoch 176/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7547 - acc: 0.6478 - val_loss: 0.7712 - val_acc: 0.6490\n",
      "Epoch 177/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7540 - acc: 0.6478 - val_loss: 0.7648 - val_acc: 0.6510\n",
      "Epoch 178/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7507 - acc: 0.6508 - val_loss: 0.7614 - val_acc: 0.6497\n",
      "Epoch 179/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7451 - acc: 0.6532 - val_loss: 0.7694 - val_acc: 0.6510\n",
      "Epoch 180/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7565 - acc: 0.6471 - val_loss: 0.7693 - val_acc: 0.6497\n",
      "Epoch 181/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7487 - acc: 0.6508 - val_loss: 0.7710 - val_acc: 0.6490\n",
      "Epoch 182/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7502 - acc: 0.6480 - val_loss: 0.7734 - val_acc: 0.6537\n",
      "Epoch 183/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7475 - acc: 0.6503 - val_loss: 0.7642 - val_acc: 0.6477\n",
      "Epoch 184/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7426 - acc: 0.6516 - val_loss: 0.7622 - val_acc: 0.6504\n",
      "Epoch 185/1000\n",
      "5968/5968 [==============================] - 3s 472us/sample - loss: 0.7420 - acc: 0.6505 - val_loss: 0.7646 - val_acc: 0.6477\n",
      "Epoch 186/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7394 - acc: 0.6548 - val_loss: 0.7619 - val_acc: 0.6510\n",
      "Epoch 187/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7418 - acc: 0.6565 - val_loss: 0.7781 - val_acc: 0.6450\n",
      "Epoch 188/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7466 - acc: 0.6490 - val_loss: 0.7671 - val_acc: 0.6477\n",
      "Epoch 189/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7416 - acc: 0.6513 - val_loss: 0.7643 - val_acc: 0.6490\n",
      "Epoch 190/1000\n",
      "5968/5968 [==============================] - 3s 476us/sample - loss: 0.7538 - acc: 0.6470 - val_loss: 0.7718 - val_acc: 0.6530\n",
      "Epoch 191/1000\n",
      "5968/5968 [==============================] - 3s 473us/sample - loss: 0.7552 - acc: 0.6476 - val_loss: 0.7589 - val_acc: 0.6557\n",
      "Epoch 192/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7488 - acc: 0.6481 - val_loss: 0.7591 - val_acc: 0.6490\n",
      "Epoch 193/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7443 - acc: 0.6490 - val_loss: 0.7603 - val_acc: 0.6544\n",
      "Epoch 194/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7495 - acc: 0.6486 - val_loss: 0.7604 - val_acc: 0.6551\n",
      "Epoch 195/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7421 - acc: 0.6510 - val_loss: 0.7672 - val_acc: 0.6450\n",
      "Epoch 196/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7446 - acc: 0.6493 - val_loss: 0.7677 - val_acc: 0.6383\n",
      "Epoch 197/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7458 - acc: 0.6548 - val_loss: 0.7763 - val_acc: 0.6457\n",
      "Epoch 198/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7437 - acc: 0.6535 - val_loss: 0.7769 - val_acc: 0.6330\n",
      "Epoch 199/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7438 - acc: 0.6592 - val_loss: 0.7951 - val_acc: 0.5720\n",
      "Epoch 200/1000\n",
      "5968/5968 [==============================] - 3s 479us/sample - loss: 0.7526 - acc: 0.6401 - val_loss: 0.7672 - val_acc: 0.6477\n",
      "Epoch 201/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7367 - acc: 0.6535 - val_loss: 0.7777 - val_acc: 0.6510\n",
      "Epoch 202/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7404 - acc: 0.6444 - val_loss: 0.7704 - val_acc: 0.6463\n",
      "Epoch 203/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7400 - acc: 0.6543 - val_loss: 0.7646 - val_acc: 0.6430\n",
      "Epoch 204/1000\n",
      "5968/5968 [==============================] - 3s 459us/sample - loss: 0.7379 - acc: 0.6518 - val_loss: 0.7680 - val_acc: 0.6376\n",
      "Epoch 205/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7384 - acc: 0.6496 - val_loss: 0.7617 - val_acc: 0.6497\n",
      "Epoch 206/1000\n",
      "5968/5968 [==============================] - 3s 459us/sample - loss: 0.7307 - acc: 0.6567 - val_loss: 0.7652 - val_acc: 0.6457\n",
      "Epoch 207/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7306 - acc: 0.6602 - val_loss: 0.7622 - val_acc: 0.6477\n",
      "Epoch 208/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7367 - acc: 0.6503 - val_loss: 0.7752 - val_acc: 0.6470\n",
      "Epoch 209/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7386 - acc: 0.6525 - val_loss: 0.7743 - val_acc: 0.6544\n",
      "Epoch 210/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7398 - acc: 0.6518 - val_loss: 0.7800 - val_acc: 0.6303\n",
      "Epoch 211/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7372 - acc: 0.6572 - val_loss: 0.7648 - val_acc: 0.6484\n",
      "Epoch 212/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7366 - acc: 0.6516 - val_loss: 0.7701 - val_acc: 0.6437\n",
      "Epoch 213/1000\n",
      "5968/5968 [==============================] - 3s 474us/sample - loss: 0.7308 - acc: 0.6515 - val_loss: 0.7613 - val_acc: 0.6544\n",
      "Epoch 214/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7278 - acc: 0.6508 - val_loss: 0.7677 - val_acc: 0.6443\n",
      "Epoch 215/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7369 - acc: 0.6572 - val_loss: 0.7600 - val_acc: 0.6517\n",
      "Epoch 216/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7342 - acc: 0.6615 - val_loss: 0.7649 - val_acc: 0.6490\n",
      "Epoch 217/1000\n",
      "5968/5968 [==============================] - 3s 459us/sample - loss: 0.7291 - acc: 0.6568 - val_loss: 0.7764 - val_acc: 0.6256\n",
      "Epoch 218/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7393 - acc: 0.6572 - val_loss: 0.7701 - val_acc: 0.6484\n",
      "Epoch 219/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7334 - acc: 0.6553 - val_loss: 0.7727 - val_acc: 0.6450\n",
      "Epoch 220/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7355 - acc: 0.6548 - val_loss: 0.7773 - val_acc: 0.6229\n",
      "Epoch 221/1000\n",
      "5968/5968 [==============================] - 3s 456us/sample - loss: 0.7360 - acc: 0.6605 - val_loss: 0.7635 - val_acc: 0.6504\n",
      "Epoch 222/1000\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.7251 - acc: 0.6624 - val_loss: 0.7703 - val_acc: 0.6504\n",
      "Epoch 223/1000\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.7266 - acc: 0.6620 - val_loss: 0.7698 - val_acc: 0.6336\n",
      "Epoch 224/1000\n",
      "5968/5968 [==============================] - 3s 474us/sample - loss: 0.7364 - acc: 0.6461 - val_loss: 0.7685 - val_acc: 0.6497\n",
      "Epoch 225/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7382 - acc: 0.6480 - val_loss: 0.7739 - val_acc: 0.6316\n",
      "Epoch 226/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7389 - acc: 0.6520 - val_loss: 0.7669 - val_acc: 0.6437\n",
      "Epoch 227/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7268 - acc: 0.6609 - val_loss: 0.7684 - val_acc: 0.6651\n",
      "Epoch 228/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7441 - acc: 0.6506 - val_loss: 0.7661 - val_acc: 0.6430\n",
      "Epoch 229/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7375 - acc: 0.6547 - val_loss: 0.7713 - val_acc: 0.6477\n",
      "Epoch 230/1000\n",
      "5968/5968 [==============================] - 3s 468us/sample - loss: 0.7302 - acc: 0.6604 - val_loss: 0.7668 - val_acc: 0.6477\n",
      "Epoch 231/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7313 - acc: 0.6590 - val_loss: 0.7634 - val_acc: 0.6517\n",
      "Epoch 232/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7303 - acc: 0.6599 - val_loss: 0.7694 - val_acc: 0.6443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7379 - acc: 0.6475 - val_loss: 0.7790 - val_acc: 0.6423\n",
      "Epoch 234/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7430 - acc: 0.6523 - val_loss: 0.7720 - val_acc: 0.6504\n",
      "Epoch 235/1000\n",
      "5968/5968 [==============================] - 3s 472us/sample - loss: 0.7274 - acc: 0.6575 - val_loss: 0.7692 - val_acc: 0.6390\n",
      "Epoch 236/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7335 - acc: 0.6532 - val_loss: 0.7772 - val_acc: 0.6336\n",
      "Epoch 237/1000\n",
      "5968/5968 [==============================] - 3s 474us/sample - loss: 0.7303 - acc: 0.6547 - val_loss: 0.7769 - val_acc: 0.6437\n",
      "Epoch 238/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7303 - acc: 0.6557 - val_loss: 0.7784 - val_acc: 0.6242\n",
      "Epoch 239/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7402 - acc: 0.6545 - val_loss: 0.7711 - val_acc: 0.6530\n",
      "Epoch 240/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7312 - acc: 0.6511 - val_loss: 0.7694 - val_acc: 0.6490\n",
      "Epoch 241/1000\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.7329 - acc: 0.6578 - val_loss: 0.7659 - val_acc: 0.6457\n",
      "Epoch 242/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7301 - acc: 0.6557 - val_loss: 0.7765 - val_acc: 0.6497\n",
      "Epoch 243/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7273 - acc: 0.6657 - val_loss: 0.7775 - val_acc: 0.6376\n",
      "Epoch 244/1000\n",
      "5968/5968 [==============================] - 3s 459us/sample - loss: 0.7291 - acc: 0.6609 - val_loss: 0.7886 - val_acc: 0.6303\n",
      "Epoch 245/1000\n",
      "5968/5968 [==============================] - 3s 456us/sample - loss: 0.7500 - acc: 0.6471 - val_loss: 0.7889 - val_acc: 0.6457\n",
      "Epoch 246/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7434 - acc: 0.6495 - val_loss: 0.7852 - val_acc: 0.6403\n",
      "Epoch 247/1000\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.7455 - acc: 0.6575 - val_loss: 0.7775 - val_acc: 0.6410\n",
      "Epoch 248/1000\n",
      "5968/5968 [==============================] - 3s 459us/sample - loss: 0.7335 - acc: 0.6575 - val_loss: 0.7695 - val_acc: 0.6423\n",
      "Epoch 249/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7350 - acc: 0.6575 - val_loss: 0.7725 - val_acc: 0.6283\n",
      "Epoch 250/1000\n",
      "5968/5968 [==============================] - 3s 454us/sample - loss: 0.7310 - acc: 0.6597 - val_loss: 0.7789 - val_acc: 0.6457\n",
      "Epoch 251/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7228 - acc: 0.6620 - val_loss: 0.7836 - val_acc: 0.6256\n",
      "Epoch 252/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7317 - acc: 0.6587 - val_loss: 0.7783 - val_acc: 0.6437\n",
      "Epoch 253/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7401 - acc: 0.6550 - val_loss: 0.7712 - val_acc: 0.6430\n",
      "Epoch 254/1000\n",
      "5968/5968 [==============================] - 3s 472us/sample - loss: 0.7214 - acc: 0.6676 - val_loss: 0.7759 - val_acc: 0.6242\n",
      "Epoch 255/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7262 - acc: 0.6620 - val_loss: 0.7711 - val_acc: 0.6484\n",
      "Epoch 256/1000\n",
      "5968/5968 [==============================] - 3s 473us/sample - loss: 0.7254 - acc: 0.6585 - val_loss: 0.7722 - val_acc: 0.6410\n",
      "Epoch 257/1000\n",
      "5968/5968 [==============================] - 3s 493us/sample - loss: 0.7218 - acc: 0.6610 - val_loss: 0.7837 - val_acc: 0.6129\n",
      "Epoch 258/1000\n",
      "5968/5968 [==============================] - 3s 486us/sample - loss: 0.7258 - acc: 0.6639 - val_loss: 0.7741 - val_acc: 0.6463\n",
      "Epoch 259/1000\n",
      "5968/5968 [==============================] - 3s 481us/sample - loss: 0.7260 - acc: 0.6627 - val_loss: 0.7719 - val_acc: 0.6356\n",
      "Epoch 260/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7213 - acc: 0.6624 - val_loss: 0.7670 - val_acc: 0.6584\n",
      "Epoch 261/1000\n",
      "5968/5968 [==============================] - 3s 467us/sample - loss: 0.7249 - acc: 0.6676 - val_loss: 0.7696 - val_acc: 0.6530\n",
      "Epoch 262/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7262 - acc: 0.6610 - val_loss: 0.7703 - val_acc: 0.6410\n",
      "Epoch 263/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7162 - acc: 0.6624 - val_loss: 0.7711 - val_acc: 0.6330\n",
      "Epoch 264/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7144 - acc: 0.6662 - val_loss: 0.7760 - val_acc: 0.6209\n",
      "Epoch 265/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7167 - acc: 0.6640 - val_loss: 0.7735 - val_acc: 0.6510\n",
      "Epoch 266/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7125 - acc: 0.6689 - val_loss: 0.7686 - val_acc: 0.6316\n",
      "Epoch 267/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7325 - acc: 0.6493 - val_loss: 0.7742 - val_acc: 0.6316\n",
      "Epoch 268/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7316 - acc: 0.6538 - val_loss: 0.7702 - val_acc: 0.6510\n",
      "Epoch 269/1000\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.7139 - acc: 0.6657 - val_loss: 0.7762 - val_acc: 0.6296\n",
      "Epoch 270/1000\n",
      "5968/5968 [==============================] - 3s 459us/sample - loss: 0.7128 - acc: 0.6647 - val_loss: 0.7795 - val_acc: 0.6263\n",
      "Epoch 271/1000\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.7123 - acc: 0.6701 - val_loss: 0.7776 - val_acc: 0.6216\n",
      "Epoch 272/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7147 - acc: 0.6642 - val_loss: 0.7797 - val_acc: 0.6524\n",
      "Epoch 273/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7274 - acc: 0.6575 - val_loss: 0.7762 - val_acc: 0.6490\n",
      "Epoch 274/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7135 - acc: 0.6686 - val_loss: 0.7740 - val_acc: 0.6403\n",
      "Epoch 275/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7488 - acc: 0.6464 - val_loss: 0.7704 - val_acc: 0.6517\n",
      "Epoch 276/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7218 - acc: 0.6595 - val_loss: 0.7763 - val_acc: 0.6430\n",
      "Epoch 277/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7103 - acc: 0.6761 - val_loss: 0.7753 - val_acc: 0.6229\n",
      "Epoch 278/1000\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.7106 - acc: 0.6669 - val_loss: 0.7837 - val_acc: 0.6330\n",
      "Epoch 279/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7174 - acc: 0.6667 - val_loss: 0.7811 - val_acc: 0.6423\n",
      "Epoch 280/1000\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7135 - acc: 0.6635 - val_loss: 0.7850 - val_acc: 0.6390\n",
      "Epoch 281/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7075 - acc: 0.6701 - val_loss: 0.7808 - val_acc: 0.6517\n",
      "Epoch 282/1000\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.7598 - acc: 0.6545 - val_loss: 0.7780 - val_acc: 0.6544\n",
      "Epoch 283/1000\n",
      "5968/5968 [==============================] - 3s 469us/sample - loss: 0.7316 - acc: 0.6593 - val_loss: 0.7739 - val_acc: 0.6437\n",
      "Epoch 284/1000\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.7135 - acc: 0.6728 - val_loss: 0.7834 - val_acc: 0.6202\n",
      "Epoch 285/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7271 - acc: 0.6607 - val_loss: 0.7747 - val_acc: 0.6336\n",
      "Epoch 286/1000\n",
      "5968/5968 [==============================] - 3s 463us/sample - loss: 0.7220 - acc: 0.6689 - val_loss: 0.7719 - val_acc: 0.6316\n",
      "Epoch 287/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7147 - acc: 0.6652 - val_loss: 0.7695 - val_acc: 0.6296\n",
      "Epoch 288/1000\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.7315 - acc: 0.6582 - val_loss: 0.7717 - val_acc: 0.6336\n",
      "Epoch 289/1000\n",
      "5968/5968 [==============================] - 3s 474us/sample - loss: 0.7264 - acc: 0.6645 - val_loss: 0.7693 - val_acc: 0.6256\n",
      "Epoch 290/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7132 - acc: 0.6712 - val_loss: 0.7749 - val_acc: 0.6383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/1000\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7138 - acc: 0.6662 - val_loss: 0.7718 - val_acc: 0.6370\n",
      "Epoch 291: early stopping\n"
     ]
    }
   ],
   "source": [
    "# label encoding으로 fit\n",
    "single_lstm_model_label = KerasClassifier(build_fn = lstm_label\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])\n",
    "single_lstm_model_label.fit(X3_train_label, y3_train_label)\n",
    "y3_pred_single_lstm_label = single_lstm_model_label.predict(X3_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c71f8909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accouracy :  0.6334405144694534\n"
     ]
    }
   ],
   "source": [
    "print(\"accouracy : \", accuracy_score(y3_pred_single_lstm_label,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ebf58",
   "metadata": {},
   "source": [
    "## multi layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1739052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_lstm_ohe():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape = inputs, return_sequences = True))\n",
    "    model.add(LSTM(units, return_sequences = False))\n",
    "    model.add(Dense(outputs))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035205af",
   "metadata": {},
   "source": [
    "### Multi Layer LSTM (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8c8067f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5968 samples, validate on 1493 samples\n",
      "Epoch 1/1000\n",
      "5968/5968 [==============================] - 7s 1ms/sample - loss: 0.9483 - acc: 0.6170 - val_loss: 0.8262 - val_acc: 0.6443\n",
      "Epoch 2/1000\n",
      "5968/5968 [==============================] - 7s 1ms/sample - loss: 0.8204 - acc: 0.6356 - val_loss: 0.8096 - val_acc: 0.6450\n",
      "Epoch 3/1000\n",
      "5968/5968 [==============================] - 7s 1ms/sample - loss: 0.8151 - acc: 0.6369 - val_loss: 0.8050 - val_acc: 0.6450\n",
      "Epoch 4/1000\n",
      "5968/5968 [==============================] - 7s 1ms/sample - loss: 0.8129 - acc: 0.6387 - val_loss: 0.8000 - val_acc: 0.6450\n",
      "Epoch 5/1000\n",
      "5968/5968 [==============================] - 7s 1ms/sample - loss: 0.8154 - acc: 0.6379 - val_loss: 0.8001 - val_acc: 0.6463\n",
      "Epoch 6/1000\n",
      "5968/5968 [==============================] - 7s 1ms/sample - loss: 0.8122 - acc: 0.6394 - val_loss: 0.7969 - val_acc: 0.6437\n",
      "Epoch 7/1000\n",
      "5968/5968 [==============================] - 8s 1ms/sample - loss: 0.8109 - acc: 0.6397 - val_loss: 0.7991 - val_acc: 0.6450\n",
      "Epoch 8/1000\n",
      "5968/5968 [==============================] - 8s 1ms/sample - loss: 0.8116 - acc: 0.6384 - val_loss: 0.8042 - val_acc: 0.6450\n",
      "Epoch 9/1000\n",
      "5968/5968 [==============================] - 8s 1ms/sample - loss: 0.8104 - acc: 0.6381 - val_loss: 0.7985 - val_acc: 0.6463\n",
      "Epoch 10/1000\n",
      "5968/5968 [==============================] - 8s 1ms/sample - loss: 0.8107 - acc: 0.6382 - val_loss: 0.8023 - val_acc: 0.6457\n",
      "Epoch 11/1000\n",
      "5968/5968 [==============================] - 8s 1ms/sample - loss: 0.8109 - acc: 0.6382 - val_loss: 0.8000 - val_acc: 0.6457\n",
      "Epoch 12/1000\n",
      "5968/5968 [==============================] - 8s 1ms/sample - loss: 0.8061 - acc: 0.6392 - val_loss: 0.7979 - val_acc: 0.6463\n",
      "Epoch 13/1000\n",
      "5968/5968 [==============================] - 8s 1ms/sample - loss: 0.8050 - acc: 0.6387 - val_loss: 0.7932 - val_acc: 0.6477\n",
      "Epoch 14/1000\n",
      "5968/5968 [==============================] - 8s 1ms/sample - loss: 0.8047 - acc: 0.6382 - val_loss: 0.7937 - val_acc: 0.6470\n",
      "Epoch 15/1000\n",
      "5968/5968 [==============================] - 9s 1ms/sample - loss: 0.8041 - acc: 0.6397 - val_loss: 0.8040 - val_acc: 0.6457\n",
      "Epoch 16/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8021 - acc: 0.6387 - val_loss: 0.7919 - val_acc: 0.6477\n",
      "Epoch 17/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8035 - acc: 0.6391 - val_loss: 0.7936 - val_acc: 0.6463\n",
      "Epoch 18/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7989 - acc: 0.6387 - val_loss: 0.7951 - val_acc: 0.6477\n",
      "Epoch 19/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7986 - acc: 0.6397 - val_loss: 0.7923 - val_acc: 0.6470\n",
      "Epoch 20/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7993 - acc: 0.6404 - val_loss: 0.7918 - val_acc: 0.6463\n",
      "Epoch 21/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8021 - acc: 0.6391 - val_loss: 0.8034 - val_acc: 0.6484\n",
      "Epoch 22/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7993 - acc: 0.6413 - val_loss: 0.7927 - val_acc: 0.6477\n",
      "Epoch 23/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7970 - acc: 0.6404 - val_loss: 0.7948 - val_acc: 0.6470\n",
      "Epoch 24/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7952 - acc: 0.6394 - val_loss: 0.7923 - val_acc: 0.6477\n",
      "Epoch 25/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7921 - acc: 0.6409 - val_loss: 0.7900 - val_acc: 0.6457\n",
      "Epoch 26/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7907 - acc: 0.6418 - val_loss: 0.7871 - val_acc: 0.6470\n",
      "Epoch 27/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7899 - acc: 0.6416 - val_loss: 0.7917 - val_acc: 0.6457\n",
      "Epoch 28/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7980 - acc: 0.6404 - val_loss: 0.7881 - val_acc: 0.6490\n",
      "Epoch 29/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7922 - acc: 0.6419 - val_loss: 0.7871 - val_acc: 0.6484\n",
      "Epoch 30/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7888 - acc: 0.6431 - val_loss: 0.7921 - val_acc: 0.6477\n",
      "Epoch 31/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7971 - acc: 0.6413 - val_loss: 0.7945 - val_acc: 0.6470\n",
      "Epoch 32/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7906 - acc: 0.6421 - val_loss: 0.7880 - val_acc: 0.6457\n",
      "Epoch 33/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7889 - acc: 0.6416 - val_loss: 0.7912 - val_acc: 0.6490\n",
      "Epoch 34/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7907 - acc: 0.6409 - val_loss: 0.7904 - val_acc: 0.6484\n",
      "Epoch 35/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7857 - acc: 0.6433 - val_loss: 0.7830 - val_acc: 0.6470\n",
      "Epoch 36/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7865 - acc: 0.6438 - val_loss: 0.7818 - val_acc: 0.6497\n",
      "Epoch 37/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7855 - acc: 0.6443 - val_loss: 0.7898 - val_acc: 0.6504\n",
      "Epoch 38/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7827 - acc: 0.6438 - val_loss: 0.7977 - val_acc: 0.6497\n",
      "Epoch 39/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7911 - acc: 0.6392 - val_loss: 0.7978 - val_acc: 0.6504\n",
      "Epoch 40/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7915 - acc: 0.6431 - val_loss: 0.7904 - val_acc: 0.6524\n",
      "Epoch 41/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7877 - acc: 0.6436 - val_loss: 0.8026 - val_acc: 0.6497\n",
      "Epoch 42/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7843 - acc: 0.6439 - val_loss: 0.7742 - val_acc: 0.6544\n",
      "Epoch 43/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7801 - acc: 0.6438 - val_loss: 0.7879 - val_acc: 0.6490\n",
      "Epoch 44/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7860 - acc: 0.6439 - val_loss: 0.7918 - val_acc: 0.6530\n",
      "Epoch 45/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7849 - acc: 0.6454 - val_loss: 0.7808 - val_acc: 0.6510\n",
      "Epoch 46/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7752 - acc: 0.6436 - val_loss: 0.7803 - val_acc: 0.6524\n",
      "Epoch 47/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7759 - acc: 0.6449 - val_loss: 0.7789 - val_acc: 0.6517\n",
      "Epoch 48/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7747 - acc: 0.6453 - val_loss: 0.7679 - val_acc: 0.6564\n",
      "Epoch 49/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7701 - acc: 0.6478 - val_loss: 0.7822 - val_acc: 0.6497\n",
      "Epoch 50/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7778 - acc: 0.6483 - val_loss: 0.7722 - val_acc: 0.6557\n",
      "Epoch 51/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7678 - acc: 0.6476 - val_loss: 0.7828 - val_acc: 0.6484\n",
      "Epoch 52/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7717 - acc: 0.6470 - val_loss: 0.7830 - val_acc: 0.6524\n",
      "Epoch 53/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7707 - acc: 0.6471 - val_loss: 0.7833 - val_acc: 0.6504\n",
      "Epoch 54/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7776 - acc: 0.6471 - val_loss: 0.7935 - val_acc: 0.6477\n",
      "Epoch 55/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7808 - acc: 0.6436 - val_loss: 0.7777 - val_acc: 0.6497\n",
      "Epoch 56/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7727 - acc: 0.6478 - val_loss: 0.7737 - val_acc: 0.6524\n",
      "Epoch 57/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7692 - acc: 0.6491 - val_loss: 0.7766 - val_acc: 0.6510\n",
      "Epoch 58/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7725 - acc: 0.6480 - val_loss: 0.7954 - val_acc: 0.6517\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7743 - acc: 0.6464 - val_loss: 0.7746 - val_acc: 0.6484\n",
      "Epoch 60/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7885 - acc: 0.6449 - val_loss: 0.7950 - val_acc: 0.6443\n",
      "Epoch 61/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7822 - acc: 0.6453 - val_loss: 0.7750 - val_acc: 0.6510\n",
      "Epoch 62/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7722 - acc: 0.6475 - val_loss: 0.7819 - val_acc: 0.6510\n",
      "Epoch 63/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7699 - acc: 0.6488 - val_loss: 0.7717 - val_acc: 0.6537\n",
      "Epoch 64/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7688 - acc: 0.6485 - val_loss: 0.7719 - val_acc: 0.6504\n",
      "Epoch 65/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7684 - acc: 0.6493 - val_loss: 0.7782 - val_acc: 0.6524\n",
      "Epoch 66/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7694 - acc: 0.6488 - val_loss: 0.7694 - val_acc: 0.6544\n",
      "Epoch 67/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7680 - acc: 0.6438 - val_loss: 0.7674 - val_acc: 0.6423\n",
      "Epoch 68/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7691 - acc: 0.6495 - val_loss: 0.7713 - val_acc: 0.6551\n",
      "Epoch 69/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7750 - acc: 0.6483 - val_loss: 0.7842 - val_acc: 0.6530\n",
      "Epoch 70/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7850 - acc: 0.6421 - val_loss: 0.7752 - val_acc: 0.6484\n",
      "Epoch 71/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7796 - acc: 0.6468 - val_loss: 0.7677 - val_acc: 0.6537\n",
      "Epoch 72/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7778 - acc: 0.6466 - val_loss: 0.7763 - val_acc: 0.6484\n",
      "Epoch 73/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7653 - acc: 0.6478 - val_loss: 0.7704 - val_acc: 0.6524\n",
      "Epoch 74/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7639 - acc: 0.6486 - val_loss: 0.7672 - val_acc: 0.6504\n",
      "Epoch 75/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7616 - acc: 0.6486 - val_loss: 0.7687 - val_acc: 0.6517\n",
      "Epoch 76/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7588 - acc: 0.6488 - val_loss: 0.7741 - val_acc: 0.6490\n",
      "Epoch 77/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7617 - acc: 0.6501 - val_loss: 0.7621 - val_acc: 0.6551\n",
      "Epoch 78/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7540 - acc: 0.6481 - val_loss: 0.7741 - val_acc: 0.6537\n",
      "Epoch 79/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7574 - acc: 0.6525 - val_loss: 0.7679 - val_acc: 0.6564\n",
      "Epoch 80/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7573 - acc: 0.6535 - val_loss: 0.7699 - val_acc: 0.6571\n",
      "Epoch 81/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7899 - acc: 0.6433 - val_loss: 0.7969 - val_acc: 0.6450\n",
      "Epoch 82/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7851 - acc: 0.6413 - val_loss: 0.7834 - val_acc: 0.6490\n",
      "Epoch 83/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7838 - acc: 0.6349 - val_loss: 0.7988 - val_acc: 0.6450\n",
      "Epoch 84/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7860 - acc: 0.6404 - val_loss: 0.7829 - val_acc: 0.6463\n",
      "Epoch 85/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7739 - acc: 0.6414 - val_loss: 0.7747 - val_acc: 0.6497\n",
      "Epoch 86/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7693 - acc: 0.6444 - val_loss: 0.7762 - val_acc: 0.6463\n",
      "Epoch 87/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7706 - acc: 0.6468 - val_loss: 0.7754 - val_acc: 0.6470\n",
      "Epoch 88/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7713 - acc: 0.6408 - val_loss: 0.7874 - val_acc: 0.6417\n",
      "Epoch 89/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7720 - acc: 0.6470 - val_loss: 0.7637 - val_acc: 0.6490\n",
      "Epoch 90/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7625 - acc: 0.6475 - val_loss: 0.7715 - val_acc: 0.6530\n",
      "Epoch 91/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7638 - acc: 0.6449 - val_loss: 0.7708 - val_acc: 0.6504\n",
      "Epoch 92/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7650 - acc: 0.6478 - val_loss: 0.7676 - val_acc: 0.6510\n",
      "Epoch 93/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7596 - acc: 0.6485 - val_loss: 0.7679 - val_acc: 0.6504\n",
      "Epoch 94/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7671 - acc: 0.6454 - val_loss: 0.7702 - val_acc: 0.6497\n",
      "Epoch 95/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7664 - acc: 0.6433 - val_loss: 0.7742 - val_acc: 0.6484\n",
      "Epoch 96/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7635 - acc: 0.6439 - val_loss: 0.7846 - val_acc: 0.6397\n",
      "Epoch 97/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7596 - acc: 0.6481 - val_loss: 0.7644 - val_acc: 0.6517\n",
      "Epoch 98/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7578 - acc: 0.6506 - val_loss: 0.7671 - val_acc: 0.6524\n",
      "Epoch 99/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7687 - acc: 0.6513 - val_loss: 0.7648 - val_acc: 0.6537\n",
      "Epoch 100/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7659 - acc: 0.6446 - val_loss: 0.7706 - val_acc: 0.6490\n",
      "Epoch 101/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7656 - acc: 0.6470 - val_loss: 0.7659 - val_acc: 0.6537\n",
      "Epoch 102/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7556 - acc: 0.6520 - val_loss: 0.7638 - val_acc: 0.6504\n",
      "Epoch 103/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7582 - acc: 0.6426 - val_loss: 0.7612 - val_acc: 0.6544\n",
      "Epoch 104/1000\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7531 - acc: 0.6511 - val_loss: 0.7592 - val_acc: 0.6557\n",
      "Epoch 105/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7570 - acc: 0.6521 - val_loss: 0.7633 - val_acc: 0.6597\n",
      "Epoch 106/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7576 - acc: 0.6557 - val_loss: 0.7716 - val_acc: 0.6470\n",
      "Epoch 107/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7564 - acc: 0.6500 - val_loss: 0.7724 - val_acc: 0.6430\n",
      "Epoch 108/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7586 - acc: 0.6513 - val_loss: 0.7652 - val_acc: 0.6510\n",
      "Epoch 109/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7548 - acc: 0.6526 - val_loss: 0.7631 - val_acc: 0.6557\n",
      "Epoch 110/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7465 - acc: 0.6562 - val_loss: 0.7632 - val_acc: 0.6537\n",
      "Epoch 111/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7467 - acc: 0.6520 - val_loss: 0.7635 - val_acc: 0.6497\n",
      "Epoch 112/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7490 - acc: 0.6562 - val_loss: 0.7627 - val_acc: 0.6477\n",
      "Epoch 113/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7489 - acc: 0.6563 - val_loss: 0.7759 - val_acc: 0.6390\n",
      "Epoch 114/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7480 - acc: 0.6532 - val_loss: 0.7626 - val_acc: 0.6604\n",
      "Epoch 115/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7465 - acc: 0.6587 - val_loss: 0.7589 - val_acc: 0.6577\n",
      "Epoch 116/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7425 - acc: 0.6557 - val_loss: 0.7580 - val_acc: 0.6470\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7470 - acc: 0.6537 - val_loss: 0.7595 - val_acc: 0.6597\n",
      "Epoch 118/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7467 - acc: 0.6597 - val_loss: 0.7753 - val_acc: 0.6390\n",
      "Epoch 119/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7438 - acc: 0.6557 - val_loss: 0.7652 - val_acc: 0.6584\n",
      "Epoch 120/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7409 - acc: 0.6570 - val_loss: 0.7706 - val_acc: 0.6430\n",
      "Epoch 121/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7466 - acc: 0.6587 - val_loss: 0.7512 - val_acc: 0.6678\n",
      "Epoch 122/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7462 - acc: 0.6599 - val_loss: 0.7547 - val_acc: 0.6624\n",
      "Epoch 123/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7433 - acc: 0.6567 - val_loss: 0.7587 - val_acc: 0.6551\n",
      "Epoch 124/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7394 - acc: 0.6634 - val_loss: 0.7604 - val_acc: 0.6597\n",
      "Epoch 125/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7485 - acc: 0.6568 - val_loss: 0.7632 - val_acc: 0.6544\n",
      "Epoch 126/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7395 - acc: 0.6652 - val_loss: 0.7573 - val_acc: 0.6591\n",
      "Epoch 127/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7437 - acc: 0.6568 - val_loss: 0.7629 - val_acc: 0.6544\n",
      "Epoch 128/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7349 - acc: 0.6640 - val_loss: 0.7557 - val_acc: 0.6664\n",
      "Epoch 129/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7345 - acc: 0.6647 - val_loss: 0.7608 - val_acc: 0.6678\n",
      "Epoch 130/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7495 - acc: 0.6528 - val_loss: 0.7581 - val_acc: 0.6597\n",
      "Epoch 131/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7424 - acc: 0.6619 - val_loss: 0.7651 - val_acc: 0.6551\n",
      "Epoch 132/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7327 - acc: 0.6640 - val_loss: 0.7553 - val_acc: 0.6691\n",
      "Epoch 133/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7375 - acc: 0.6682 - val_loss: 0.7614 - val_acc: 0.6470\n",
      "Epoch 134/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7361 - acc: 0.6617 - val_loss: 0.7553 - val_acc: 0.6731\n",
      "Epoch 135/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7269 - acc: 0.6721 - val_loss: 0.7548 - val_acc: 0.6658\n",
      "Epoch 136/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7301 - acc: 0.6714 - val_loss: 0.7669 - val_acc: 0.6443\n",
      "Epoch 137/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7352 - acc: 0.6637 - val_loss: 0.7695 - val_acc: 0.6484\n",
      "Epoch 138/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7437 - acc: 0.6592 - val_loss: 0.7605 - val_acc: 0.6591\n",
      "Epoch 139/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7350 - acc: 0.6652 - val_loss: 0.7660 - val_acc: 0.6618\n",
      "Epoch 140/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7382 - acc: 0.6612 - val_loss: 0.7716 - val_acc: 0.6544\n",
      "Epoch 141/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7399 - acc: 0.6639 - val_loss: 0.7622 - val_acc: 0.6571\n",
      "Epoch 142/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7312 - acc: 0.6709 - val_loss: 0.7504 - val_acc: 0.6698\n",
      "Epoch 143/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7329 - acc: 0.6649 - val_loss: 0.7545 - val_acc: 0.6664\n",
      "Epoch 144/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7264 - acc: 0.6691 - val_loss: 0.7563 - val_acc: 0.6658\n",
      "Epoch 145/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7224 - acc: 0.6696 - val_loss: 0.7581 - val_acc: 0.6611\n",
      "Epoch 146/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7276 - acc: 0.6659 - val_loss: 0.7529 - val_acc: 0.6658\n",
      "Epoch 147/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7215 - acc: 0.6699 - val_loss: 0.7520 - val_acc: 0.6638\n",
      "Epoch 148/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7259 - acc: 0.6697 - val_loss: 0.7565 - val_acc: 0.6624\n",
      "Epoch 149/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7191 - acc: 0.6739 - val_loss: 0.7603 - val_acc: 0.6537\n",
      "Epoch 150/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7248 - acc: 0.6664 - val_loss: 0.7512 - val_acc: 0.6571\n",
      "Epoch 151/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7212 - acc: 0.6694 - val_loss: 0.7533 - val_acc: 0.6557\n",
      "Epoch 152/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7161 - acc: 0.6751 - val_loss: 0.7682 - val_acc: 0.6577\n",
      "Epoch 153/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7204 - acc: 0.6734 - val_loss: 0.7673 - val_acc: 0.6477\n",
      "Epoch 154/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7463 - acc: 0.6592 - val_loss: 0.7652 - val_acc: 0.6604\n",
      "Epoch 155/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7309 - acc: 0.6687 - val_loss: 0.7739 - val_acc: 0.6557\n",
      "Epoch 156/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7376 - acc: 0.6650 - val_loss: 0.7656 - val_acc: 0.6564\n",
      "Epoch 157/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7284 - acc: 0.6694 - val_loss: 0.7724 - val_acc: 0.6524\n",
      "Epoch 158/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7240 - acc: 0.6697 - val_loss: 0.7542 - val_acc: 0.6644\n",
      "Epoch 159/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7137 - acc: 0.6769 - val_loss: 0.7647 - val_acc: 0.6564\n",
      "Epoch 160/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7109 - acc: 0.6798 - val_loss: 0.7757 - val_acc: 0.6530\n",
      "Epoch 161/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7300 - acc: 0.6687 - val_loss: 0.7664 - val_acc: 0.6604\n",
      "Epoch 162/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7255 - acc: 0.6714 - val_loss: 0.7708 - val_acc: 0.6611\n",
      "Epoch 163/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7212 - acc: 0.6711 - val_loss: 0.7642 - val_acc: 0.6591\n",
      "Epoch 164/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7196 - acc: 0.6749 - val_loss: 0.7587 - val_acc: 0.6711\n",
      "Epoch 165/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7072 - acc: 0.6795 - val_loss: 0.7591 - val_acc: 0.6591\n",
      "Epoch 166/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7132 - acc: 0.6828 - val_loss: 0.7606 - val_acc: 0.6658\n",
      "Epoch 167/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7272 - acc: 0.6696 - val_loss: 0.7701 - val_acc: 0.6510\n",
      "Epoch 168/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7199 - acc: 0.6738 - val_loss: 0.7629 - val_acc: 0.6624\n",
      "Epoch 169/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7157 - acc: 0.6766 - val_loss: 0.7712 - val_acc: 0.6504\n",
      "Epoch 170/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7126 - acc: 0.6744 - val_loss: 0.7607 - val_acc: 0.6658\n",
      "Epoch 171/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7165 - acc: 0.6779 - val_loss: 0.7814 - val_acc: 0.6383\n",
      "Epoch 172/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7206 - acc: 0.6795 - val_loss: 0.7553 - val_acc: 0.6624\n",
      "Epoch 173/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7660 - acc: 0.6419 - val_loss: 0.7727 - val_acc: 0.6484\n",
      "Epoch 174/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7258 - acc: 0.6707 - val_loss: 0.7639 - val_acc: 0.6551\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7180 - acc: 0.6759 - val_loss: 0.7564 - val_acc: 0.6591\n",
      "Epoch 176/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7143 - acc: 0.6759 - val_loss: 0.7657 - val_acc: 0.6557\n",
      "Epoch 177/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7190 - acc: 0.6751 - val_loss: 0.7530 - val_acc: 0.6611\n",
      "Epoch 178/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7145 - acc: 0.6729 - val_loss: 0.7588 - val_acc: 0.6557\n",
      "Epoch 179/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7059 - acc: 0.6850 - val_loss: 0.7661 - val_acc: 0.6577\n",
      "Epoch 180/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7136 - acc: 0.6768 - val_loss: 0.7711 - val_acc: 0.6530\n",
      "Epoch 181/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7110 - acc: 0.6840 - val_loss: 0.7665 - val_acc: 0.6604\n",
      "Epoch 182/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7013 - acc: 0.6892 - val_loss: 0.7702 - val_acc: 0.6584\n",
      "Epoch 183/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7123 - acc: 0.6796 - val_loss: 0.7740 - val_acc: 0.6577\n",
      "Epoch 184/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7136 - acc: 0.6776 - val_loss: 0.7606 - val_acc: 0.6631\n",
      "Epoch 185/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6978 - acc: 0.6872 - val_loss: 0.7579 - val_acc: 0.6664\n",
      "Epoch 186/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7080 - acc: 0.6810 - val_loss: 0.7584 - val_acc: 0.6604\n",
      "Epoch 187/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7008 - acc: 0.6860 - val_loss: 0.7557 - val_acc: 0.6644\n",
      "Epoch 188/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7194 - acc: 0.6746 - val_loss: 0.7773 - val_acc: 0.6450\n",
      "Epoch 189/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7162 - acc: 0.6701 - val_loss: 0.7659 - val_acc: 0.6564\n",
      "Epoch 190/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6978 - acc: 0.6868 - val_loss: 0.7529 - val_acc: 0.6624\n",
      "Epoch 191/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7006 - acc: 0.6863 - val_loss: 0.7610 - val_acc: 0.6698\n",
      "Epoch 192/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6940 - acc: 0.6875 - val_loss: 0.7595 - val_acc: 0.6604\n",
      "Epoch 193/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7338 - acc: 0.6640 - val_loss: 0.7682 - val_acc: 0.6611\n",
      "Epoch 194/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7120 - acc: 0.6774 - val_loss: 0.7609 - val_acc: 0.6597\n",
      "Epoch 195/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7144 - acc: 0.6733 - val_loss: 0.7706 - val_acc: 0.6557\n",
      "Epoch 196/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7265 - acc: 0.6684 - val_loss: 0.7643 - val_acc: 0.6638\n",
      "Epoch 197/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7163 - acc: 0.6726 - val_loss: 0.7597 - val_acc: 0.6678\n",
      "Epoch 198/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7001 - acc: 0.6840 - val_loss: 0.7630 - val_acc: 0.6698\n",
      "Epoch 199/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6975 - acc: 0.6868 - val_loss: 0.7645 - val_acc: 0.6651\n",
      "Epoch 200/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7048 - acc: 0.6840 - val_loss: 0.7603 - val_acc: 0.6564\n",
      "Epoch 201/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7166 - acc: 0.6759 - val_loss: 0.7818 - val_acc: 0.6591\n",
      "Epoch 202/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7040 - acc: 0.6805 - val_loss: 0.7552 - val_acc: 0.6698\n",
      "Epoch 203/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7038 - acc: 0.6836 - val_loss: 0.8071 - val_acc: 0.6437\n",
      "Epoch 204/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7407 - acc: 0.6657 - val_loss: 0.7592 - val_acc: 0.6624\n",
      "Epoch 205/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7027 - acc: 0.6793 - val_loss: 0.7625 - val_acc: 0.6530\n",
      "Epoch 206/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7069 - acc: 0.6788 - val_loss: 0.7643 - val_acc: 0.6604\n",
      "Epoch 207/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7010 - acc: 0.6795 - val_loss: 0.7610 - val_acc: 0.6618\n",
      "Epoch 208/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6939 - acc: 0.6870 - val_loss: 0.7735 - val_acc: 0.6530\n",
      "Epoch 209/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6960 - acc: 0.6897 - val_loss: 0.7670 - val_acc: 0.6571\n",
      "Epoch 210/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6992 - acc: 0.6868 - val_loss: 0.7664 - val_acc: 0.6624\n",
      "Epoch 211/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6952 - acc: 0.6873 - val_loss: 0.7658 - val_acc: 0.6638\n",
      "Epoch 212/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6859 - acc: 0.6965 - val_loss: 0.7661 - val_acc: 0.6577\n",
      "Epoch 213/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6831 - acc: 0.6934 - val_loss: 0.7739 - val_acc: 0.6571\n",
      "Epoch 214/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6839 - acc: 0.6919 - val_loss: 0.7737 - val_acc: 0.6711\n",
      "Epoch 215/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6909 - acc: 0.6909 - val_loss: 0.8036 - val_acc: 0.6209\n",
      "Epoch 216/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7634 - acc: 0.6473 - val_loss: 0.7837 - val_acc: 0.6544\n",
      "Epoch 217/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7402 - acc: 0.6583 - val_loss: 0.7783 - val_acc: 0.6564\n",
      "Epoch 218/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7223 - acc: 0.6654 - val_loss: 0.7670 - val_acc: 0.6544\n",
      "Epoch 219/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7008 - acc: 0.6798 - val_loss: 0.7729 - val_acc: 0.6551\n",
      "Epoch 220/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6920 - acc: 0.6857 - val_loss: 0.7679 - val_acc: 0.6611\n",
      "Epoch 221/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6964 - acc: 0.6826 - val_loss: 0.7766 - val_acc: 0.6564\n",
      "Epoch 222/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6940 - acc: 0.6905 - val_loss: 0.7747 - val_acc: 0.6517\n",
      "Epoch 223/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6914 - acc: 0.6878 - val_loss: 0.7802 - val_acc: 0.6618\n",
      "Epoch 224/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6914 - acc: 0.6903 - val_loss: 0.7685 - val_acc: 0.6604\n",
      "Epoch 225/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6870 - acc: 0.6954 - val_loss: 0.7769 - val_acc: 0.6584\n",
      "Epoch 226/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6838 - acc: 0.6944 - val_loss: 0.7949 - val_acc: 0.6597\n",
      "Epoch 227/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6909 - acc: 0.6835 - val_loss: 0.7875 - val_acc: 0.6624\n",
      "Epoch 228/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6885 - acc: 0.6930 - val_loss: 0.7758 - val_acc: 0.6631\n",
      "Epoch 229/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6976 - acc: 0.6870 - val_loss: 0.7789 - val_acc: 0.6517\n",
      "Epoch 230/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6818 - acc: 0.6924 - val_loss: 0.7860 - val_acc: 0.6571\n",
      "Epoch 231/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6766 - acc: 0.6971 - val_loss: 0.7827 - val_acc: 0.6591\n",
      "Epoch 232/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6747 - acc: 0.6986 - val_loss: 0.7893 - val_acc: 0.6484\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6750 - acc: 0.6967 - val_loss: 0.7797 - val_acc: 0.6564\n",
      "Epoch 234/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6862 - acc: 0.6937 - val_loss: 0.7788 - val_acc: 0.6678\n",
      "Epoch 235/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6891 - acc: 0.6893 - val_loss: 0.7803 - val_acc: 0.6577\n",
      "Epoch 236/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6852 - acc: 0.6939 - val_loss: 0.7912 - val_acc: 0.6577\n",
      "Epoch 237/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7057 - acc: 0.6835 - val_loss: 0.7802 - val_acc: 0.6584\n",
      "Epoch 238/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6798 - acc: 0.6967 - val_loss: 0.7797 - val_acc: 0.6584\n",
      "Epoch 239/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6759 - acc: 0.6945 - val_loss: 0.7809 - val_acc: 0.6597\n",
      "Epoch 240/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6775 - acc: 0.6964 - val_loss: 0.7717 - val_acc: 0.6658\n",
      "Epoch 241/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6738 - acc: 0.6987 - val_loss: 0.7808 - val_acc: 0.6504\n",
      "Epoch 242/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6821 - acc: 0.6910 - val_loss: 0.7912 - val_acc: 0.6571\n",
      "Epoch 242: early stopping\n"
     ]
    }
   ],
   "source": [
    "multi_lstm_model_ohe = KerasClassifier(build_fn = stacked_lstm_ohe\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])\n",
    "multi_lstm_model_ohe.fit(X3_train_ohe, y3_train_ohe)\n",
    "y3_pred_multi_lstm_ohe = multi_lstm_model_ohe.predict(X3_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41171b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM multi  layer accouracy :  0.6312968917470525\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM multi  layer accouracy : \", accuracy_score(y3_pred_multi_lstm_ohe,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1412f00c",
   "metadata": {},
   "source": [
    "### Multi Layer LSTM (Label encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f9929a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_lstm_label():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape = inputs, return_sequences = True))\n",
    "    model.add(LSTM(units, return_sequences = False))\n",
    "    model.add(Dense(outputs))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0562249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5968 samples, validate on 1493 samples\n",
      "Epoch 1/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.9496 - acc: 0.6263 - val_loss: 0.8425 - val_acc: 0.6450\n",
      "Epoch 2/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.8256 - acc: 0.6377 - val_loss: 0.8084 - val_acc: 0.6463\n",
      "Epoch 3/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.8163 - acc: 0.6364 - val_loss: 0.8152 - val_acc: 0.6450\n",
      "Epoch 4/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8160 - acc: 0.6352 - val_loss: 0.8019 - val_acc: 0.6457\n",
      "Epoch 5/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8135 - acc: 0.6382 - val_loss: 0.8020 - val_acc: 0.6450\n",
      "Epoch 6/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8123 - acc: 0.6391 - val_loss: 0.7990 - val_acc: 0.6470\n",
      "Epoch 7/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.8124 - acc: 0.6387 - val_loss: 0.7999 - val_acc: 0.6450\n",
      "Epoch 8/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.8119 - acc: 0.6387 - val_loss: 0.8037 - val_acc: 0.6450\n",
      "Epoch 9/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8127 - acc: 0.6377 - val_loss: 0.8011 - val_acc: 0.6463\n",
      "Epoch 10/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8116 - acc: 0.6387 - val_loss: 0.8018 - val_acc: 0.6463\n",
      "Epoch 11/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8092 - acc: 0.6399 - val_loss: 0.7997 - val_acc: 0.6457\n",
      "Epoch 12/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8061 - acc: 0.6389 - val_loss: 0.8013 - val_acc: 0.6443\n",
      "Epoch 13/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8073 - acc: 0.6389 - val_loss: 0.7962 - val_acc: 0.6457\n",
      "Epoch 14/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8050 - acc: 0.6396 - val_loss: 0.7928 - val_acc: 0.6470\n",
      "Epoch 15/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8031 - acc: 0.6409 - val_loss: 0.7968 - val_acc: 0.6463\n",
      "Epoch 16/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8006 - acc: 0.6397 - val_loss: 0.8099 - val_acc: 0.6477\n",
      "Epoch 17/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8039 - acc: 0.6419 - val_loss: 0.7932 - val_acc: 0.6463\n",
      "Epoch 18/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8008 - acc: 0.6416 - val_loss: 0.8085 - val_acc: 0.6463\n",
      "Epoch 19/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8047 - acc: 0.6404 - val_loss: 0.7943 - val_acc: 0.6497\n",
      "Epoch 20/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7985 - acc: 0.6414 - val_loss: 0.7982 - val_acc: 0.6477\n",
      "Epoch 21/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8004 - acc: 0.6399 - val_loss: 0.8038 - val_acc: 0.6477\n",
      "Epoch 22/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8002 - acc: 0.6413 - val_loss: 0.7944 - val_acc: 0.6490\n",
      "Epoch 23/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7959 - acc: 0.6416 - val_loss: 0.8019 - val_acc: 0.6484\n",
      "Epoch 24/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7976 - acc: 0.6413 - val_loss: 0.7936 - val_acc: 0.6490\n",
      "Epoch 25/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.8005 - acc: 0.6433 - val_loss: 0.7933 - val_acc: 0.6463\n",
      "Epoch 26/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8001 - acc: 0.6411 - val_loss: 0.7964 - val_acc: 0.6484\n",
      "Epoch 27/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7953 - acc: 0.6428 - val_loss: 0.7946 - val_acc: 0.6497\n",
      "Epoch 28/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7950 - acc: 0.6429 - val_loss: 0.8039 - val_acc: 0.6463\n",
      "Epoch 29/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7997 - acc: 0.6433 - val_loss: 0.7937 - val_acc: 0.6477\n",
      "Epoch 30/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7930 - acc: 0.6423 - val_loss: 0.7944 - val_acc: 0.6490\n",
      "Epoch 31/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7939 - acc: 0.6436 - val_loss: 0.7906 - val_acc: 0.6497\n",
      "Epoch 32/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7925 - acc: 0.6444 - val_loss: 0.7889 - val_acc: 0.6497\n",
      "Epoch 33/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7935 - acc: 0.6438 - val_loss: 0.7902 - val_acc: 0.6490\n",
      "Epoch 34/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7957 - acc: 0.6424 - val_loss: 0.7968 - val_acc: 0.6484\n",
      "Epoch 35/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7914 - acc: 0.6438 - val_loss: 0.7943 - val_acc: 0.6504\n",
      "Epoch 36/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7911 - acc: 0.6453 - val_loss: 0.7918 - val_acc: 0.6497\n",
      "Epoch 37/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7922 - acc: 0.6443 - val_loss: 0.7899 - val_acc: 0.6477\n",
      "Epoch 38/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7901 - acc: 0.6448 - val_loss: 0.7836 - val_acc: 0.6504\n",
      "Epoch 39/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7925 - acc: 0.6426 - val_loss: 0.7952 - val_acc: 0.6490\n",
      "Epoch 40/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7876 - acc: 0.6441 - val_loss: 0.7853 - val_acc: 0.6497\n",
      "Epoch 41/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7863 - acc: 0.6449 - val_loss: 0.8004 - val_acc: 0.6450\n",
      "Epoch 42/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7935 - acc: 0.6431 - val_loss: 0.7826 - val_acc: 0.6537\n",
      "Epoch 43/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7863 - acc: 0.6441 - val_loss: 0.7954 - val_acc: 0.6477\n",
      "Epoch 44/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7898 - acc: 0.6448 - val_loss: 0.7813 - val_acc: 0.6517\n",
      "Epoch 45/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7874 - acc: 0.6448 - val_loss: 0.7875 - val_acc: 0.6443\n",
      "Epoch 46/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7858 - acc: 0.6448 - val_loss: 0.7895 - val_acc: 0.6484\n",
      "Epoch 47/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7868 - acc: 0.6443 - val_loss: 0.7831 - val_acc: 0.6564\n",
      "Epoch 48/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7837 - acc: 0.6451 - val_loss: 0.7792 - val_acc: 0.6497\n",
      "Epoch 49/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7934 - acc: 0.6439 - val_loss: 0.7947 - val_acc: 0.6510\n",
      "Epoch 50/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7899 - acc: 0.6443 - val_loss: 0.7886 - val_acc: 0.6497\n",
      "Epoch 51/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7811 - acc: 0.6475 - val_loss: 0.7865 - val_acc: 0.6510\n",
      "Epoch 52/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7806 - acc: 0.6490 - val_loss: 0.7834 - val_acc: 0.6484\n",
      "Epoch 53/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7785 - acc: 0.6464 - val_loss: 0.7847 - val_acc: 0.6497\n",
      "Epoch 54/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7908 - acc: 0.6414 - val_loss: 0.7946 - val_acc: 0.6457\n",
      "Epoch 55/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7807 - acc: 0.6449 - val_loss: 0.7836 - val_acc: 0.6524\n",
      "Epoch 56/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7785 - acc: 0.6446 - val_loss: 0.8209 - val_acc: 0.6470\n",
      "Epoch 57/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7920 - acc: 0.6443 - val_loss: 0.7832 - val_acc: 0.6497\n",
      "Epoch 58/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7793 - acc: 0.6464 - val_loss: 0.7791 - val_acc: 0.6504\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7720 - acc: 0.6485 - val_loss: 0.7902 - val_acc: 0.6484\n",
      "Epoch 60/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7766 - acc: 0.6493 - val_loss: 0.7998 - val_acc: 0.6484\n",
      "Epoch 61/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7793 - acc: 0.6451 - val_loss: 0.7765 - val_acc: 0.6504\n",
      "Epoch 62/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7718 - acc: 0.6486 - val_loss: 0.7853 - val_acc: 0.6510\n",
      "Epoch 63/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7729 - acc: 0.6493 - val_loss: 0.7758 - val_acc: 0.6517\n",
      "Epoch 64/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7672 - acc: 0.6503 - val_loss: 0.7707 - val_acc: 0.6537\n",
      "Epoch 65/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7674 - acc: 0.6496 - val_loss: 0.7771 - val_acc: 0.6524\n",
      "Epoch 66/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7734 - acc: 0.6476 - val_loss: 0.7773 - val_acc: 0.6551\n",
      "Epoch 67/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7723 - acc: 0.6480 - val_loss: 0.7820 - val_acc: 0.6517\n",
      "Epoch 68/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7704 - acc: 0.6501 - val_loss: 0.7744 - val_acc: 0.6510\n",
      "Epoch 69/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7729 - acc: 0.6486 - val_loss: 0.7806 - val_acc: 0.6557\n",
      "Epoch 70/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7774 - acc: 0.6470 - val_loss: 0.8145 - val_acc: 0.6484\n",
      "Epoch 71/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7943 - acc: 0.6451 - val_loss: 0.7894 - val_acc: 0.6484\n",
      "Epoch 72/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7777 - acc: 0.6490 - val_loss: 0.7820 - val_acc: 0.6497\n",
      "Epoch 73/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7723 - acc: 0.6498 - val_loss: 0.7819 - val_acc: 0.6544\n",
      "Epoch 74/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7660 - acc: 0.6526 - val_loss: 0.7875 - val_acc: 0.6510\n",
      "Epoch 75/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7677 - acc: 0.6505 - val_loss: 0.7729 - val_acc: 0.6577\n",
      "Epoch 76/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7701 - acc: 0.6513 - val_loss: 0.7946 - val_acc: 0.6484\n",
      "Epoch 77/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7723 - acc: 0.6481 - val_loss: 0.7764 - val_acc: 0.6470\n",
      "Epoch 78/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7635 - acc: 0.6511 - val_loss: 0.7795 - val_acc: 0.6450\n",
      "Epoch 79/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7623 - acc: 0.6526 - val_loss: 0.7766 - val_acc: 0.6470\n",
      "Epoch 80/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7590 - acc: 0.6518 - val_loss: 0.7770 - val_acc: 0.6490\n",
      "Epoch 81/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7583 - acc: 0.6532 - val_loss: 0.7774 - val_acc: 0.6551\n",
      "Epoch 82/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7681 - acc: 0.6521 - val_loss: 0.7845 - val_acc: 0.6497\n",
      "Epoch 83/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7587 - acc: 0.6501 - val_loss: 0.7749 - val_acc: 0.6484\n",
      "Epoch 84/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7613 - acc: 0.6511 - val_loss: 0.7776 - val_acc: 0.6551\n",
      "Epoch 85/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7625 - acc: 0.6490 - val_loss: 0.7777 - val_acc: 0.6484\n",
      "Epoch 86/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7703 - acc: 0.6453 - val_loss: 0.7898 - val_acc: 0.6463\n",
      "Epoch 87/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7678 - acc: 0.6495 - val_loss: 0.7849 - val_acc: 0.6484\n",
      "Epoch 88/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7599 - acc: 0.6503 - val_loss: 0.7771 - val_acc: 0.6497\n",
      "Epoch 89/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7769 - acc: 0.6495 - val_loss: 0.7717 - val_acc: 0.6524\n",
      "Epoch 90/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7600 - acc: 0.6495 - val_loss: 0.7767 - val_acc: 0.6490\n",
      "Epoch 91/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7555 - acc: 0.6548 - val_loss: 0.7764 - val_acc: 0.6504\n",
      "Epoch 92/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7553 - acc: 0.6557 - val_loss: 0.7758 - val_acc: 0.6517\n",
      "Epoch 93/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7443 - acc: 0.6552 - val_loss: 0.7699 - val_acc: 0.6510\n",
      "Epoch 94/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7472 - acc: 0.6550 - val_loss: 0.7729 - val_acc: 0.6450\n",
      "Epoch 95/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7530 - acc: 0.6540 - val_loss: 0.7744 - val_acc: 0.6443\n",
      "Epoch 96/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7556 - acc: 0.6518 - val_loss: 0.7740 - val_acc: 0.6484\n",
      "Epoch 97/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7460 - acc: 0.6572 - val_loss: 0.7655 - val_acc: 0.6530\n",
      "Epoch 98/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7461 - acc: 0.6537 - val_loss: 0.8006 - val_acc: 0.6175\n",
      "Epoch 99/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7489 - acc: 0.6560 - val_loss: 0.7745 - val_acc: 0.6410\n",
      "Epoch 100/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7460 - acc: 0.6505 - val_loss: 0.7700 - val_acc: 0.6463\n",
      "Epoch 101/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7439 - acc: 0.6629 - val_loss: 0.7740 - val_acc: 0.6376\n",
      "Epoch 102/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7393 - acc: 0.6612 - val_loss: 0.7735 - val_acc: 0.6457\n",
      "Epoch 103/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7406 - acc: 0.6578 - val_loss: 0.7704 - val_acc: 0.6537\n",
      "Epoch 104/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7365 - acc: 0.6615 - val_loss: 0.7715 - val_acc: 0.6443\n",
      "Epoch 105/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7495 - acc: 0.6588 - val_loss: 0.7770 - val_acc: 0.6430\n",
      "Epoch 106/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7448 - acc: 0.6573 - val_loss: 0.7766 - val_acc: 0.6423\n",
      "Epoch 107/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7433 - acc: 0.6622 - val_loss: 0.7694 - val_acc: 0.6470\n",
      "Epoch 108/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7364 - acc: 0.6666 - val_loss: 0.7822 - val_acc: 0.6524\n",
      "Epoch 109/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7480 - acc: 0.6572 - val_loss: 0.7771 - val_acc: 0.6376\n",
      "Epoch 110/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7463 - acc: 0.6537 - val_loss: 0.7735 - val_acc: 0.6524\n",
      "Epoch 111/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7376 - acc: 0.6615 - val_loss: 0.7691 - val_acc: 0.6510\n",
      "Epoch 112/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7400 - acc: 0.6632 - val_loss: 0.7663 - val_acc: 0.6484\n",
      "Epoch 113/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7444 - acc: 0.6552 - val_loss: 0.7742 - val_acc: 0.6490\n",
      "Epoch 114/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7387 - acc: 0.6593 - val_loss: 0.7733 - val_acc: 0.6457\n",
      "Epoch 115/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7347 - acc: 0.6669 - val_loss: 0.7786 - val_acc: 0.6410\n",
      "Epoch 116/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7417 - acc: 0.6664 - val_loss: 0.7761 - val_acc: 0.6524\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7452 - acc: 0.6535 - val_loss: 0.7694 - val_acc: 0.6537\n",
      "Epoch 118/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7342 - acc: 0.6610 - val_loss: 0.7729 - val_acc: 0.6504\n",
      "Epoch 119/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7383 - acc: 0.6634 - val_loss: 0.7715 - val_acc: 0.6564\n",
      "Epoch 120/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7399 - acc: 0.6647 - val_loss: 0.7748 - val_acc: 0.6477\n",
      "Epoch 121/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7276 - acc: 0.6711 - val_loss: 0.7737 - val_acc: 0.6376\n",
      "Epoch 122/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7535 - acc: 0.6545 - val_loss: 0.7688 - val_acc: 0.6470\n",
      "Epoch 123/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7369 - acc: 0.6676 - val_loss: 0.7669 - val_acc: 0.6450\n",
      "Epoch 124/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7330 - acc: 0.6655 - val_loss: 0.7699 - val_acc: 0.6383\n",
      "Epoch 125/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7263 - acc: 0.6676 - val_loss: 0.7738 - val_acc: 0.6470\n",
      "Epoch 126/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7247 - acc: 0.6691 - val_loss: 0.7755 - val_acc: 0.6564\n",
      "Epoch 127/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7222 - acc: 0.6689 - val_loss: 0.7774 - val_acc: 0.6443\n",
      "Epoch 128/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7326 - acc: 0.6649 - val_loss: 0.7748 - val_acc: 0.6544\n",
      "Epoch 129/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7233 - acc: 0.6662 - val_loss: 0.7700 - val_acc: 0.6437\n",
      "Epoch 130/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7316 - acc: 0.6647 - val_loss: 0.7742 - val_acc: 0.6530\n",
      "Epoch 131/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7299 - acc: 0.6721 - val_loss: 0.7765 - val_acc: 0.6530\n",
      "Epoch 132/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7312 - acc: 0.6637 - val_loss: 0.7836 - val_acc: 0.6544\n",
      "Epoch 133/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7374 - acc: 0.6570 - val_loss: 0.7758 - val_acc: 0.6463\n",
      "Epoch 134/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7276 - acc: 0.6682 - val_loss: 0.7708 - val_acc: 0.6477\n",
      "Epoch 135/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7295 - acc: 0.6686 - val_loss: 0.7747 - val_acc: 0.6463\n",
      "Epoch 136/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7334 - acc: 0.6632 - val_loss: 0.7758 - val_acc: 0.6504\n",
      "Epoch 137/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7261 - acc: 0.6672 - val_loss: 0.7771 - val_acc: 0.6430\n",
      "Epoch 138/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7292 - acc: 0.6659 - val_loss: 0.7840 - val_acc: 0.6524\n",
      "Epoch 139/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7291 - acc: 0.6659 - val_loss: 0.7758 - val_acc: 0.6417\n",
      "Epoch 140/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7189 - acc: 0.6736 - val_loss: 0.7772 - val_acc: 0.6350\n",
      "Epoch 141/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7191 - acc: 0.6751 - val_loss: 0.7797 - val_acc: 0.6423\n",
      "Epoch 142/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7416 - acc: 0.6620 - val_loss: 0.7690 - val_acc: 0.6443\n",
      "Epoch 143/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7449 - acc: 0.6624 - val_loss: 0.7634 - val_acc: 0.6457\n",
      "Epoch 144/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7325 - acc: 0.6659 - val_loss: 0.7755 - val_acc: 0.6343\n",
      "Epoch 145/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7209 - acc: 0.6733 - val_loss: 0.7736 - val_acc: 0.6423\n",
      "Epoch 146/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7274 - acc: 0.6661 - val_loss: 0.7668 - val_acc: 0.6450\n",
      "Epoch 147/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7240 - acc: 0.6711 - val_loss: 0.7816 - val_acc: 0.6256\n",
      "Epoch 148/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7314 - acc: 0.6664 - val_loss: 0.7728 - val_acc: 0.6470\n",
      "Epoch 149/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7282 - acc: 0.6661 - val_loss: 0.7763 - val_acc: 0.6430\n",
      "Epoch 150/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7346 - acc: 0.6639 - val_loss: 0.7789 - val_acc: 0.6504\n",
      "Epoch 151/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7263 - acc: 0.6706 - val_loss: 0.7708 - val_acc: 0.6504\n",
      "Epoch 152/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7273 - acc: 0.6692 - val_loss: 0.7807 - val_acc: 0.6477\n",
      "Epoch 153/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7311 - acc: 0.6609 - val_loss: 0.7785 - val_acc: 0.6457\n",
      "Epoch 154/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7230 - acc: 0.6692 - val_loss: 0.7759 - val_acc: 0.6390\n",
      "Epoch 155/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7339 - acc: 0.6573 - val_loss: 0.7648 - val_acc: 0.6551\n",
      "Epoch 156/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7151 - acc: 0.6741 - val_loss: 0.7818 - val_acc: 0.6524\n",
      "Epoch 157/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7150 - acc: 0.6691 - val_loss: 0.7682 - val_acc: 0.6530\n",
      "Epoch 158/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7184 - acc: 0.6753 - val_loss: 0.7712 - val_acc: 0.6383\n",
      "Epoch 159/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7210 - acc: 0.6667 - val_loss: 0.7774 - val_acc: 0.6390\n",
      "Epoch 160/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7159 - acc: 0.6756 - val_loss: 0.7674 - val_acc: 0.6403\n",
      "Epoch 161/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7138 - acc: 0.6701 - val_loss: 0.7721 - val_acc: 0.6517\n",
      "Epoch 162/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7088 - acc: 0.6771 - val_loss: 0.7763 - val_acc: 0.6437\n",
      "Epoch 163/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7143 - acc: 0.6741 - val_loss: 0.7730 - val_acc: 0.6370\n",
      "Epoch 164/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7335 - acc: 0.6639 - val_loss: 0.7841 - val_acc: 0.6263\n",
      "Epoch 165/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7184 - acc: 0.6687 - val_loss: 0.7771 - val_acc: 0.6336\n",
      "Epoch 166/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7144 - acc: 0.6724 - val_loss: 0.7768 - val_acc: 0.6443\n",
      "Epoch 167/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7164 - acc: 0.6731 - val_loss: 0.7791 - val_acc: 0.6437\n",
      "Epoch 168/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7520 - acc: 0.6466 - val_loss: 0.7800 - val_acc: 0.6276\n",
      "Epoch 169/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7314 - acc: 0.6657 - val_loss: 0.7755 - val_acc: 0.6423\n",
      "Epoch 170/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7333 - acc: 0.6629 - val_loss: 0.7641 - val_acc: 0.6517\n",
      "Epoch 171/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7227 - acc: 0.6714 - val_loss: 0.7663 - val_acc: 0.6524\n",
      "Epoch 172/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7131 - acc: 0.6785 - val_loss: 0.7681 - val_acc: 0.6517\n",
      "Epoch 173/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7166 - acc: 0.6731 - val_loss: 0.7729 - val_acc: 0.6410\n",
      "Epoch 174/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7140 - acc: 0.6739 - val_loss: 0.7733 - val_acc: 0.6470\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7307 - acc: 0.6692 - val_loss: 0.7737 - val_acc: 0.6457\n",
      "Epoch 176/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7295 - acc: 0.6712 - val_loss: 0.7784 - val_acc: 0.6457\n",
      "Epoch 177/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7228 - acc: 0.6734 - val_loss: 0.7699 - val_acc: 0.6383\n",
      "Epoch 178/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7197 - acc: 0.6733 - val_loss: 0.7770 - val_acc: 0.6376\n",
      "Epoch 179/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7172 - acc: 0.6773 - val_loss: 0.7828 - val_acc: 0.6397\n",
      "Epoch 180/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7250 - acc: 0.6719 - val_loss: 0.7806 - val_acc: 0.6517\n",
      "Epoch 181/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7183 - acc: 0.6766 - val_loss: 0.7739 - val_acc: 0.6484\n",
      "Epoch 182/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7151 - acc: 0.6734 - val_loss: 0.7797 - val_acc: 0.6497\n",
      "Epoch 183/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7145 - acc: 0.6731 - val_loss: 0.7737 - val_acc: 0.6450\n",
      "Epoch 184/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7074 - acc: 0.6818 - val_loss: 0.7725 - val_acc: 0.6484\n",
      "Epoch 185/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7038 - acc: 0.6776 - val_loss: 0.7816 - val_acc: 0.6457\n",
      "Epoch 186/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7070 - acc: 0.6816 - val_loss: 0.7793 - val_acc: 0.6463\n",
      "Epoch 187/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7231 - acc: 0.6645 - val_loss: 0.7947 - val_acc: 0.6343\n",
      "Epoch 188/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7215 - acc: 0.6650 - val_loss: 0.7868 - val_acc: 0.6443\n",
      "Epoch 189/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7153 - acc: 0.6753 - val_loss: 0.7756 - val_acc: 0.6450\n",
      "Epoch 190/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6999 - acc: 0.6833 - val_loss: 0.7785 - val_acc: 0.6457\n",
      "Epoch 191/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7031 - acc: 0.6811 - val_loss: 0.7685 - val_acc: 0.6564\n",
      "Epoch 192/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7557 - acc: 0.6583 - val_loss: 0.7913 - val_acc: 0.6289\n",
      "Epoch 193/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7443 - acc: 0.6592 - val_loss: 0.7799 - val_acc: 0.6363\n",
      "Epoch 194/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7174 - acc: 0.6728 - val_loss: 0.7819 - val_acc: 0.6350\n",
      "Epoch 195/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7074 - acc: 0.6773 - val_loss: 0.7811 - val_acc: 0.6551\n",
      "Epoch 196/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7007 - acc: 0.6818 - val_loss: 0.7753 - val_acc: 0.6376\n",
      "Epoch 197/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7033 - acc: 0.6815 - val_loss: 0.7782 - val_acc: 0.6390\n",
      "Epoch 198/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7074 - acc: 0.6803 - val_loss: 0.7773 - val_acc: 0.6470\n",
      "Epoch 199/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7148 - acc: 0.6706 - val_loss: 0.7761 - val_acc: 0.6457\n",
      "Epoch 200/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7037 - acc: 0.6781 - val_loss: 0.7853 - val_acc: 0.6390\n",
      "Epoch 201/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6969 - acc: 0.6841 - val_loss: 0.7835 - val_acc: 0.6283\n",
      "Epoch 202/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7002 - acc: 0.6758 - val_loss: 0.7928 - val_acc: 0.6417\n",
      "Epoch 203/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6940 - acc: 0.6883 - val_loss: 0.7749 - val_acc: 0.6517\n",
      "Epoch 204/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6917 - acc: 0.6852 - val_loss: 0.7835 - val_acc: 0.6450\n",
      "Epoch 205/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6922 - acc: 0.6858 - val_loss: 0.7889 - val_acc: 0.6283\n",
      "Epoch 206/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6902 - acc: 0.6865 - val_loss: 0.7746 - val_acc: 0.6397\n",
      "Epoch 207/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7078 - acc: 0.6793 - val_loss: 0.7909 - val_acc: 0.6383\n",
      "Epoch 208/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7289 - acc: 0.6652 - val_loss: 0.7990 - val_acc: 0.6316\n",
      "Epoch 209/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7094 - acc: 0.6744 - val_loss: 0.7807 - val_acc: 0.6423\n",
      "Epoch 210/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7047 - acc: 0.6835 - val_loss: 0.7592 - val_acc: 0.6564\n",
      "Epoch 211/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.6941 - acc: 0.6882 - val_loss: 0.7642 - val_acc: 0.6423\n",
      "Epoch 212/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6917 - acc: 0.6857 - val_loss: 0.7824 - val_acc: 0.6330\n",
      "Epoch 213/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7068 - acc: 0.6826 - val_loss: 0.7700 - val_acc: 0.6470\n",
      "Epoch 214/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7102 - acc: 0.6764 - val_loss: 0.7883 - val_acc: 0.6504\n",
      "Epoch 215/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7085 - acc: 0.6781 - val_loss: 0.7866 - val_acc: 0.6296\n",
      "Epoch 216/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6934 - acc: 0.6848 - val_loss: 0.7813 - val_acc: 0.6383\n",
      "Epoch 217/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6915 - acc: 0.6872 - val_loss: 0.7794 - val_acc: 0.6470\n",
      "Epoch 218/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6927 - acc: 0.6895 - val_loss: 0.7794 - val_acc: 0.6356\n",
      "Epoch 219/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6846 - acc: 0.6924 - val_loss: 0.7960 - val_acc: 0.6410\n",
      "Epoch 220/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6909 - acc: 0.6865 - val_loss: 0.7758 - val_acc: 0.6437\n",
      "Epoch 221/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7026 - acc: 0.6736 - val_loss: 0.7751 - val_acc: 0.6450\n",
      "Epoch 222/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6832 - acc: 0.6950 - val_loss: 0.8076 - val_acc: 0.6403\n",
      "Epoch 223/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6960 - acc: 0.6867 - val_loss: 0.7864 - val_acc: 0.6564\n",
      "Epoch 224/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6960 - acc: 0.6836 - val_loss: 0.7870 - val_acc: 0.6470\n",
      "Epoch 225/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6990 - acc: 0.6783 - val_loss: 0.7817 - val_acc: 0.6504\n",
      "Epoch 226/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6852 - acc: 0.6860 - val_loss: 0.7874 - val_acc: 0.6423\n",
      "Epoch 227/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6844 - acc: 0.6863 - val_loss: 0.7742 - val_acc: 0.6417\n",
      "Epoch 228/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6833 - acc: 0.6903 - val_loss: 0.7937 - val_acc: 0.6370\n",
      "Epoch 229/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6832 - acc: 0.6890 - val_loss: 0.7995 - val_acc: 0.6363\n",
      "Epoch 230/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6840 - acc: 0.6860 - val_loss: 0.7948 - val_acc: 0.6430\n",
      "Epoch 231/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6800 - acc: 0.6930 - val_loss: 0.7975 - val_acc: 0.6383\n",
      "Epoch 232/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6830 - acc: 0.6887 - val_loss: 0.7927 - val_acc: 0.6450\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6899 - acc: 0.6818 - val_loss: 0.7886 - val_acc: 0.6376\n",
      "Epoch 234/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6816 - acc: 0.6947 - val_loss: 0.7781 - val_acc: 0.6477\n",
      "Epoch 235/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6880 - acc: 0.6850 - val_loss: 0.7916 - val_acc: 0.6397\n",
      "Epoch 236/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6817 - acc: 0.6942 - val_loss: 0.7999 - val_acc: 0.6263\n",
      "Epoch 237/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6909 - acc: 0.6882 - val_loss: 0.7813 - val_acc: 0.6417\n",
      "Epoch 238/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6869 - acc: 0.6897 - val_loss: 0.7822 - val_acc: 0.6497\n",
      "Epoch 239/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6851 - acc: 0.6863 - val_loss: 0.8002 - val_acc: 0.6457\n",
      "Epoch 240/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6921 - acc: 0.6783 - val_loss: 0.7798 - val_acc: 0.6504\n",
      "Epoch 241/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6843 - acc: 0.6860 - val_loss: 0.7814 - val_acc: 0.6417\n",
      "Epoch 242/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6903 - acc: 0.6841 - val_loss: 0.7900 - val_acc: 0.6463\n",
      "Epoch 243/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6828 - acc: 0.6930 - val_loss: 0.7945 - val_acc: 0.6376\n",
      "Epoch 244/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6996 - acc: 0.6790 - val_loss: 0.7912 - val_acc: 0.6350\n",
      "Epoch 245/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6881 - acc: 0.6858 - val_loss: 0.7866 - val_acc: 0.6350\n",
      "Epoch 246/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6798 - acc: 0.6920 - val_loss: 0.7965 - val_acc: 0.6403\n",
      "Epoch 247/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6742 - acc: 0.6972 - val_loss: 0.7938 - val_acc: 0.6350\n",
      "Epoch 248/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6730 - acc: 0.6942 - val_loss: 0.8016 - val_acc: 0.6343\n",
      "Epoch 249/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6684 - acc: 0.6944 - val_loss: 0.7926 - val_acc: 0.6397\n",
      "Epoch 250/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6799 - acc: 0.6870 - val_loss: 0.8056 - val_acc: 0.6363\n",
      "Epoch 251/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6931 - acc: 0.6845 - val_loss: 0.7936 - val_acc: 0.6343\n",
      "Epoch 252/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7220 - acc: 0.6669 - val_loss: 0.7957 - val_acc: 0.6397\n",
      "Epoch 253/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7022 - acc: 0.6769 - val_loss: 0.7972 - val_acc: 0.6430\n",
      "Epoch 254/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6832 - acc: 0.6900 - val_loss: 0.7976 - val_acc: 0.6470\n",
      "Epoch 255/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6705 - acc: 0.6935 - val_loss: 0.8114 - val_acc: 0.6336\n",
      "Epoch 256/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6739 - acc: 0.6954 - val_loss: 0.8091 - val_acc: 0.6289\n",
      "Epoch 257/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6841 - acc: 0.6872 - val_loss: 0.8028 - val_acc: 0.6423\n",
      "Epoch 258/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6739 - acc: 0.6929 - val_loss: 0.8063 - val_acc: 0.6323\n",
      "Epoch 259/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6644 - acc: 0.6964 - val_loss: 0.7902 - val_acc: 0.6450\n",
      "Epoch 260/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6706 - acc: 0.6971 - val_loss: 0.8074 - val_acc: 0.6269\n",
      "Epoch 261/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6790 - acc: 0.6885 - val_loss: 0.8001 - val_acc: 0.6350\n",
      "Epoch 262/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6826 - acc: 0.6833 - val_loss: 0.8055 - val_acc: 0.6383\n",
      "Epoch 263/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6803 - acc: 0.6895 - val_loss: 0.8034 - val_acc: 0.6370\n",
      "Epoch 264/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6683 - acc: 0.6919 - val_loss: 0.8089 - val_acc: 0.6463\n",
      "Epoch 265/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6565 - acc: 0.7034 - val_loss: 0.8141 - val_acc: 0.6350\n",
      "Epoch 266/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6613 - acc: 0.6967 - val_loss: 0.8145 - val_acc: 0.6316\n",
      "Epoch 267/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6786 - acc: 0.6847 - val_loss: 0.7858 - val_acc: 0.6437\n",
      "Epoch 268/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6746 - acc: 0.6922 - val_loss: 0.7901 - val_acc: 0.6403\n",
      "Epoch 269/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6821 - acc: 0.6900 - val_loss: 0.8068 - val_acc: 0.6289\n",
      "Epoch 270/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6738 - acc: 0.6986 - val_loss: 0.8098 - val_acc: 0.6397\n",
      "Epoch 271/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6589 - acc: 0.6987 - val_loss: 0.8087 - val_acc: 0.6477\n",
      "Epoch 272/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6609 - acc: 0.7053 - val_loss: 0.8243 - val_acc: 0.6350\n",
      "Epoch 273/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6570 - acc: 0.7012 - val_loss: 0.8428 - val_acc: 0.6303\n",
      "Epoch 274/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6668 - acc: 0.6965 - val_loss: 0.8169 - val_acc: 0.6397\n",
      "Epoch 275/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6556 - acc: 0.7051 - val_loss: 0.8304 - val_acc: 0.6309\n",
      "Epoch 276/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6620 - acc: 0.7029 - val_loss: 0.8249 - val_acc: 0.6430\n",
      "Epoch 277/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6741 - acc: 0.6940 - val_loss: 0.8049 - val_acc: 0.6336\n",
      "Epoch 278/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6684 - acc: 0.6949 - val_loss: 0.8070 - val_acc: 0.6417\n",
      "Epoch 279/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6607 - acc: 0.7041 - val_loss: 0.8124 - val_acc: 0.6430\n",
      "Epoch 280/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6773 - acc: 0.6914 - val_loss: 0.8339 - val_acc: 0.6383\n",
      "Epoch 281/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6769 - acc: 0.6934 - val_loss: 0.8044 - val_acc: 0.6397\n",
      "Epoch 282/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.6576 - acc: 0.7022 - val_loss: 0.8175 - val_acc: 0.6309\n",
      "Epoch 283/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6710 - acc: 0.6920 - val_loss: 0.8519 - val_acc: 0.6169\n",
      "Epoch 284/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6969 - acc: 0.6790 - val_loss: 0.8321 - val_acc: 0.6256\n",
      "Epoch 285/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6698 - acc: 0.6902 - val_loss: 0.8015 - val_acc: 0.6457\n",
      "Epoch 286/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6581 - acc: 0.6965 - val_loss: 0.8264 - val_acc: 0.6383\n",
      "Epoch 287/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6516 - acc: 0.6992 - val_loss: 0.8151 - val_acc: 0.6484\n",
      "Epoch 288/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.6592 - acc: 0.7002 - val_loss: 0.8256 - val_acc: 0.6410\n",
      "Epoch 289/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.6859 - acc: 0.6865 - val_loss: 0.7950 - val_acc: 0.6336\n",
      "Epoch 290/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6721 - acc: 0.6940 - val_loss: 0.8033 - val_acc: 0.6417\n",
      "Epoch 291/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6771 - acc: 0.6937 - val_loss: 0.8233 - val_acc: 0.6403\n",
      "Epoch 292/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6696 - acc: 0.7001 - val_loss: 0.8172 - val_acc: 0.6390\n",
      "Epoch 293/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6637 - acc: 0.7006 - val_loss: 0.8189 - val_acc: 0.6463\n",
      "Epoch 294/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.6502 - acc: 0.7056 - val_loss: 0.8343 - val_acc: 0.6376\n",
      "Epoch 295/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.6538 - acc: 0.7079 - val_loss: 0.8335 - val_acc: 0.6336\n",
      "Epoch 296/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6658 - acc: 0.6971 - val_loss: 0.8050 - val_acc: 0.6403\n",
      "Epoch 297/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.6741 - acc: 0.6937 - val_loss: 0.8285 - val_acc: 0.6330\n",
      "Epoch 298/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.6589 - acc: 0.6989 - val_loss: 0.8050 - val_acc: 0.6450\n",
      "Epoch 299/1000\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.6936 - acc: 0.6808 - val_loss: 0.8187 - val_acc: 0.6296\n",
      "Epoch 300/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6549 - acc: 0.7029 - val_loss: 0.8477 - val_acc: 0.6356\n",
      "Epoch 301/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6410 - acc: 0.7096 - val_loss: 0.8321 - val_acc: 0.6390\n",
      "Epoch 302/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6379 - acc: 0.7088 - val_loss: 0.8323 - val_acc: 0.6330\n",
      "Epoch 303/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6470 - acc: 0.7054 - val_loss: 0.8318 - val_acc: 0.6390\n",
      "Epoch 304/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6403 - acc: 0.7059 - val_loss: 0.8399 - val_acc: 0.6222\n",
      "Epoch 305/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6562 - acc: 0.7012 - val_loss: 0.8351 - val_acc: 0.6303\n",
      "Epoch 306/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6626 - acc: 0.7002 - val_loss: 0.8227 - val_acc: 0.6410\n",
      "Epoch 307/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7073 - acc: 0.6810 - val_loss: 0.8134 - val_acc: 0.6343\n",
      "Epoch 308/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6664 - acc: 0.6996 - val_loss: 0.8266 - val_acc: 0.6209\n",
      "Epoch 309/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6568 - acc: 0.6999 - val_loss: 0.8325 - val_acc: 0.6242\n",
      "Epoch 310/1000\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.6538 - acc: 0.7029 - val_loss: 0.8358 - val_acc: 0.6242\n",
      "Epoch 310: early stopping\n"
     ]
    }
   ],
   "source": [
    "multi_lstm_model_label = KerasClassifier(build_fn = stacked_lstm_label\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])\n",
    "multi_lstm_model_label.fit(X3_train_label, y3_train_label)\n",
    "y3_pred_multi_lstm_label = multi_lstm_model_label.predict(X3_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5964a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM multi  layer accouracy :  0.6254019292604501\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM multi  layer accouracy : \", accuracy_score(y3_pred_multi_lstm_label,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbde336",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b458f5",
   "metadata": {},
   "source": [
    "one-hot encoding 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f7ff2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN single layer accouracy :  0.6377277599142551\n",
      "RNN multi  layer accouracy :  0.6371918542336549\n",
      "LSTM single layer accouracy :  0.6382636655948553\n",
      "LSTM multi  layer accouracy :  0.6312968917470525\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN single layer accouracy : \", accuracy_score(y3_pred_rnn_ohe,y3_test_label))\n",
    "print(\"RNN multi  layer accouracy : \", accuracy_score(y3_pred_multi_rnn_ohe,y3_test_label))\n",
    "print(\"LSTM single layer accouracy : \", accuracy_score(y3_pred_single_lstm,y3_test_label))\n",
    "print(\"LSTM multi  layer accouracy : \", accuracy_score(y3_pred_multi_lstm_ohe,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3a421",
   "metadata": {},
   "source": [
    "label encoding 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d727ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN single layer accouracy :  0.6355841371918542\n",
      "RNN multi  layer accouracy :  0.6361200428724545\n",
      "LSTM single layer accouracy :  0.6334405144694534\n",
      "LSTM multi  layer accouracy :  0.6254019292604501\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN single layer accouracy : \", accuracy_score(y3_pred_rnn_label,y3_test_label))\n",
    "print(\"RNN multi  layer accouracy : \", accuracy_score(y3_pred_multi_rnn_label,y3_test_label))\n",
    "print(\"LSTM single layer accouracy : \", accuracy_score(y3_pred_single_lstm_label,y3_test_label))\n",
    "print(\"LSTM multi  layer accouracy : \", accuracy_score(y3_pred_multi_lstm_label,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c686389",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
