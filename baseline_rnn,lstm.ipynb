{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0b69bf",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8474f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "from scipy import stats #Analysis \n",
    "from scipy.stats import norm \n",
    "\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import matplotli# Importb.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800f577b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\pythontemp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507af345",
   "metadata": {},
   "source": [
    "# 1. Data Load  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23305c3e",
   "metadata": {},
   "source": [
    "**치매 예방을 위한 라이프로그 치매 분류**\n",
    "\n",
    "**9,327 rows × 66 columns**\n",
    "\n",
    "For more details https://aihub.or.kr/problem_contest/nipa-learning-platform/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb32c5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>summary_date</th>\n",
       "      <th>activity_average_met</th>\n",
       "      <th>activity_cal_active</th>\n",
       "      <th>activity_cal_total</th>\n",
       "      <th>activity_class_5min</th>\n",
       "      <th>activity_daily_movement</th>\n",
       "      <th>activity_high</th>\n",
       "      <th>activity_inactive</th>\n",
       "      <th>activity_inactivity_alerts</th>\n",
       "      <th>...</th>\n",
       "      <th>sleep_temperature_deviation</th>\n",
       "      <th>sleep_temperature_trend_deviation</th>\n",
       "      <th>timezone</th>\n",
       "      <th>sleep_total</th>\n",
       "      <th>CONVERT(activity_class_5min USING utf8)</th>\n",
       "      <th>CONVERT(activity_met_1min USING utf8)</th>\n",
       "      <th>CONVERT(sleep_hr_5min USING utf8)</th>\n",
       "      <th>CONVERT(sleep_hypnogram_5min USING utf8)</th>\n",
       "      <th>CONVERT(sleep_rmssd_5min USING utf8)</th>\n",
       "      <th>DIAG_NM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nia+404@rowan.kr</td>\n",
       "      <td>2020-11-27</td>\n",
       "      <td>1.71875</td>\n",
       "      <td>730</td>\n",
       "      <td>2944</td>\n",
       "      <td>...</td>\n",
       "      <td>14346</td>\n",
       "      <td>0</td>\n",
       "      <td>417</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>2/1/1/1/1/1/2/2/1/1/1/1/1/1/2/2/2/3/2/2/2/2/2/...</td>\n",
       "      <td>0.9/0.9/1.4/1.9/1.1/0.9/0.9/1.1/1.3/1/0.9/1.1/...</td>\n",
       "      <td>0/73/73/73/72/71/70/71/71/71/70/70/73/72/74/74...</td>\n",
       "      <td>4/2/4/3/3/1/2/2/2/2/2/2/3/3/3/4/4/3/2/2/2/2/2/...</td>\n",
       "      <td>0/10/10/10/11/11/10/12/18/13/14/12/10/10/18/17...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nia+404@rowan.kr</td>\n",
       "      <td>2020-11-28</td>\n",
       "      <td>1.40625</td>\n",
       "      <td>342</td>\n",
       "      <td>2449</td>\n",
       "      <td>...</td>\n",
       "      <td>6352</td>\n",
       "      <td>0</td>\n",
       "      <td>473</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/2/2/2/2/2/2/...</td>\n",
       "      <td>1.2/1.1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....</td>\n",
       "      <td>69/70/69/69/70/72/71/72/70/69/69/69/68/68/63/6...</td>\n",
       "      <td>2/4/2/2/2/2/3/1/2/2/4/4/2/2/2/2/2/2/2/2/2/2/4/...</td>\n",
       "      <td>23/23/26/24/18/13/15/14/17/20/24/30/23/25/22/1...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nia+404@rowan.kr</td>\n",
       "      <td>2020-11-29</td>\n",
       "      <td>1.46875</td>\n",
       "      <td>401</td>\n",
       "      <td>2544</td>\n",
       "      <td>...</td>\n",
       "      <td>7297</td>\n",
       "      <td>0</td>\n",
       "      <td>586</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/1/2/1/1/1/1/2/2/2/2/2/1/1/1/1/1/2/...</td>\n",
       "      <td>1.1/1.1/1.2/1.1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....</td>\n",
       "      <td>0/74/73/73/74/74/74/71/71/70/70/69/70/68/66/69...</td>\n",
       "      <td>4/2/4/4/1/1/1/4/4/4/4/4/4/4/2/3/4/2/2/4/2/2/2/...</td>\n",
       "      <td>0/11/14/20/13/14/14/16/27/29/27/20/19/19/14/12...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nia+404@rowan.kr</td>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>27</td>\n",
       "      <td>1850</td>\n",
       "      <td>...</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>2/1/2/2/1/2/1/1/2/1/1/1/1/1/2/1/1/1/1/1/2/2/2/...</td>\n",
       "      <td>0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/...</td>\n",
       "      <td>73/70/71/72/75/75/73/70/70/70/67/63/63/63/63/6...</td>\n",
       "      <td>4/4/4/4/3/3/3/2/4/4/4/2/2/2/2/2/2/2/2/4/2/2/2/...</td>\n",
       "      <td>24/28/19/17/12/10/17/20/23/23/25/31/26/25/34/3...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nia+404@rowan.kr</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>1.46875</td>\n",
       "      <td>333</td>\n",
       "      <td>2518</td>\n",
       "      <td>...</td>\n",
       "      <td>5861</td>\n",
       "      <td>0</td>\n",
       "      <td>646</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/2/2/3/3/2/...</td>\n",
       "      <td>0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....</td>\n",
       "      <td>0/0/0/0/0/0/0/0/69/69/71/69/65/66/64/64/65/66/...</td>\n",
       "      <td>4/4/4/4/4/4/4/4/4/4/4/2/2/2/2/3/3/2/4/4/4/2/2/...</td>\n",
       "      <td>0/0/0/0/0/0/0/0/21/22/26/23/19/29/22/17/14/13/...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>nia+206@rowan.kr</td>\n",
       "      <td>2020-12-22</td>\n",
       "      <td>1.34375</td>\n",
       "      <td>227</td>\n",
       "      <td>2316</td>\n",
       "      <td>...</td>\n",
       "      <td>3863</td>\n",
       "      <td>3</td>\n",
       "      <td>735</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>2/1/1/2/2/1/1/1/1/2/2/3/2/1/1/1/1/1/1/1/1/1/1/...</td>\n",
       "      <td>1.1/1.4/1.2/0.9/1.2/1.1/0.9/0.9/0.9/1/0.9/0.9/...</td>\n",
       "      <td>0/54/54/54/55/56/0/0/55/52/52/53/54/54/56/57/6...</td>\n",
       "      <td>4/4/4/2/2/2/2/4/2/2/2/1/1/1/1/4/2/2/3/3/3/3/3/...</td>\n",
       "      <td>0/35/39/28/26/41/0/0/52/31/27/30/21/22/27/40/3...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>nia+206@rowan.kr</td>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>1.34375</td>\n",
       "      <td>249</td>\n",
       "      <td>2351</td>\n",
       "      <td>...</td>\n",
       "      <td>4411</td>\n",
       "      <td>1</td>\n",
       "      <td>780</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/1/1/1/1/2/3/2/2/2/1/1/1/1/1/1/1/1/...</td>\n",
       "      <td>0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....</td>\n",
       "      <td>68/66/67/67/68/69/69/70/71/71/71/69/72/70/70/7...</td>\n",
       "      <td>4/4/2/2/2/1/1/1/1/1/1/2/2/3/3/2/2/2/2/2/3/4/2/...</td>\n",
       "      <td>26/16/19/18/16/18/19/17/15/16/15/13/11/13/13/1...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>nia+206@rowan.kr</td>\n",
       "      <td>2020-12-26</td>\n",
       "      <td>1.53125</td>\n",
       "      <td>570</td>\n",
       "      <td>2682</td>\n",
       "      <td>...</td>\n",
       "      <td>11057</td>\n",
       "      <td>1</td>\n",
       "      <td>518</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.41</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/2/2/2/2/2/2/1/1/1/1/1/1/1/1/1/1/1/...</td>\n",
       "      <td>0.9/0.9/0.9/0.9/1.1/0.9/0.9/1.1/0.9/0.9/0.9/0....</td>\n",
       "      <td>0/0/65/65/66/67/69/72/73/72/73/74/73/74/75/75/...</td>\n",
       "      <td>4/4/2/2/2/2/2/2/2/2/2/2/4/4/4/4/2/3/3/3/2/2/4/...</td>\n",
       "      <td>0/0/13/12/12/13/12/12/12/16/13/12/13/17/13/18/...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>nia+206@rowan.kr</td>\n",
       "      <td>2020-12-27</td>\n",
       "      <td>1.34375</td>\n",
       "      <td>295</td>\n",
       "      <td>2331</td>\n",
       "      <td>...</td>\n",
       "      <td>5135</td>\n",
       "      <td>4</td>\n",
       "      <td>578</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/2/3/2/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/...</td>\n",
       "      <td>1.1/0.9/0.9/0.9/1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/...</td>\n",
       "      <td>0/0/0/64/64/64/64/65/65/66/66/66/66/68/68/68/6...</td>\n",
       "      <td>4/4/4/4/2/2/2/2/2/2/2/2/2/3/3/3/3/2/4/4/2/2/2/...</td>\n",
       "      <td>0/0/0/14/17/18/14/14/17/15/17/12/17/12/13/12/1...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>nia+206@rowan.kr</td>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>1.31250</td>\n",
       "      <td>186</td>\n",
       "      <td>2280</td>\n",
       "      <td>...</td>\n",
       "      <td>3178</td>\n",
       "      <td>0</td>\n",
       "      <td>778</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>99.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "      <td>1/1/1/1/1/1/1/1/1/1/1/1/2/2/0/0/0/0/0/0/0/0/0/...</td>\n",
       "      <td>0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....</td>\n",
       "      <td>0/60/60/61/60/60/60/62/64/64/64/64/65/68/70/70...</td>\n",
       "      <td>4/4/1/2/2/2/2/1/2/2/2/1/1/1/3/3/1/1/2/2/3/3/3/...</td>\n",
       "      <td>0/14/15/14/15/16/17/16/16/18/18/20/23/15/21/20...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 EMAIL summary_date  activity_average_met  \\\n",
       "0     nia+404@rowan.kr   2020-11-27               1.71875   \n",
       "1     nia+404@rowan.kr   2020-11-28               1.40625   \n",
       "2     nia+404@rowan.kr   2020-11-29               1.46875   \n",
       "3     nia+404@rowan.kr   2020-11-30               0.34375   \n",
       "4     nia+404@rowan.kr   2020-12-01               1.46875   \n",
       "...                ...          ...                   ...   \n",
       "9322  nia+206@rowan.kr   2020-12-22               1.34375   \n",
       "9323  nia+206@rowan.kr   2020-12-24               1.34375   \n",
       "9324  nia+206@rowan.kr   2020-12-26               1.53125   \n",
       "9325  nia+206@rowan.kr   2020-12-27               1.34375   \n",
       "9326  nia+206@rowan.kr   2020-12-28               1.31250   \n",
       "\n",
       "      activity_cal_active  activity_cal_total activity_class_5min  \\\n",
       "0                     730                2944                 ...   \n",
       "1                     342                2449                 ...   \n",
       "2                     401                2544                 ...   \n",
       "3                      27                1850                 ...   \n",
       "4                     333                2518                 ...   \n",
       "...                   ...                 ...                 ...   \n",
       "9322                  227                2316                 ...   \n",
       "9323                  249                2351                 ...   \n",
       "9324                  570                2682                 ...   \n",
       "9325                  295                2331                 ...   \n",
       "9326                  186                2280                 ...   \n",
       "\n",
       "      activity_daily_movement  activity_high  activity_inactive  \\\n",
       "0                       14346              0                417   \n",
       "1                        6352              0                473   \n",
       "2                        7297              0                586   \n",
       "3                         491              0                176   \n",
       "4                        5861              0                646   \n",
       "...                       ...            ...                ...   \n",
       "9322                     3863              3                735   \n",
       "9323                     4411              1                780   \n",
       "9324                    11057              1                518   \n",
       "9325                     5135              4                578   \n",
       "9326                     3178              0                778   \n",
       "\n",
       "      activity_inactivity_alerts  ...  sleep_temperature_deviation  \\\n",
       "0                              0  ...                        -0.12   \n",
       "1                              0  ...                        -0.32   \n",
       "2                              0  ...                         0.07   \n",
       "3                              0  ...                        -0.41   \n",
       "4                              0  ...                        -0.27   \n",
       "...                          ...  ...                          ...   \n",
       "9322                           2  ...                        -0.16   \n",
       "9323                           4  ...                        -0.09   \n",
       "9324                           1  ...                         0.41   \n",
       "9325                           0  ...                         0.27   \n",
       "9326                           1  ...                        -0.05   \n",
       "\n",
       "      sleep_temperature_trend_deviation timezone  sleep_total  \\\n",
       "0                                 99.99      NaN           \\r   \n",
       "1                                 99.99      NaN           \\r   \n",
       "2                                 99.99      NaN           \\r   \n",
       "3                                 99.99      NaN           \\r   \n",
       "4                                 99.99      NaN           \\r   \n",
       "...                                 ...      ...          ...   \n",
       "9322                              99.99      NaN           \\r   \n",
       "9323                              99.99      NaN           \\r   \n",
       "9324                              99.99      NaN           \\r   \n",
       "9325                              99.99      NaN           \\r   \n",
       "9326                              99.99      NaN           \\r   \n",
       "\n",
       "                CONVERT(activity_class_5min USING utf8)  \\\n",
       "0     2/1/1/1/1/1/2/2/1/1/1/1/1/1/2/2/2/3/2/2/2/2/2/...   \n",
       "1     1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/2/2/2/2/2/2/...   \n",
       "2     1/1/1/1/1/1/1/2/1/1/1/1/2/2/2/2/2/1/1/1/1/1/2/...   \n",
       "3     2/1/2/2/1/2/1/1/2/1/1/1/1/1/2/1/1/1/1/1/2/2/2/...   \n",
       "4     1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/2/2/3/3/2/...   \n",
       "...                                                 ...   \n",
       "9322  2/1/1/2/2/1/1/1/1/2/2/3/2/1/1/1/1/1/1/1/1/1/1/...   \n",
       "9323  1/1/1/1/1/1/1/1/1/1/2/3/2/2/2/1/1/1/1/1/1/1/1/...   \n",
       "9324  1/1/1/1/1/1/2/2/2/2/2/2/1/1/1/1/1/1/1/1/1/1/1/...   \n",
       "9325  1/1/1/2/3/2/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/...   \n",
       "9326  1/1/1/1/1/1/1/1/1/1/1/1/2/2/0/0/0/0/0/0/0/0/0/...   \n",
       "\n",
       "                  CONVERT(activity_met_1min USING utf8)  \\\n",
       "0     0.9/0.9/1.4/1.9/1.1/0.9/0.9/1.1/1.3/1/0.9/1.1/...   \n",
       "1     1.2/1.1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....   \n",
       "2     1.1/1.1/1.2/1.1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....   \n",
       "3     0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/...   \n",
       "4     0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....   \n",
       "...                                                 ...   \n",
       "9322  1.1/1.4/1.2/0.9/1.2/1.1/0.9/0.9/0.9/1/0.9/0.9/...   \n",
       "9323  0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....   \n",
       "9324  0.9/0.9/0.9/0.9/1.1/0.9/0.9/1.1/0.9/0.9/0.9/0....   \n",
       "9325  1.1/0.9/0.9/0.9/1/0.9/0.9/0.9/0.9/0.9/0.9/0.9/...   \n",
       "9326  0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0.9/0....   \n",
       "\n",
       "                      CONVERT(sleep_hr_5min USING utf8)  \\\n",
       "0     0/73/73/73/72/71/70/71/71/71/70/70/73/72/74/74...   \n",
       "1     69/70/69/69/70/72/71/72/70/69/69/69/68/68/63/6...   \n",
       "2     0/74/73/73/74/74/74/71/71/70/70/69/70/68/66/69...   \n",
       "3     73/70/71/72/75/75/73/70/70/70/67/63/63/63/63/6...   \n",
       "4     0/0/0/0/0/0/0/0/69/69/71/69/65/66/64/64/65/66/...   \n",
       "...                                                 ...   \n",
       "9322  0/54/54/54/55/56/0/0/55/52/52/53/54/54/56/57/6...   \n",
       "9323  68/66/67/67/68/69/69/70/71/71/71/69/72/70/70/7...   \n",
       "9324  0/0/65/65/66/67/69/72/73/72/73/74/73/74/75/75/...   \n",
       "9325  0/0/0/64/64/64/64/65/65/66/66/66/66/68/68/68/6...   \n",
       "9326  0/60/60/61/60/60/60/62/64/64/64/64/65/68/70/70...   \n",
       "\n",
       "               CONVERT(sleep_hypnogram_5min USING utf8)  \\\n",
       "0     4/2/4/3/3/1/2/2/2/2/2/2/3/3/3/4/4/3/2/2/2/2/2/...   \n",
       "1     2/4/2/2/2/2/3/1/2/2/4/4/2/2/2/2/2/2/2/2/2/2/4/...   \n",
       "2     4/2/4/4/1/1/1/4/4/4/4/4/4/4/2/3/4/2/2/4/2/2/2/...   \n",
       "3     4/4/4/4/3/3/3/2/4/4/4/2/2/2/2/2/2/2/2/4/2/2/2/...   \n",
       "4     4/4/4/4/4/4/4/4/4/4/4/2/2/2/2/3/3/2/4/4/4/2/2/...   \n",
       "...                                                 ...   \n",
       "9322  4/4/4/2/2/2/2/4/2/2/2/1/1/1/1/4/2/2/3/3/3/3/3/...   \n",
       "9323  4/4/2/2/2/1/1/1/1/1/1/2/2/3/3/2/2/2/2/2/3/4/2/...   \n",
       "9324  4/4/2/2/2/2/2/2/2/2/2/2/4/4/4/4/2/3/3/3/2/2/4/...   \n",
       "9325  4/4/4/4/2/2/2/2/2/2/2/2/2/3/3/3/3/2/4/4/2/2/2/...   \n",
       "9326  4/4/1/2/2/2/2/1/2/2/2/1/1/1/3/3/1/1/2/2/3/3/3/...   \n",
       "\n",
       "                   CONVERT(sleep_rmssd_5min USING utf8)  DIAG_NM  \n",
       "0     0/10/10/10/11/11/10/12/18/13/14/12/10/10/18/17...       CN  \n",
       "1     23/23/26/24/18/13/15/14/17/20/24/30/23/25/22/1...       CN  \n",
       "2     0/11/14/20/13/14/14/16/27/29/27/20/19/19/14/12...       CN  \n",
       "3     24/28/19/17/12/10/17/20/23/23/25/31/26/25/34/3...       CN  \n",
       "4     0/0/0/0/0/0/0/0/21/22/26/23/19/29/22/17/14/13/...       CN  \n",
       "...                                                 ...      ...  \n",
       "9322  0/35/39/28/26/41/0/0/52/31/27/30/21/22/27/40/3...       CN  \n",
       "9323  26/16/19/18/16/18/19/17/15/16/15/13/11/13/13/1...       CN  \n",
       "9324  0/0/13/12/12/13/12/12/12/16/13/12/13/17/13/18/...       CN  \n",
       "9325  0/0/0/14/17/18/14/14/17/15/17/12/17/12/13/12/1...       CN  \n",
       "9326  0/14/15/14/15/16/17/16/16/18/18/20/23/15/21/20...       CN  \n",
       "\n",
       "[9327 rows x 66 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./Dataset/\"\n",
    "\n",
    "# original data : data \n",
    "data =  pd.read_csv(path + \"dementia_data.csv\", parse_dates=['summary_date']) # 애초에 datatime 유형의 데이터를 읽어 올 때부터 형변환하기\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c72751",
   "metadata": {},
   "source": [
    "# 2. EDA  \n",
    "\n",
    "**9,327 rows × 66 columns**\n",
    "\n",
    "148명에 대한 데일리 라이프로그 데이터  \n",
    "크게 **1. 기본 정보 2. 걸음거리 3.수면 4.컨버팅 데이터**로 나눌 수 있겠습니다. \n",
    "\n",
    "**4. converting**\n",
    "- CONVERT(activity_class_5min USING utf8)    \n",
    "- CONVERT(activity_met_1min USING utf8)      \n",
    "- CONVERT(sleep_hr_5min USING utf8)          \n",
    "- CONVERT(sleep_hypnogram_5min USING utf8)  \n",
    "- CONVERT(sleep_rmssd_5min USING utf8) \n",
    "\n",
    "**y variable**\n",
    "- DIAG_NM \n",
    "    - 정상(CN), 경도인지 장애(MCI), 치매(Dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8f4ee4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9327 entries, 0 to 9326\n",
      "Data columns (total 66 columns):\n",
      " #   Column                                    Non-Null Count  Dtype         \n",
      "---  ------                                    --------------  -----         \n",
      " 0   EMAIL                                     9327 non-null   object        \n",
      " 1   summary_date                              9327 non-null   datetime64[ns]\n",
      " 2   activity_average_met                      9327 non-null   float64       \n",
      " 3   activity_cal_active                       9327 non-null   int64         \n",
      " 4   activity_cal_total                        9327 non-null   int64         \n",
      " 5   activity_class_5min                       9327 non-null   object        \n",
      " 6   activity_daily_movement                   9327 non-null   int64         \n",
      " 7   activity_high                             9327 non-null   int64         \n",
      " 8   activity_inactive                         9327 non-null   int64         \n",
      " 9   activity_inactivity_alerts                9327 non-null   int64         \n",
      " 10  activity_low                              9327 non-null   int64         \n",
      " 11  activity_medium                           9327 non-null   int64         \n",
      " 12  activity_met_1min                         9327 non-null   object        \n",
      " 13  activity_met_min_high                     9327 non-null   int64         \n",
      " 14  activity_met_min_inactive                 9327 non-null   int64         \n",
      " 15  activity_met_min_low                      9327 non-null   int64         \n",
      " 16  activity_met_min_medium                   9327 non-null   int64         \n",
      " 17  activity_non_wear                         9327 non-null   int64         \n",
      " 18  activity_rest                             9327 non-null   int64         \n",
      " 19  activity_score                            9327 non-null   int64         \n",
      " 20  activity_score_meet_daily_targets         9327 non-null   int64         \n",
      " 21  activity_score_move_every_hour            9327 non-null   int64         \n",
      " 22  activity_score_recovery_time              9327 non-null   int64         \n",
      " 23  activity_score_stay_active                9327 non-null   int64         \n",
      " 24  activity_score_training_frequency         9327 non-null   int64         \n",
      " 25  activity_score_training_volume            9327 non-null   int64         \n",
      " 26  activity_steps                            9327 non-null   int64         \n",
      " 27  activity_total                            9327 non-null   int64         \n",
      " 28  sleep_awake                               9327 non-null   int64         \n",
      " 29  sleep_breath_average                      9327 non-null   float64       \n",
      " 30  sleep_deep                                9327 non-null   int64         \n",
      " 31  sleep_duration                            9327 non-null   int64         \n",
      " 32  sleep_efficiency                          9327 non-null   int64         \n",
      " 33  sleep_hr_5min                             9327 non-null   object        \n",
      " 34  sleep_hr_average                          9327 non-null   float64       \n",
      " 35  sleep_hr_lowest                           9327 non-null   int64         \n",
      " 36  sleep_hypnogram_5min                      9327 non-null   object        \n",
      " 37  sleep_is_longest                          9327 non-null   int64         \n",
      " 38  sleep_light                               9327 non-null   int64         \n",
      " 39  sleep_midpoint_at_delta                   9327 non-null   int64         \n",
      " 40  sleep_midpoint_time                       9327 non-null   int64         \n",
      " 41  sleep_onset_latency                       9327 non-null   int64         \n",
      " 42  sleep_period_id                           9327 non-null   int64         \n",
      " 43  sleep_rem                                 9327 non-null   int64         \n",
      " 44  sleep_restless                            9327 non-null   int64         \n",
      " 45  sleep_rmssd                               9327 non-null   int64         \n",
      " 46  sleep_rmssd_5min                          9327 non-null   object        \n",
      " 47  sleep_score                               9327 non-null   int64         \n",
      " 48  sleep_score_alignment                     9327 non-null   int64         \n",
      " 49  sleep_score_deep                          9327 non-null   int64         \n",
      " 50  sleep_score_disturbances                  9327 non-null   int64         \n",
      " 51  sleep_score_efficiency                    9327 non-null   int64         \n",
      " 52  sleep_score_latency                       9327 non-null   int64         \n",
      " 53  sleep_score_rem                           9327 non-null   int64         \n",
      " 54  sleep_score_total                         9327 non-null   int64         \n",
      " 55  sleep_temperature_delta                   9327 non-null   float64       \n",
      " 56  sleep_temperature_deviation               9327 non-null   float64       \n",
      " 57  sleep_temperature_trend_deviation         9327 non-null   float64       \n",
      " 58  timezone                                  0 non-null      float64       \n",
      " 59  sleep_total                               9327 non-null   object        \n",
      " 60  CONVERT(activity_class_5min USING utf8)   9327 non-null   object        \n",
      " 61  CONVERT(activity_met_1min USING utf8)     9327 non-null   object        \n",
      " 62  CONVERT(sleep_hr_5min USING utf8)         9327 non-null   object        \n",
      " 63  CONVERT(sleep_hypnogram_5min USING utf8)  9327 non-null   object        \n",
      " 64  CONVERT(sleep_rmssd_5min USING utf8)      9327 non-null   object        \n",
      " 65  DIAG_NM                                   9327 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(7), int64(45), object(13)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4808801",
   "metadata": {},
   "source": [
    "# 3. Preprocessing \n",
    "\n",
    "**9,327 rows × 66 columns**\n",
    "\n",
    "**9,327 rows × 5 columns**\n",
    "\n",
    "**===================Default 하게 진행======================**\n",
    "\n",
    "RNN, LSTM 모델에서는 converting features 사용할 예정  \n",
    "각각의 converting data가 가지는 길이가 다르기 때문에 sig1, sig2, sig3... sig5 data로 나누어 사용할 것  \n",
    "\n",
    "**feature processing**\n",
    "- astype : float \n",
    "- sig1_data, sig2_data, sig3_data, sig4_data, sig5_data \n",
    "    - CONVERT(activity_class_5min USING utf8)    \n",
    "    - CONVERT(activity_met_1min USING utf8)      \n",
    "    - CONVERT(sleep_hr_5min USING utf8)          \n",
    "    - CONVERT(sleep_hypnogram_5min USING utf8)  \n",
    "    - CONVERT(sleep_rmssd_5min USING utf8) \n",
    "\n",
    "**=========================Y Varible=========================**\n",
    "\n",
    "- DIAG_NM \n",
    "    - 정상(CN), 경도인지 장애(MCI), 치매(Dem)\n",
    "    - Lable Encoding \n",
    "    - 모델마다 y 변수를 자동으로 label encoding 기능을 지원하기도 하지만 공통적으로 진행하고 넘어갑시다\n",
    "    \n",
    "So we may use 5 columns for training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8110629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data를 따로 관리합니다 \n",
    "# original data -> data \n",
    "# preprocessing data -> processed_data\n",
    "processed_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adf93fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection - column drop \n",
    "# 사용할 column만 잘라냅니다\n",
    "processed_data = processed_data.iloc[:, 60:66]\n",
    "# 9327 x 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a9bbe7",
   "metadata": {},
   "source": [
    "processed_data -> 5개의 sig1_data, sig2_data, sig3_data, sig4_data, sig5_data 으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b4b35b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  278  279  280  \\\n",
       "0     2.0  1.0  1.0  1.0  1.0  1.0  2.0  2.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
       "1     1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
       "2     1.0  1.0  1.0  1.0  1.0  1.0  1.0  2.0  1.0  1.0  ...  1.0  2.0  2.0   \n",
       "3     2.0  1.0  2.0  2.0  1.0  2.0  1.0  1.0  2.0  1.0  ...  1.0  1.0  1.0   \n",
       "4     1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9322  2.0  1.0  1.0  2.0  2.0  1.0  1.0  1.0  1.0  2.0  ...  1.0  1.0  1.0   \n",
       "9323  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  2.0  1.0  1.0   \n",
       "9324  1.0  1.0  1.0  1.0  1.0  1.0  2.0  2.0  2.0  2.0  ...  1.0  1.0  1.0   \n",
       "9325  1.0  1.0  1.0  2.0  3.0  2.0  1.0  1.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
       "9326  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
       "\n",
       "      281  282  283  284  285  286  287  \n",
       "0     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "1     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "2     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "3     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "4     2.0  2.0  2.0  2.0  1.0  1.0  1.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "9322  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "9323  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "9324  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "9325  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "9326  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "\n",
       "[9327 rows x 288 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signal 01\n",
    "signal_list = list(processed_data['CONVERT(activity_class_5min USING utf8)'].str.split('/'))\n",
    "sig1_data = pd.DataFrame(signal_list).drop(288, axis=1).astype('float')\n",
    "\n",
    "sig1_data # 9327 x 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e193737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1430</th>\n",
       "      <th>1431</th>\n",
       "      <th>1432</th>\n",
       "      <th>1433</th>\n",
       "      <th>1434</th>\n",
       "      <th>1435</th>\n",
       "      <th>1436</th>\n",
       "      <th>1437</th>\n",
       "      <th>1438</th>\n",
       "      <th>1439</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 1440 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...  1430  \\\n",
       "0      0.9   0.9   1.4   1.9   1.1   0.9   0.9   1.1   1.3   1.0  ...   0.9   \n",
       "1      1.2   1.1   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  ...   1.0   \n",
       "2      1.1   1.1   1.2   1.1   0.9   0.9   0.9   0.9   0.9   0.9  ...   0.9   \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4      0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  ...   0.9   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "9322   1.1   1.4   1.2   0.9   1.2   1.1   0.9   0.9   0.9   1.0  ...   0.9   \n",
       "9323   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  ...   0.9   \n",
       "9324   0.9   0.9   0.9   0.9   1.1   0.9   0.9   1.1   0.9   0.9  ...   0.9   \n",
       "9325   1.1   0.9   0.9   0.9   1.0   0.9   0.9   0.9   0.9   0.9  ...   0.9   \n",
       "9326   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  ...   0.9   \n",
       "\n",
       "      1431  1432  1433  1434  1435  1436  1437  1438  1439  \n",
       "0      0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "1      0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "2      0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4      0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "9322   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "9323   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "9324   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "9325   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "9326   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9  \n",
       "\n",
       "[9327 rows x 1440 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signal 02\n",
    "signal_list = list(processed_data['CONVERT(activity_met_1min USING utf8)'].str.split('/'))\n",
    "sig2_data = pd.DataFrame(signal_list).drop(1440, axis=1).astype('float')\n",
    "\n",
    "sig2_data # 9327 x 1440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29928e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>68.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9    ...  171  \\\n",
       "0      0.0  73.0  73.0  73.0  72.0  71.0  70.0  71.0  71.0  71.0  ...  0.0   \n",
       "1     69.0  70.0  69.0  69.0  70.0  72.0  71.0  72.0  70.0  69.0  ...  0.0   \n",
       "2      0.0  74.0  73.0  73.0  74.0  74.0  74.0  71.0  71.0  70.0  ...  0.0   \n",
       "3     73.0  70.0  71.0  72.0  75.0  75.0  73.0  70.0  70.0  70.0  ...  0.0   \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  69.0  69.0  ...  0.0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...  ...   \n",
       "9322   0.0  54.0  54.0  54.0  55.0  56.0   0.0   0.0  55.0  52.0  ...  0.0   \n",
       "9323  68.0  66.0  67.0  67.0  68.0  69.0  69.0  70.0  71.0  71.0  ...  0.0   \n",
       "9324   0.0   0.0  65.0  65.0  66.0  67.0  69.0  72.0  73.0  72.0  ...  0.0   \n",
       "9325   0.0   0.0   0.0  64.0  64.0  64.0  64.0  65.0  65.0  66.0  ...  0.0   \n",
       "9326   0.0  60.0  60.0  61.0  60.0  60.0  60.0  62.0  64.0  64.0  ...  0.0   \n",
       "\n",
       "      172  173  174  175  176  177  178  179  180  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "9322  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9323  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9324  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9325  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9326  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[9327 rows x 181 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signal 03\n",
    "signal_list = list(processed_data['CONVERT(sleep_hr_5min USING utf8)'].str.split('/'))\n",
    "sig3_data = pd.DataFrame(signal_list).drop(181, axis=1).transpose() # 결측치 처리를 위한 transpose \n",
    "\n",
    "# 결측치 처리 Nan, '', ' ' -> 0으로 replace \n",
    "sig3_data = sig3_data.fillna(0) \n",
    "sig3_data = sig3_data.replace('', 0)\n",
    "sig3_data = sig3_data.replace(' ', 0)\n",
    "sig3_data = sig3_data.astype('float').transpose() # 되돌리기 \n",
    "\n",
    "sig3_data # 9327 x 181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffa76205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  170  171  172  \\\n",
       "0     4.0  2.0  4.0  3.0  3.0  1.0  2.0  2.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "1     2.0  4.0  2.0  2.0  2.0  2.0  3.0  1.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "2     4.0  2.0  4.0  4.0  1.0  1.0  1.0  4.0  4.0  4.0  ...  0.0  0.0  0.0   \n",
       "3     4.0  4.0  4.0  4.0  3.0  3.0  3.0  2.0  4.0  4.0  ...  0.0  0.0  0.0   \n",
       "4     4.0  4.0  4.0  4.0  4.0  4.0  4.0  4.0  4.0  4.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9322  4.0  4.0  4.0  2.0  2.0  2.0  2.0  4.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "9323  4.0  4.0  2.0  2.0  2.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0   \n",
       "9324  4.0  4.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "9325  4.0  4.0  4.0  4.0  2.0  2.0  2.0  2.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "9326  4.0  4.0  1.0  2.0  2.0  2.0  2.0  1.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      173  174  175  176  177  178  179  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "9322  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9323  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9324  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9325  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9326  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[9327 rows x 180 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signal 04\n",
    "signal_list = list(processed_data['CONVERT(sleep_hypnogram_5min USING utf8)'].str.split('/'))\n",
    "sig4_data = pd.DataFrame(signal_list).drop(180, axis=1).transpose() # 결측치 처리를 위한 transpose \n",
    "\n",
    "# 결측치 처리 Nan, '', ' ' -> 0으로 replace \n",
    "sig4_data = sig4_data.fillna(0) \n",
    "sig4_data = sig4_data.replace('', 0)\n",
    "sig4_data = sig4_data.replace(' ', 0)\n",
    "sig4_data = sig4_data.astype('float').transpose() # 되돌리기 \n",
    "\n",
    "sig4_data # 9327 x 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "470df2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>26.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9    ...  171  \\\n",
       "0      0.0  10.0  10.0  10.0  11.0  11.0  10.0  12.0  18.0  13.0  ...  0.0   \n",
       "1     23.0  23.0  26.0  24.0  18.0  13.0  15.0  14.0  17.0  20.0  ...  0.0   \n",
       "2      0.0  11.0  14.0  20.0  13.0  14.0  14.0  16.0  27.0  29.0  ...  0.0   \n",
       "3     24.0  28.0  19.0  17.0  12.0  10.0  17.0  20.0  23.0  23.0  ...  0.0   \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  21.0  22.0  ...  0.0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...  ...   \n",
       "9322   0.0  35.0  39.0  28.0  26.0  41.0   0.0   0.0  52.0  31.0  ...  0.0   \n",
       "9323  26.0  16.0  19.0  18.0  16.0  18.0  19.0  17.0  15.0  16.0  ...  0.0   \n",
       "9324   0.0   0.0  13.0  12.0  12.0  13.0  12.0  12.0  12.0  16.0  ...  0.0   \n",
       "9325   0.0   0.0   0.0  14.0  17.0  18.0  14.0  14.0  17.0  15.0  ...  0.0   \n",
       "9326   0.0  14.0  15.0  14.0  15.0  16.0  17.0  16.0  16.0  18.0  ...  0.0   \n",
       "\n",
       "      172  173  174  175  176  177  178  179  180  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "9322  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9323  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9324  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9325  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9326  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[9327 rows x 181 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signal 05\n",
    "signal_list = list(processed_data['CONVERT(sleep_rmssd_5min USING utf8)'].str.split('/'))\n",
    "sig5_data = pd.DataFrame(signal_list).drop(181, axis=1).transpose() # 결측치 처리를 위한 transpose \n",
    "\n",
    "# 결측치 처리 Nan, '', ' ' -> 0으로 replace \n",
    "sig5_data = sig5_data.fillna(0) \n",
    "sig5_data = sig5_data.replace('', 0)\n",
    "sig5_data = sig5_data.replace(' ', 0)\n",
    "sig5_data = sig5_data.astype('float').transpose() # 되돌리기 \n",
    "\n",
    "sig5_data # 9327 x 181"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520471a",
   "metadata": {},
   "source": [
    "# 4. Data Split + y Variable Processing \n",
    "\n",
    "모든 전처리와 y variable labeling이 완료되었다.  \n",
    "train / valid / test data로 분할하고 용도에 맞게 model을 돌리도록 하자.\n",
    "\n",
    "- **3d - array 처리** \n",
    "    - Sequential model 의 input layer에는 ndarray 데이터만 들어가야 함\n",
    "- **y 변수 두 가지로 encoding** \n",
    "    - Label Encoding (le) -> sparse_categorical_crossentropy 실험 \n",
    "    - One Hot Encoding (ohe) -> categorical_crossentropy 실험 \n",
    "- **train / test** \n",
    "    - Sequential model option 중 validation dataset을 자동으로 나눠주는 것이 존재 하므로 \n",
    "\n",
    "\n",
    "- 즉 다음과 같은 조합이 하나의 dataset 이며  \n",
    "    - input_data_X1 + input_data_y_le + input_data_y_ohe\n",
    "    - 실험은 한 모델당 두 가지 y 변수로 2번 실험하면 됨\n",
    "    - **inpu_data_X1_le + input_data_y_le**\n",
    "    - **inpu_data_X1_ohe + input_data_y_ohe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "831f2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# data를 따로 관리\n",
    "# preprocessing data -> processed_data\n",
    "# input data -> input_data => y 변수와 X 변수 분할 관리 \n",
    "input_data_X1 = sig1_data.copy()\n",
    "input_data_X2 = sig2_data.copy()\n",
    "input_data_X3 = sig3_data.copy()\n",
    "input_data_X4 = sig4_data.copy()\n",
    "input_data_X5 = sig5_data.copy()\n",
    "\n",
    "input_data_y = processed_data['DIAG_NM'] .copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c15d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d-array 처리 - X variables \n",
    "input_data_X1 =  input_data_X1.to_numpy().reshape(input_data_X1.shape[0], input_data_X1.shape[1], 1)\n",
    "input_data_X2 =  input_data_X2.to_numpy().reshape(input_data_X2.shape[0], input_data_X2.shape[1], 1)\n",
    "input_data_X3 =  input_data_X3.to_numpy().reshape(input_data_X3.shape[0], input_data_X3.shape[1], 1)\n",
    "input_data_X4 =  input_data_X4.to_numpy().reshape(input_data_X4.shape[0], input_data_X4.shape[1], 1)\n",
    "input_data_X5 =  input_data_X5.to_numpy().reshape(input_data_X5.shape[0], input_data_X5.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f97b7439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original gain_label \n",
      " ['CN' 'Dem' 'MCI']\n",
      "gain_lable label \n",
      " {0, 1, 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LabelEncoder - y variable \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "input_data_y_label = le.fit_transform(list(input_data_y))\n",
    "\n",
    "print(\"original gain_label \\n\", le.classes_)\n",
    "print(\"gain_lable label \\n\", set(input_data_y_label))\n",
    "\n",
    "input_data_y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7662a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[array(['CN', 'Dem', 'MCI'], dtype=object)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CN</th>\n",
       "      <th>Dem</th>\n",
       "      <th>MCI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9327 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CN  Dem  MCI\n",
       "0     1.0  0.0  0.0\n",
       "1     1.0  0.0  0.0\n",
       "2     1.0  0.0  0.0\n",
       "3     1.0  0.0  0.0\n",
       "4     1.0  0.0  0.0\n",
       "...   ...  ...  ...\n",
       "9322  1.0  0.0  0.0\n",
       "9323  1.0  0.0  0.0\n",
       "9324  1.0  0.0  0.0\n",
       "9325  1.0  0.0  0.0\n",
       "9326  1.0  0.0  0.0\n",
       "\n",
       "[9327 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Hot Encdoer - y variable \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "input_data_y.shape\n",
    "input_data_y.values.reshape(-1,1).shape\n",
    "ohe.fit(input_data_y.values.reshape(-1,1))\n",
    "one_hot_encoded = ohe.transform(input_data_y.values.reshape(-1,1))\n",
    "\n",
    "print(type(ohe.categories_))\n",
    "print(ohe.categories_)\n",
    "\n",
    "input_data_y_ohe = pd.DataFrame(one_hot_encoded, columns=ohe.categories_[0])\n",
    "input_data_y_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ed348655",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train_label, X1_test_label, y1_train_label, y1_test_label = train_test_split(\n",
    "    input_data_X1, input_data_y_label, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X2_train_label, X2_test_label, y2_train_label, y2_test_label = train_test_split(\n",
    "    input_data_X2, input_data_y_label, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X3_train_label, X3_test_label, y3_train_label, y3_test_label = train_test_split(\n",
    "    input_data_X3, input_data_y_label, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X4_train_label, X4_test_label, y4_train_label, y4_test_label = train_test_split(\n",
    "    input_data_X4, input_data_y_label, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X5_train_label, X5_test_label, y5_train_label, y5_test_label = train_test_split(\n",
    "    input_data_X5, input_data_y_label, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac7943f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train_ohe, X1_test_ohe, y1_train_ohe, y1_test_ohe = train_test_split(\n",
    "    input_data_X1, input_data_y_ohe, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X2_train_ohe, X2_test_ohe, y2_train_ohe, y2_test_ohe = train_test_split(\n",
    "    input_data_X2, input_data_y_ohe, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X3_train_ohe, X3_test_ohe, y3_train_ohe, y3_test_ohe = train_test_split(\n",
    "    input_data_X3, input_data_y_ohe, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X4_train_ohe, X4_test_ohe, y4_train_ohe, y4_test_ohe = train_test_split(\n",
    "    input_data_X4, input_data_y_ohe, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)\n",
    "X5_train_ohe, X5_test_ohe, y5_train_ohe, y5_test_ohe = train_test_split(\n",
    "    input_data_X5, input_data_y_ohe, test_size=0.2, shuffle=True, stratify=input_data_y, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be2558ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1_train_label : (7461, 288, 1)\n",
      "X1_train_ohe : (7461, 288, 1)\n",
      "y1_train_label : (7461,)\n",
      "y1_train_ohe : (7461, 3)\n",
      "X1_test_label : (1866, 288, 1)\n",
      "X1_test_ohe : (1866, 288, 1)\n",
      "y1_test_label : (1866,)\n",
      "y1_test_ohe : (1866, 3)\n"
     ]
    }
   ],
   "source": [
    "# 데이터가 잘 분할되었을까요?\n",
    "# 잘 분할되었습니다 \n",
    "# 7461 + 1866 = 9327\n",
    "print(\"X1_train_label :\" ,X1_train_label.shape)\n",
    "print(\"X1_train_ohe :\" ,X1_train_ohe.shape)\n",
    "print(\"y1_train_label :\" ,y1_train_label.shape)\n",
    "print(\"y1_train_ohe :\" ,y1_train_ohe.shape)\n",
    "print(\"X1_test_label :\" ,X1_test_label.shape)\n",
    "print(\"X1_test_ohe :\" ,X1_test_ohe.shape)\n",
    "print(\"y1_test_label :\" ,y1_test_label.shape)\n",
    "print(\"y1_test_ohe :\" ,y1_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e012e",
   "metadata": {},
   "source": [
    "# 5. Modeling - DL \n",
    "\n",
    "- **simple RNN** \n",
    "    - ohe **0.638**\n",
    "    - le **0.639**\n",
    "- **multi-layer RNN**\n",
    "    - ohe 0.637\n",
    "    - le 0.637\n",
    "- **simple LSTM**\n",
    "    - ohe \n",
    "    - le \n",
    "- **multi-layer LSTM**\n",
    "    - ohe \n",
    "    - le "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ab57c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Activation, SimpleRNN\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "128be75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "units = 50\n",
    "timesteps = 1\n",
    "inputs = (X3_train_ohe.shape[1],X3_train_ohe.shape[2])\n",
    "outputs = 3\n",
    "epochs = 1000\n",
    "batch_size = 256\n",
    "# early_stop 하기 위해 관찰 값 monitor\n",
    "# 관찰 값이 loss 이므로 최소화 시키는 방향으로 training이 진행 (default 깂 Auto)\n",
    "# patience : 성능이 증가하지 않더라도 epoch을 얼마나 더 진행해볼 것인가?\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode = 'min'\n",
    "                               , patience=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517759bd",
   "metadata": {},
   "source": [
    "### simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "937825c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units, input_shape = inputs, return_sequences = False)) \n",
    "    model.add(Dense(outputs)) #target 개수 = 3\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    #y를 one-hot encoding 시 'categorical_crossentropy 사용\n",
    "    #y를 label encoding 시 'sparse_categorical_crossentropy' 사용\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = adam, metrics = ['accuracy']) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a58086c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "24/24 [==============================] - 2s 41ms/step - loss: 0.9879 - accuracy: 0.4967 - val_loss: 0.8335 - val_accuracy: 0.6450\n",
      "Epoch 2/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8285 - accuracy: 0.6364 - val_loss: 0.8298 - val_accuracy: 0.6450\n",
      "Epoch 3/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8257 - accuracy: 0.6436 - val_loss: 0.8300 - val_accuracy: 0.6457\n",
      "Epoch 4/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8293 - accuracy: 0.6347 - val_loss: 0.8278 - val_accuracy: 0.6450\n",
      "Epoch 5/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8304 - accuracy: 0.6356 - val_loss: 0.8256 - val_accuracy: 0.6450\n",
      "Epoch 6/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8250 - accuracy: 0.6377 - val_loss: 0.8220 - val_accuracy: 0.6457\n",
      "Epoch 7/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 1.0839 - accuracy: 0.5180 - val_loss: 0.8241 - val_accuracy: 0.6450\n",
      "Epoch 8/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8195 - accuracy: 0.6427 - val_loss: 0.8246 - val_accuracy: 0.6457\n",
      "Epoch 9/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8244 - accuracy: 0.6454 - val_loss: 0.8265 - val_accuracy: 0.6457\n",
      "Epoch 10/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8304 - accuracy: 0.6348 - val_loss: 0.8243 - val_accuracy: 0.6457\n",
      "Epoch 11/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8279 - accuracy: 0.6343 - val_loss: 0.8227 - val_accuracy: 0.6470\n",
      "Epoch 12/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8181 - accuracy: 0.6391 - val_loss: 0.8233 - val_accuracy: 0.6463\n",
      "Epoch 13/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8350 - accuracy: 0.6259 - val_loss: 0.8259 - val_accuracy: 0.6457\n",
      "Epoch 14/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8282 - accuracy: 0.6320 - val_loss: 0.8142 - val_accuracy: 0.6463\n",
      "Epoch 15/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8213 - accuracy: 0.6390 - val_loss: 0.8285 - val_accuracy: 0.6457\n",
      "Epoch 16/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8361 - accuracy: 0.6285 - val_loss: 0.8240 - val_accuracy: 0.6457\n",
      "Epoch 17/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8348 - accuracy: 0.6282 - val_loss: 0.8148 - val_accuracy: 0.6457\n",
      "Epoch 18/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8200 - accuracy: 0.6443 - val_loss: 0.8172 - val_accuracy: 0.6463\n",
      "Epoch 19/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8159 - accuracy: 0.6363 - val_loss: 0.8128 - val_accuracy: 0.6457\n",
      "Epoch 20/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8070 - accuracy: 0.6471 - val_loss: 0.8175 - val_accuracy: 0.6457\n",
      "Epoch 21/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8132 - accuracy: 0.6386 - val_loss: 0.8222 - val_accuracy: 0.6457\n",
      "Epoch 22/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8218 - accuracy: 0.6430 - val_loss: 0.8148 - val_accuracy: 0.6463\n",
      "Epoch 23/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8046 - accuracy: 0.6483 - val_loss: 0.8310 - val_accuracy: 0.6457\n",
      "Epoch 24/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8252 - accuracy: 0.6335 - val_loss: 0.8225 - val_accuracy: 0.6457\n",
      "Epoch 25/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8148 - accuracy: 0.6409 - val_loss: 0.8214 - val_accuracy: 0.6457\n",
      "Epoch 26/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8202 - accuracy: 0.6388 - val_loss: 0.8071 - val_accuracy: 0.6463\n",
      "Epoch 27/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8245 - accuracy: 0.6301 - val_loss: 0.8118 - val_accuracy: 0.6463\n",
      "Epoch 28/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8225 - accuracy: 0.6315 - val_loss: 0.8118 - val_accuracy: 0.6463\n",
      "Epoch 29/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8219 - accuracy: 0.6367 - val_loss: 0.8107 - val_accuracy: 0.6463\n",
      "Epoch 30/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8096 - accuracy: 0.6413 - val_loss: 0.8020 - val_accuracy: 0.6457\n",
      "Epoch 31/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8134 - accuracy: 0.6431 - val_loss: 0.8253 - val_accuracy: 0.6463\n",
      "Epoch 32/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8267 - accuracy: 0.6426 - val_loss: 0.8242 - val_accuracy: 0.6457\n",
      "Epoch 33/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8290 - accuracy: 0.6319 - val_loss: 0.8253 - val_accuracy: 0.6457\n",
      "Epoch 34/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8258 - accuracy: 0.6420 - val_loss: 0.8272 - val_accuracy: 0.6457\n",
      "Epoch 35/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8306 - accuracy: 0.6282 - val_loss: 0.8247 - val_accuracy: 0.6457\n",
      "Epoch 36/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8212 - accuracy: 0.6365 - val_loss: 0.8244 - val_accuracy: 0.6457\n",
      "Epoch 37/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8249 - accuracy: 0.6350 - val_loss: 0.8236 - val_accuracy: 0.6463\n",
      "Epoch 38/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8141 - accuracy: 0.6363 - val_loss: 0.8224 - val_accuracy: 0.6463\n",
      "Epoch 39/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8188 - accuracy: 0.6407 - val_loss: 0.8190 - val_accuracy: 0.6463\n",
      "Epoch 40/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8209 - accuracy: 0.6397 - val_loss: 0.8156 - val_accuracy: 0.6477\n",
      "Epoch 41/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8154 - accuracy: 0.6375 - val_loss: 0.8067 - val_accuracy: 0.6470\n",
      "Epoch 42/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8211 - accuracy: 0.6303 - val_loss: 0.8216 - val_accuracy: 0.6463\n",
      "Epoch 43/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8092 - accuracy: 0.6464 - val_loss: 0.8170 - val_accuracy: 0.6463\n",
      "Epoch 44/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8127 - accuracy: 0.6436 - val_loss: 0.8189 - val_accuracy: 0.6463\n",
      "Epoch 45/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8056 - accuracy: 0.6499 - val_loss: 0.8037 - val_accuracy: 0.6463\n",
      "Epoch 46/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8081 - accuracy: 0.6484 - val_loss: 0.8085 - val_accuracy: 0.6463\n",
      "Epoch 47/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8107 - accuracy: 0.6400 - val_loss: 0.8031 - val_accuracy: 0.6477\n",
      "Epoch 48/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8085 - accuracy: 0.6425 - val_loss: 0.7990 - val_accuracy: 0.6477\n",
      "Epoch 49/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8034 - accuracy: 0.6366 - val_loss: 0.7975 - val_accuracy: 0.6463\n",
      "Epoch 50/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8167 - accuracy: 0.6347 - val_loss: 0.7977 - val_accuracy: 0.6463\n",
      "Epoch 51/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8190 - accuracy: 0.6333 - val_loss: 0.7984 - val_accuracy: 0.6470\n",
      "Epoch 52/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8008 - accuracy: 0.6388 - val_loss: 0.8005 - val_accuracy: 0.6484\n",
      "Epoch 53/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8121 - accuracy: 0.6350 - val_loss: 0.8007 - val_accuracy: 0.6477\n",
      "Epoch 54/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8052 - accuracy: 0.6390 - val_loss: 0.8053 - val_accuracy: 0.6477\n",
      "Epoch 55/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8063 - accuracy: 0.6382 - val_loss: 0.8000 - val_accuracy: 0.6463\n",
      "Epoch 56/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8473 - accuracy: 0.6350 - val_loss: 0.8340 - val_accuracy: 0.6457\n",
      "Epoch 57/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8379 - accuracy: 0.6343 - val_loss: 0.8281 - val_accuracy: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8210 - accuracy: 0.6421 - val_loss: 0.8268 - val_accuracy: 0.6457\n",
      "Epoch 59/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8096 - accuracy: 0.6545 - val_loss: 0.8265 - val_accuracy: 0.6457\n",
      "Epoch 60/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8225 - accuracy: 0.6366 - val_loss: 0.8264 - val_accuracy: 0.6457\n",
      "Epoch 61/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8125 - accuracy: 0.6459 - val_loss: 0.8269 - val_accuracy: 0.6457\n",
      "Epoch 62/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8226 - accuracy: 0.6392 - val_loss: 0.8259 - val_accuracy: 0.6457\n",
      "Epoch 63/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8185 - accuracy: 0.6461 - val_loss: 0.8299 - val_accuracy: 0.6457\n",
      "Epoch 64/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8284 - accuracy: 0.6355 - val_loss: 0.8258 - val_accuracy: 0.6457\n",
      "Epoch 65/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8183 - accuracy: 0.6481 - val_loss: 0.8277 - val_accuracy: 0.6457\n",
      "Epoch 66/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8257 - accuracy: 0.6344 - val_loss: 0.8233 - val_accuracy: 0.6457\n",
      "Epoch 67/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8213 - accuracy: 0.6439 - val_loss: 0.8198 - val_accuracy: 0.6463\n",
      "Epoch 68/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8238 - accuracy: 0.6444 - val_loss: 0.8261 - val_accuracy: 0.6463\n",
      "Epoch 69/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8275 - accuracy: 0.6358 - val_loss: 0.8119 - val_accuracy: 0.6463\n",
      "Epoch 70/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8196 - accuracy: 0.6326 - val_loss: 0.8169 - val_accuracy: 0.6457\n",
      "Epoch 71/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8220 - accuracy: 0.6326 - val_loss: 0.8093 - val_accuracy: 0.6463\n",
      "Epoch 72/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8081 - accuracy: 0.6427 - val_loss: 0.8202 - val_accuracy: 0.6463\n",
      "Epoch 73/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8206 - accuracy: 0.6392 - val_loss: 0.8059 - val_accuracy: 0.6463\n",
      "Epoch 74/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8073 - accuracy: 0.6480 - val_loss: 0.8626 - val_accuracy: 0.6463\n",
      "Epoch 75/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8325 - accuracy: 0.6379 - val_loss: 0.8138 - val_accuracy: 0.6463\n",
      "Epoch 76/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8127 - accuracy: 0.6388 - val_loss: 0.8190 - val_accuracy: 0.6463\n",
      "Epoch 77/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8077 - accuracy: 0.6455 - val_loss: 0.8002 - val_accuracy: 0.6457\n",
      "Epoch 78/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8165 - accuracy: 0.6333 - val_loss: 0.8020 - val_accuracy: 0.6463\n",
      "Epoch 79/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8050 - accuracy: 0.6498 - val_loss: 0.7983 - val_accuracy: 0.6443\n",
      "Epoch 80/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8064 - accuracy: 0.6378 - val_loss: 0.8031 - val_accuracy: 0.6457\n",
      "Epoch 81/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8193 - accuracy: 0.6340 - val_loss: 0.7995 - val_accuracy: 0.6457\n",
      "Epoch 82/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7998 - accuracy: 0.6498 - val_loss: 0.7995 - val_accuracy: 0.6450\n",
      "Epoch 83/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8166 - accuracy: 0.6372 - val_loss: 0.8006 - val_accuracy: 0.6457\n",
      "Epoch 84/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8065 - accuracy: 0.6358 - val_loss: 0.7993 - val_accuracy: 0.6457\n",
      "Epoch 85/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8019 - accuracy: 0.6445 - val_loss: 0.7998 - val_accuracy: 0.6457\n",
      "Epoch 86/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8170 - accuracy: 0.6332 - val_loss: 0.8206 - val_accuracy: 0.6463\n",
      "Epoch 87/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8357 - accuracy: 0.6313 - val_loss: 0.8065 - val_accuracy: 0.6457\n",
      "Epoch 88/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8043 - accuracy: 0.6388 - val_loss: 0.8270 - val_accuracy: 0.6457\n",
      "Epoch 89/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8321 - accuracy: 0.6296 - val_loss: 0.8222 - val_accuracy: 0.6457\n",
      "Epoch 90/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8212 - accuracy: 0.6409 - val_loss: 0.8050 - val_accuracy: 0.6463\n",
      "Epoch 91/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8173 - accuracy: 0.6382 - val_loss: 0.8073 - val_accuracy: 0.6450\n",
      "Epoch 92/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8117 - accuracy: 0.6378 - val_loss: 0.8000 - val_accuracy: 0.6477\n",
      "Epoch 93/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8074 - accuracy: 0.6428 - val_loss: 0.8023 - val_accuracy: 0.6470\n",
      "Epoch 94/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8104 - accuracy: 0.6371 - val_loss: 0.8008 - val_accuracy: 0.6477\n",
      "Epoch 95/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8105 - accuracy: 0.6372 - val_loss: 0.8003 - val_accuracy: 0.6477\n",
      "Epoch 96/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8117 - accuracy: 0.6446 - val_loss: 0.8029 - val_accuracy: 0.6463\n",
      "Epoch 97/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8001 - accuracy: 0.6431 - val_loss: 0.7967 - val_accuracy: 0.6463\n",
      "Epoch 98/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8080 - accuracy: 0.6347 - val_loss: 0.7960 - val_accuracy: 0.6470\n",
      "Epoch 99/1000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 0.8076 - accuracy: 0.6396 - val_loss: 0.7985 - val_accuracy: 0.6477\n",
      "Epoch 100/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8028 - accuracy: 0.6406 - val_loss: 0.7998 - val_accuracy: 0.6463\n",
      "Epoch 101/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8145 - accuracy: 0.6352 - val_loss: 0.8003 - val_accuracy: 0.6457\n",
      "Epoch 102/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8111 - accuracy: 0.6390 - val_loss: 0.8041 - val_accuracy: 0.6463\n",
      "Epoch 103/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8023 - accuracy: 0.6428 - val_loss: 0.7983 - val_accuracy: 0.6457\n",
      "Epoch 104/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7997 - accuracy: 0.6502 - val_loss: 0.8008 - val_accuracy: 0.6457\n",
      "Epoch 105/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8113 - accuracy: 0.6370 - val_loss: 0.7984 - val_accuracy: 0.6470\n",
      "Epoch 106/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8067 - accuracy: 0.6432 - val_loss: 0.7973 - val_accuracy: 0.6477\n",
      "Epoch 107/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7924 - accuracy: 0.6493 - val_loss: 0.7987 - val_accuracy: 0.6450\n",
      "Epoch 108/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7994 - accuracy: 0.6367 - val_loss: 0.8049 - val_accuracy: 0.6463\n",
      "Epoch 109/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8113 - accuracy: 0.6327 - val_loss: 0.7968 - val_accuracy: 0.6450\n",
      "Epoch 110/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8038 - accuracy: 0.6420 - val_loss: 0.8025 - val_accuracy: 0.6457\n",
      "Epoch 111/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8180 - accuracy: 0.6272 - val_loss: 0.8003 - val_accuracy: 0.6470\n",
      "Epoch 112/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8057 - accuracy: 0.6401 - val_loss: 0.7968 - val_accuracy: 0.6450\n",
      "Epoch 113/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8123 - accuracy: 0.6300 - val_loss: 0.8002 - val_accuracy: 0.6463\n",
      "Epoch 114/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8126 - accuracy: 0.6428 - val_loss: 0.8032 - val_accuracy: 0.6470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8072 - accuracy: 0.6427 - val_loss: 0.7998 - val_accuracy: 0.6470\n",
      "Epoch 116/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8021 - accuracy: 0.6423 - val_loss: 0.8011 - val_accuracy: 0.6470\n",
      "Epoch 117/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8039 - accuracy: 0.6446 - val_loss: 0.8041 - val_accuracy: 0.6457\n",
      "Epoch 118/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.7992 - accuracy: 0.6418 - val_loss: 0.8047 - val_accuracy: 0.6450\n",
      "Epoch 119/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8115 - accuracy: 0.6381 - val_loss: 0.8020 - val_accuracy: 0.6477\n",
      "Epoch 120/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8094 - accuracy: 0.6421 - val_loss: 0.7989 - val_accuracy: 0.6450\n",
      "Epoch 121/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8060 - accuracy: 0.6455 - val_loss: 0.8022 - val_accuracy: 0.6457\n",
      "Epoch 122/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8051 - accuracy: 0.6403 - val_loss: 0.7994 - val_accuracy: 0.6470\n",
      "Epoch 123/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.7969 - accuracy: 0.6528 - val_loss: 0.7973 - val_accuracy: 0.6457\n",
      "Epoch 124/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8111 - accuracy: 0.6371 - val_loss: 0.7983 - val_accuracy: 0.6457\n",
      "Epoch 125/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8037 - accuracy: 0.6377 - val_loss: 0.7984 - val_accuracy: 0.6463\n",
      "Epoch 126/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8051 - accuracy: 0.6471 - val_loss: 0.8016 - val_accuracy: 0.6463\n",
      "Epoch 127/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8102 - accuracy: 0.6370 - val_loss: 0.7999 - val_accuracy: 0.6463\n",
      "Epoch 128/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8040 - accuracy: 0.6414 - val_loss: 0.8012 - val_accuracy: 0.6470\n",
      "Epoch 129/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8079 - accuracy: 0.6415 - val_loss: 0.8010 - val_accuracy: 0.6463\n",
      "Epoch 130/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.7995 - accuracy: 0.6463 - val_loss: 0.8048 - val_accuracy: 0.6463\n",
      "Epoch 131/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8041 - accuracy: 0.6403 - val_loss: 0.7969 - val_accuracy: 0.6470\n",
      "Epoch 132/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8071 - accuracy: 0.6429 - val_loss: 0.8027 - val_accuracy: 0.6457\n",
      "Epoch 133/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7853 - accuracy: 0.6581 - val_loss: 0.8057 - val_accuracy: 0.6463\n",
      "Epoch 134/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8177 - accuracy: 0.6414 - val_loss: 0.8026 - val_accuracy: 0.6470\n",
      "Epoch 135/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8146 - accuracy: 0.6339 - val_loss: 0.8011 - val_accuracy: 0.6470\n",
      "Epoch 136/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8021 - accuracy: 0.6473 - val_loss: 0.7998 - val_accuracy: 0.6463\n",
      "Epoch 137/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8191 - accuracy: 0.6415 - val_loss: 0.8013 - val_accuracy: 0.6470\n",
      "Epoch 138/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8011 - accuracy: 0.6456 - val_loss: 0.8012 - val_accuracy: 0.6463\n",
      "Epoch 139/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8134 - accuracy: 0.6395 - val_loss: 0.7988 - val_accuracy: 0.6463\n",
      "Epoch 140/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8088 - accuracy: 0.6409 - val_loss: 0.8060 - val_accuracy: 0.6463\n",
      "Epoch 141/1000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8063 - accuracy: 0.6393 - val_loss: 0.7983 - val_accuracy: 0.6457\n",
      "Epoch 142/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.7861 - accuracy: 0.6570 - val_loss: 0.8081 - val_accuracy: 0.6470\n",
      "Epoch 143/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8012 - accuracy: 0.6470 - val_loss: 0.7991 - val_accuracy: 0.6450\n",
      "Epoch 144/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8074 - accuracy: 0.6365 - val_loss: 0.8045 - val_accuracy: 0.6463\n",
      "Epoch 145/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8092 - accuracy: 0.6309 - val_loss: 0.7998 - val_accuracy: 0.6463\n",
      "Epoch 146/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8083 - accuracy: 0.6405 - val_loss: 0.8014 - val_accuracy: 0.6470\n",
      "Epoch 147/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8065 - accuracy: 0.6414 - val_loss: 0.8036 - val_accuracy: 0.6463\n",
      "Epoch 148/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7940 - accuracy: 0.6450 - val_loss: 0.7985 - val_accuracy: 0.6470\n",
      "Epoch 149/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7990 - accuracy: 0.6437 - val_loss: 0.8013 - val_accuracy: 0.6457\n",
      "Epoch 150/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8096 - accuracy: 0.6341 - val_loss: 0.8014 - val_accuracy: 0.6463\n",
      "Epoch 151/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8080 - accuracy: 0.6441 - val_loss: 0.8034 - val_accuracy: 0.6457\n",
      "Epoch 152/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8205 - accuracy: 0.6315 - val_loss: 0.7989 - val_accuracy: 0.6470\n",
      "Epoch 153/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8125 - accuracy: 0.6335 - val_loss: 0.8042 - val_accuracy: 0.6457\n",
      "Epoch 154/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8044 - accuracy: 0.6442 - val_loss: 0.8043 - val_accuracy: 0.6463\n",
      "Epoch 155/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7907 - accuracy: 0.6456 - val_loss: 0.8013 - val_accuracy: 0.6457\n",
      "Epoch 156/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8055 - accuracy: 0.6408 - val_loss: 0.8000 - val_accuracy: 0.6463\n",
      "Epoch 157/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8120 - accuracy: 0.6260 - val_loss: 0.7998 - val_accuracy: 0.6463\n",
      "Epoch 158/1000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 0.8048 - accuracy: 0.6397 - val_loss: 0.8018 - val_accuracy: 0.6463\n",
      "Epoch 159/1000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 0.8027 - accuracy: 0.6476 - val_loss: 0.8021 - val_accuracy: 0.6437\n",
      "Epoch 160/1000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 0.8089 - accuracy: 0.6319 - val_loss: 0.8009 - val_accuracy: 0.6457\n",
      "Epoch 161/1000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8142 - accuracy: 0.6310 - val_loss: 0.8003 - val_accuracy: 0.6457\n",
      "Epoch 162/1000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8124 - accuracy: 0.6380 - val_loss: 0.8002 - val_accuracy: 0.6463\n",
      "Epoch 163/1000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.7995 - accuracy: 0.6426 - val_loss: 0.8008 - val_accuracy: 0.6463\n",
      "Epoch 164/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8047 - accuracy: 0.6369 - val_loss: 0.7955 - val_accuracy: 0.6443\n",
      "Epoch 165/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8118 - accuracy: 0.6320 - val_loss: 0.7994 - val_accuracy: 0.6457\n",
      "Epoch 166/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8014 - accuracy: 0.6435 - val_loss: 0.7997 - val_accuracy: 0.6457\n",
      "Epoch 167/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.7901 - accuracy: 0.6541 - val_loss: 0.8003 - val_accuracy: 0.6463\n",
      "Epoch 168/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8008 - accuracy: 0.6453 - val_loss: 0.7988 - val_accuracy: 0.6450\n",
      "Epoch 169/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7998 - accuracy: 0.6477 - val_loss: 0.7990 - val_accuracy: 0.6450\n",
      "Epoch 170/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8129 - accuracy: 0.6275 - val_loss: 0.7995 - val_accuracy: 0.6463\n",
      "Epoch 171/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7938 - accuracy: 0.6458 - val_loss: 0.8004 - val_accuracy: 0.6450\n",
      "Epoch 172/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.7968 - accuracy: 0.6414 - val_loss: 0.7972 - val_accuracy: 0.6457\n",
      "Epoch 173/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.7916 - accuracy: 0.6473 - val_loss: 0.8004 - val_accuracy: 0.6457\n",
      "Epoch 174/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7988 - accuracy: 0.6443 - val_loss: 0.7983 - val_accuracy: 0.6457\n",
      "Epoch 175/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8055 - accuracy: 0.6384 - val_loss: 0.7994 - val_accuracy: 0.6463\n",
      "Epoch 176/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7981 - accuracy: 0.6391 - val_loss: 0.7994 - val_accuracy: 0.6457\n",
      "Epoch 177/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8015 - accuracy: 0.6456 - val_loss: 0.8029 - val_accuracy: 0.6463\n",
      "Epoch 178/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7947 - accuracy: 0.6423 - val_loss: 0.7990 - val_accuracy: 0.6457\n",
      "Epoch 179/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7911 - accuracy: 0.6413 - val_loss: 0.7982 - val_accuracy: 0.6443\n",
      "Epoch 180/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.7991 - accuracy: 0.6378 - val_loss: 0.7983 - val_accuracy: 0.6457\n",
      "Epoch 181/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8185 - accuracy: 0.6306 - val_loss: 0.7974 - val_accuracy: 0.6450\n",
      "Epoch 182/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8136 - accuracy: 0.6334 - val_loss: 0.8004 - val_accuracy: 0.6450\n",
      "Epoch 183/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7951 - accuracy: 0.6441 - val_loss: 0.8005 - val_accuracy: 0.6457\n",
      "Epoch 184/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8037 - accuracy: 0.6390 - val_loss: 0.7977 - val_accuracy: 0.6457\n",
      "Epoch 185/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8044 - accuracy: 0.6409 - val_loss: 0.8011 - val_accuracy: 0.6457\n",
      "Epoch 186/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8038 - accuracy: 0.6417 - val_loss: 0.8008 - val_accuracy: 0.6457\n",
      "Epoch 187/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8066 - accuracy: 0.6324 - val_loss: 0.7999 - val_accuracy: 0.6450\n",
      "Epoch 188/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8047 - accuracy: 0.6352 - val_loss: 0.8007 - val_accuracy: 0.6470\n",
      "Epoch 189/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7986 - accuracy: 0.6425 - val_loss: 0.7967 - val_accuracy: 0.6450\n",
      "Epoch 190/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8029 - accuracy: 0.6416 - val_loss: 0.8003 - val_accuracy: 0.6457\n",
      "Epoch 191/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8060 - accuracy: 0.6381 - val_loss: 0.7996 - val_accuracy: 0.6457\n",
      "Epoch 192/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7900 - accuracy: 0.6497 - val_loss: 0.8031 - val_accuracy: 0.6450\n",
      "Epoch 193/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7947 - accuracy: 0.6420 - val_loss: 0.8000 - val_accuracy: 0.6457\n",
      "Epoch 194/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8063 - accuracy: 0.6364 - val_loss: 0.7956 - val_accuracy: 0.6443\n",
      "Epoch 195/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.7962 - accuracy: 0.6436 - val_loss: 0.7994 - val_accuracy: 0.6450\n",
      "Epoch 196/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8042 - accuracy: 0.6392 - val_loss: 0.7984 - val_accuracy: 0.6457\n",
      "Epoch 197/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8093 - accuracy: 0.6355 - val_loss: 0.7979 - val_accuracy: 0.6443\n",
      "Epoch 198/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8121 - accuracy: 0.6313 - val_loss: 0.8006 - val_accuracy: 0.6457\n",
      "Epoch 199/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7911 - accuracy: 0.6484 - val_loss: 0.8012 - val_accuracy: 0.6457\n",
      "Epoch 200/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7989 - accuracy: 0.6444 - val_loss: 0.8015 - val_accuracy: 0.6457\n",
      "Epoch 201/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8027 - accuracy: 0.6356 - val_loss: 0.7994 - val_accuracy: 0.6457\n",
      "Epoch 202/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8004 - accuracy: 0.6406 - val_loss: 0.7993 - val_accuracy: 0.6457\n",
      "Epoch 203/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7909 - accuracy: 0.6426 - val_loss: 0.7997 - val_accuracy: 0.6463\n",
      "Epoch 204/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8047 - accuracy: 0.6361 - val_loss: 0.8003 - val_accuracy: 0.6457\n",
      "Epoch 205/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.7936 - accuracy: 0.6433 - val_loss: 0.7991 - val_accuracy: 0.6450\n",
      "Epoch 206/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7873 - accuracy: 0.6464 - val_loss: 0.8015 - val_accuracy: 0.6457\n",
      "Epoch 207/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7941 - accuracy: 0.6477 - val_loss: 0.7995 - val_accuracy: 0.6457\n",
      "Epoch 208/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7895 - accuracy: 0.6472 - val_loss: 0.8009 - val_accuracy: 0.6457\n",
      "Epoch 209/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7993 - accuracy: 0.6429 - val_loss: 0.7974 - val_accuracy: 0.6450\n",
      "Epoch 210/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7926 - accuracy: 0.6469 - val_loss: 0.8000 - val_accuracy: 0.6457\n",
      "Epoch 211/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7858 - accuracy: 0.6473 - val_loss: 0.8038 - val_accuracy: 0.6450\n",
      "Epoch 212/1000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8025 - accuracy: 0.6435 - val_loss: 0.7998 - val_accuracy: 0.6450\n",
      "Epoch 213/1000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.7906 - accuracy: 0.6445 - val_loss: 0.7985 - val_accuracy: 0.6437\n",
      "Epoch 214/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8032 - accuracy: 0.6415 - val_loss: 0.7999 - val_accuracy: 0.6463\n",
      "Epoch 215/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7958 - accuracy: 0.6449 - val_loss: 0.8007 - val_accuracy: 0.6470\n",
      "Epoch 216/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7938 - accuracy: 0.6433 - val_loss: 0.7977 - val_accuracy: 0.6477\n",
      "Epoch 217/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7951 - accuracy: 0.6478 - val_loss: 0.7991 - val_accuracy: 0.6463\n",
      "Epoch 218/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8045 - accuracy: 0.6456 - val_loss: 0.8038 - val_accuracy: 0.6463\n",
      "Epoch 219/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7972 - accuracy: 0.6434 - val_loss: 0.7992 - val_accuracy: 0.6463\n",
      "Epoch 220/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8028 - accuracy: 0.6367 - val_loss: 0.7992 - val_accuracy: 0.6457\n",
      "Epoch 221/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7976 - accuracy: 0.6449 - val_loss: 0.8011 - val_accuracy: 0.6457\n",
      "Epoch 222/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7947 - accuracy: 0.6409 - val_loss: 0.7994 - val_accuracy: 0.6450\n",
      "Epoch 223/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7962 - accuracy: 0.6428 - val_loss: 0.7997 - val_accuracy: 0.6463\n",
      "Epoch 224/1000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 0.8056 - accuracy: 0.6324 - val_loss: 0.8006 - val_accuracy: 0.6457\n",
      "Epoch 225/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7980 - accuracy: 0.6427 - val_loss: 0.7999 - val_accuracy: 0.6457\n",
      "Epoch 226/1000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.7850 - accuracy: 0.6508 - val_loss: 0.8003 - val_accuracy: 0.6443\n",
      "Epoch 227/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8204 - accuracy: 0.6274 - val_loss: 0.8008 - val_accuracy: 0.6450\n",
      "Epoch 228/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8040 - accuracy: 0.6375 - val_loss: 0.8007 - val_accuracy: 0.6470\n",
      "Epoch 229/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7941 - accuracy: 0.6404 - val_loss: 0.7992 - val_accuracy: 0.6457\n",
      "Epoch 230/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8060 - accuracy: 0.6409 - val_loss: 0.7987 - val_accuracy: 0.6457\n",
      "Epoch 231/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7975 - accuracy: 0.6450 - val_loss: 0.8006 - val_accuracy: 0.6450\n",
      "Epoch 232/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7976 - accuracy: 0.6434 - val_loss: 0.8015 - val_accuracy: 0.6450\n",
      "Epoch 233/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7998 - accuracy: 0.6398 - val_loss: 0.8009 - val_accuracy: 0.6443\n",
      "Epoch 234/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8097 - accuracy: 0.6385 - val_loss: 0.7995 - val_accuracy: 0.6443\n",
      "Epoch 235/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7918 - accuracy: 0.6423 - val_loss: 0.7999 - val_accuracy: 0.6450\n",
      "Epoch 236/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7967 - accuracy: 0.6356 - val_loss: 0.8016 - val_accuracy: 0.6450\n",
      "Epoch 237/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7914 - accuracy: 0.6494 - val_loss: 0.7995 - val_accuracy: 0.6477\n",
      "Epoch 238/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7999 - accuracy: 0.6396 - val_loss: 0.7995 - val_accuracy: 0.6457\n",
      "Epoch 239/1000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 0.7969 - accuracy: 0.6375 - val_loss: 0.8001 - val_accuracy: 0.6450\n",
      "Epoch 240/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8006 - accuracy: 0.6420 - val_loss: 0.8003 - val_accuracy: 0.6457\n",
      "Epoch 241/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7923 - accuracy: 0.6428 - val_loss: 0.8009 - val_accuracy: 0.6450\n",
      "Epoch 242/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8014 - accuracy: 0.6383 - val_loss: 0.7978 - val_accuracy: 0.6470\n",
      "Epoch 243/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8009 - accuracy: 0.6391 - val_loss: 0.8012 - val_accuracy: 0.6450\n",
      "Epoch 244/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7904 - accuracy: 0.6523 - val_loss: 0.8021 - val_accuracy: 0.6450\n",
      "Epoch 245/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7931 - accuracy: 0.6543 - val_loss: 0.8037 - val_accuracy: 0.6470\n",
      "Epoch 246/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8052 - accuracy: 0.6387 - val_loss: 0.8005 - val_accuracy: 0.6477\n",
      "Epoch 247/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7878 - accuracy: 0.6522 - val_loss: 0.8001 - val_accuracy: 0.6457\n",
      "Epoch 248/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7900 - accuracy: 0.6459 - val_loss: 0.8022 - val_accuracy: 0.6463\n",
      "Epoch 249/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7904 - accuracy: 0.6439 - val_loss: 0.8032 - val_accuracy: 0.6450\n",
      "Epoch 250/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8029 - accuracy: 0.6380 - val_loss: 0.8009 - val_accuracy: 0.6470\n",
      "Epoch 251/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8029 - accuracy: 0.6376 - val_loss: 0.8009 - val_accuracy: 0.6457\n",
      "Epoch 252/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7913 - accuracy: 0.6427 - val_loss: 0.7983 - val_accuracy: 0.6457\n",
      "Epoch 253/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.8004 - accuracy: 0.6373 - val_loss: 0.8008 - val_accuracy: 0.6457\n",
      "Epoch 254/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7945 - accuracy: 0.6449 - val_loss: 0.8021 - val_accuracy: 0.6457\n",
      "Epoch 255/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8080 - accuracy: 0.6296 - val_loss: 0.8002 - val_accuracy: 0.6463\n",
      "Epoch 256/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8048 - accuracy: 0.6369 - val_loss: 0.7990 - val_accuracy: 0.6463\n",
      "Epoch 257/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8156 - accuracy: 0.6308 - val_loss: 0.7998 - val_accuracy: 0.6470\n",
      "Epoch 258/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8024 - accuracy: 0.6364 - val_loss: 0.8021 - val_accuracy: 0.6457\n",
      "Epoch 259/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.7932 - accuracy: 0.6527 - val_loss: 0.8023 - val_accuracy: 0.6457\n",
      "Epoch 260/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8006 - accuracy: 0.6398 - val_loss: 0.8004 - val_accuracy: 0.6450\n",
      "Epoch 261/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7946 - accuracy: 0.6455 - val_loss: 0.8015 - val_accuracy: 0.6457\n",
      "Epoch 262/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7940 - accuracy: 0.6361 - val_loss: 0.8026 - val_accuracy: 0.6463\n",
      "Epoch 263/1000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 0.8040 - accuracy: 0.6325 - val_loss: 0.8039 - val_accuracy: 0.6457\n",
      "Epoch 264/1000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.7957 - accuracy: 0.6434 - val_loss: 0.7986 - val_accuracy: 0.6457\n",
      "Epoch 00264: early stopping\n",
      "4/8 [==============>...............] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hyeongbin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "8/8 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding으로 fit\n",
    "signle_rnn_model = KerasClassifier(build_fn = vanilla_rnn\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])\n",
    "hist = signle_rnn_model.fit(X3_train_ohe, y3_train_ohe)\n",
    "y3_pred_rnn_ohe = signle_rnn_model.predict(X3_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fd39133a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN accouracy :  0.6355841371918542\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN accouracy : \", accuracy_score(y3_pred_rnn_ohe,y3_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "570eb80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'], label='loss')\n",
    "plt.plot(hist.history['val_loss'], label='valid loss')\n",
    "plt.xticks(range(len(hist.history['loss'])))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('precision')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea387569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "24/24 [==============================] - 2s 47ms/step - loss: 0.9497 - accuracy: 0.5448 - val_loss: 0.8362 - val_accuracy: 0.6457\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8267 - accuracy: 0.6392 - val_loss: 0.8301 - val_accuracy: 0.6430\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8354 - accuracy: 0.6375 - val_loss: 0.8410 - val_accuracy: 0.6390\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8328 - accuracy: 0.6327 - val_loss: 0.8339 - val_accuracy: 0.6450\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8349 - accuracy: 0.6392 - val_loss: 0.8323 - val_accuracy: 0.6450\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8278 - accuracy: 0.6386 - val_loss: 0.8329 - val_accuracy: 0.6450\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8291 - accuracy: 0.6389 - val_loss: 0.8312 - val_accuracy: 0.6450\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8312 - accuracy: 0.6316 - val_loss: 0.8310 - val_accuracy: 0.6450\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8261 - accuracy: 0.6333 - val_loss: 0.8300 - val_accuracy: 0.6450\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8147 - accuracy: 0.6445 - val_loss: 0.8293 - val_accuracy: 0.6450\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 0.8254 - accuracy: 0.6428 - val_loss: 0.8292 - val_accuracy: 0.6450\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8244 - accuracy: 0.6398 - val_loss: 0.8284 - val_accuracy: 0.6450\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8365 - accuracy: 0.6285 - val_loss: 0.8296 - val_accuracy: 0.6450\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8183 - accuracy: 0.6406 - val_loss: 0.8276 - val_accuracy: 0.6450\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8211 - accuracy: 0.6356 - val_loss: 0.8270 - val_accuracy: 0.6443\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8192 - accuracy: 0.6382 - val_loss: 0.8267 - val_accuracy: 0.6443\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8289 - accuracy: 0.6335 - val_loss: 0.8264 - val_accuracy: 0.6443\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8142 - accuracy: 0.6411 - val_loss: 0.8242 - val_accuracy: 0.6450\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8150 - accuracy: 0.6465 - val_loss: 0.8265 - val_accuracy: 0.6450\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8205 - accuracy: 0.6419 - val_loss: 0.8301 - val_accuracy: 0.6443\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8280 - accuracy: 0.6377 - val_loss: 0.8245 - val_accuracy: 0.6450\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8195 - accuracy: 0.6482 - val_loss: 0.8286 - val_accuracy: 0.6443\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8266 - accuracy: 0.6313 - val_loss: 0.8235 - val_accuracy: 0.6443\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8221 - accuracy: 0.6384 - val_loss: 0.8220 - val_accuracy: 0.6443\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8250 - accuracy: 0.6360 - val_loss: 0.8146 - val_accuracy: 0.6450\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8178 - accuracy: 0.6431 - val_loss: 0.8077 - val_accuracy: 0.6463\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8124 - accuracy: 0.6368 - val_loss: 0.8082 - val_accuracy: 0.6463\n",
      "Epoch 28/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8011 - accuracy: 0.6464 - val_loss: 0.8099 - val_accuracy: 0.6463\n",
      "Epoch 29/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8171 - accuracy: 0.6402 - val_loss: 0.8106 - val_accuracy: 0.6463\n",
      "Epoch 30/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8146 - accuracy: 0.6360 - val_loss: 0.8144 - val_accuracy: 0.6457\n",
      "Epoch 31/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8220 - accuracy: 0.6370 - val_loss: 0.8088 - val_accuracy: 0.6463\n",
      "Epoch 32/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8014 - accuracy: 0.6469 - val_loss: 0.8015 - val_accuracy: 0.6457\n",
      "Epoch 33/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8084 - accuracy: 0.6400 - val_loss: 0.8163 - val_accuracy: 0.6450\n",
      "Epoch 34/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8127 - accuracy: 0.6402 - val_loss: 0.8130 - val_accuracy: 0.6450\n",
      "Epoch 35/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8076 - accuracy: 0.6433 - val_loss: 0.8106 - val_accuracy: 0.6457\n",
      "Epoch 36/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8016 - accuracy: 0.6419 - val_loss: 0.8062 - val_accuracy: 0.6457\n",
      "Epoch 37/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8008 - accuracy: 0.6465 - val_loss: 0.8151 - val_accuracy: 0.6457\n",
      "Epoch 38/50\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 0.8242 - accuracy: 0.6306 - val_loss: 0.8040 - val_accuracy: 0.6450\n",
      "Epoch 39/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8083 - accuracy: 0.6455 - val_loss: 0.8056 - val_accuracy: 0.6463\n",
      "Epoch 40/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8222 - accuracy: 0.6405 - val_loss: 0.8006 - val_accuracy: 0.6457\n",
      "Epoch 41/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8100 - accuracy: 0.6356 - val_loss: 0.8002 - val_accuracy: 0.6457\n",
      "Epoch 42/50\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.8119 - accuracy: 0.6338 - val_loss: 0.8055 - val_accuracy: 0.6450\n",
      "Epoch 43/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8077 - accuracy: 0.6441 - val_loss: 0.8011 - val_accuracy: 0.6477\n",
      "Epoch 44/50\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.8034 - accuracy: 0.6350 - val_loss: 0.8164 - val_accuracy: 0.6450\n",
      "Epoch 45/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8170 - accuracy: 0.6446 - val_loss: 0.8129 - val_accuracy: 0.6450\n",
      "Epoch 46/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8291 - accuracy: 0.6282 - val_loss: 0.9986 - val_accuracy: 0.6470\n",
      "Epoch 47/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.9731 - accuracy: 0.6325 - val_loss: 0.8827 - val_accuracy: 0.6463\n",
      "Epoch 48/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8628 - accuracy: 0.6426 - val_loss: 0.8350 - val_accuracy: 0.6463\n",
      "Epoch 49/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8358 - accuracy: 0.6399 - val_loss: 0.8300 - val_accuracy: 0.6470\n",
      "Epoch 50/50\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.8164 - accuracy: 0.6469 - val_loss: 0.8283 - val_accuracy: 0.6470\n",
      "1/8 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hyeongbin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# label encoding으로 fit\n",
    "signle_rnn_model.fit(X3_train_label, y3_train_label)\n",
    "y3_pred_rnn_label = signle_rnn_model.predict(X3_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1cdc2790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN accouracy :  0.6387995712754555\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN accouracy : \", accuracy_score(y3_pred_rnn_label,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae32af7",
   "metadata": {},
   "source": [
    "### multi-layer RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c03ae6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_vanilla_rnn():\n",
    "    model = Sequential()\n",
    "    # return_sequences parameter has to be set True to stack\n",
    "    model.add(SimpleRNN(units, input_shape = inputs, return_sequences = True))   \n",
    "    model.add(SimpleRNN(units, return_sequences = False))\n",
    "    model.add(Dense(outputs))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d645e6f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "24/24 [==============================] - 4s 96ms/step - loss: 0.9325 - accuracy: 0.5639 - val_loss: 0.8324 - val_accuracy: 0.6423\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 0.8337 - accuracy: 0.6318 - val_loss: 0.8291 - val_accuracy: 0.6450\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 2s 82ms/step - loss: 0.8472 - accuracy: 0.6268 - val_loss: 0.8164 - val_accuracy: 0.6443\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 2s 83ms/step - loss: 0.8311 - accuracy: 0.6316 - val_loss: 0.8206 - val_accuracy: 0.6443\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 2s 81ms/step - loss: 0.8195 - accuracy: 0.6388 - val_loss: 0.8207 - val_accuracy: 0.6484\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 2s 74ms/step - loss: 0.8182 - accuracy: 0.6399 - val_loss: 0.8072 - val_accuracy: 0.6457\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 2s 80ms/step - loss: 0.8228 - accuracy: 0.6326 - val_loss: 0.8129 - val_accuracy: 0.6470\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8115 - accuracy: 0.6406 - val_loss: 0.8090 - val_accuracy: 0.6477\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8144 - accuracy: 0.6415 - val_loss: 0.8022 - val_accuracy: 0.6490\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8083 - accuracy: 0.6391 - val_loss: 0.8133 - val_accuracy: 0.6477\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 2s 72ms/step - loss: 0.8043 - accuracy: 0.6482 - val_loss: 0.8029 - val_accuracy: 0.6484\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 2s 74ms/step - loss: 0.8176 - accuracy: 0.6411 - val_loss: 0.8131 - val_accuracy: 0.6463\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 0.8149 - accuracy: 0.6393 - val_loss: 0.8106 - val_accuracy: 0.6477\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 2s 77ms/step - loss: 0.8091 - accuracy: 0.6395 - val_loss: 0.8022 - val_accuracy: 0.6497\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 2s 74ms/step - loss: 0.8001 - accuracy: 0.6484 - val_loss: 0.8097 - val_accuracy: 0.6477\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8027 - accuracy: 0.6460 - val_loss: 0.8095 - val_accuracy: 0.6477\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 0.8163 - accuracy: 0.6381 - val_loss: 0.7964 - val_accuracy: 0.6497\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 0.8046 - accuracy: 0.6379 - val_loss: 0.8007 - val_accuracy: 0.6504\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8089 - accuracy: 0.6436 - val_loss: 0.8033 - val_accuracy: 0.6484\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 2s 74ms/step - loss: 0.8019 - accuracy: 0.6441 - val_loss: 0.7987 - val_accuracy: 0.6477\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 0.8198 - accuracy: 0.6390 - val_loss: 0.8074 - val_accuracy: 0.6484\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 2s 72ms/step - loss: 0.8143 - accuracy: 0.6317 - val_loss: 0.8067 - val_accuracy: 0.6484\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 2s 72ms/step - loss: 0.8063 - accuracy: 0.6423 - val_loss: 0.8141 - val_accuracy: 0.6477\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 2s 74ms/step - loss: 0.8091 - accuracy: 0.6408 - val_loss: 0.7975 - val_accuracy: 0.6477\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8096 - accuracy: 0.6368 - val_loss: 0.8043 - val_accuracy: 0.6470\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 2s 71ms/step - loss: 0.8082 - accuracy: 0.6356 - val_loss: 0.8011 - val_accuracy: 0.6477\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 2s 71ms/step - loss: 0.7961 - accuracy: 0.6464 - val_loss: 0.8009 - val_accuracy: 0.6470\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hyeongbin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding으로 fit\n",
    "multi_rnn_model = KerasClassifier(build_fn = stacked_vanilla_rnn\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])\n",
    "multi_rnn_model.fit(X3_train_ohe, y3_train_ohe)\n",
    "y3_pred_multi_rnn_ohe = multi_rnn_model.predict(X3_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba97e97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accouracy :  0.6366559485530546\n"
     ]
    }
   ],
   "source": [
    "print(\"accouracy : \", accuracy_score(y3_pred_multi_rnn_ohe,y3_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "91a54e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "24/24 [==============================] - 4s 88ms/step - loss: 0.9754 - accuracy: 0.5159 - val_loss: 0.8295 - val_accuracy: 0.6463\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 2s 76ms/step - loss: 0.8237 - accuracy: 0.6402 - val_loss: 0.8255 - val_accuracy: 0.6463\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8180 - accuracy: 0.6409 - val_loss: 0.8066 - val_accuracy: 0.6470\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8216 - accuracy: 0.6369 - val_loss: 0.8292 - val_accuracy: 0.6457\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 2s 76ms/step - loss: 0.8220 - accuracy: 0.6479 - val_loss: 0.8294 - val_accuracy: 0.6470\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 0.8183 - accuracy: 0.6404 - val_loss: 0.8290 - val_accuracy: 0.6470\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 2s 79ms/step - loss: 0.8212 - accuracy: 0.6434 - val_loss: 0.8267 - val_accuracy: 0.6463\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 2s 77ms/step - loss: 0.8305 - accuracy: 0.6329 - val_loss: 0.8252 - val_accuracy: 0.6477\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 2s 76ms/step - loss: 0.8144 - accuracy: 0.6431 - val_loss: 0.8337 - val_accuracy: 0.6463\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8243 - accuracy: 0.6462 - val_loss: 0.8215 - val_accuracy: 0.6477\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 0.8154 - accuracy: 0.6430 - val_loss: 0.8193 - val_accuracy: 0.6484\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 2s 74ms/step - loss: 0.8073 - accuracy: 0.6425 - val_loss: 0.8064 - val_accuracy: 0.6470\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8189 - accuracy: 0.6423 - val_loss: 0.8227 - val_accuracy: 0.6484\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 0.8197 - accuracy: 0.6431 - val_loss: 0.8220 - val_accuracy: 0.6470\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 0.8190 - accuracy: 0.6355 - val_loss: 0.8074 - val_accuracy: 0.6484\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 2s 75ms/step - loss: 0.8257 - accuracy: 0.6335 - val_loss: 0.8091 - val_accuracy: 0.6497\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 2s 74ms/step - loss: 0.7892 - accuracy: 0.6526 - val_loss: 0.8160 - val_accuracy: 0.6450\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8121 - accuracy: 0.6435 - val_loss: 0.8442 - val_accuracy: 0.6242\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 2s 74ms/step - loss: 0.8353 - accuracy: 0.6357 - val_loss: 0.8311 - val_accuracy: 0.6457\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 2s 73ms/step - loss: 0.8271 - accuracy: 0.6379 - val_loss: 0.8305 - val_accuracy: 0.6457\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 2s 76ms/step - loss: 0.8267 - accuracy: 0.6364 - val_loss: 0.8272 - val_accuracy: 0.6470\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 2s 76ms/step - loss: 0.8135 - accuracy: 0.6478 - val_loss: 0.8320 - val_accuracy: 0.6463\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hyeongbin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "# Label encoding으로 fit\n",
    "multi_rnn_model.fit(X3_train_label, y3_train_label)\n",
    "y3_pred_multi_rnn_label = multi_rnn_model.predict(X3_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d730bc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accouracy :  0.6366559485530546\n"
     ]
    }
   ],
   "source": [
    "print(\"accouracy : \", accuracy_score(y3_pred_multi_rnn_label,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2a83a",
   "metadata": {},
   "source": [
    "### simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64543d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape = inputs, return_sequences = False))\n",
    "    model.add(Dense(outputs))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3b095fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "24/24 [==============================] - 5s 141ms/step - loss: 1.0635 - accuracy: 0.5872 - val_loss: 0.8383 - val_accuracy: 0.6450\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 3s 117ms/step - loss: 0.8186 - accuracy: 0.6407 - val_loss: 0.8085 - val_accuracy: 0.6457\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 3s 119ms/step - loss: 0.8126 - accuracy: 0.6374 - val_loss: 0.8010 - val_accuracy: 0.6443\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 3s 124ms/step - loss: 0.8122 - accuracy: 0.6353 - val_loss: 0.8005 - val_accuracy: 0.6470\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 3s 124ms/step - loss: 0.8164 - accuracy: 0.6313 - val_loss: 0.8008 - val_accuracy: 0.6463\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 3s 123ms/step - loss: 0.8148 - accuracy: 0.6274 - val_loss: 0.7979 - val_accuracy: 0.6457\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 3s 120ms/step - loss: 0.8081 - accuracy: 0.6378 - val_loss: 0.7936 - val_accuracy: 0.6463\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 3s 118ms/step - loss: 0.8081 - accuracy: 0.6390 - val_loss: 0.7947 - val_accuracy: 0.6450\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 3s 123ms/step - loss: 0.8018 - accuracy: 0.6399 - val_loss: 0.7946 - val_accuracy: 0.6457\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 3s 119ms/step - loss: 0.8069 - accuracy: 0.6361 - val_loss: 0.7927 - val_accuracy: 0.6457\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 3s 121ms/step - loss: 0.8070 - accuracy: 0.6397 - val_loss: 0.7965 - val_accuracy: 0.6457\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 3s 124ms/step - loss: 0.8019 - accuracy: 0.6399 - val_loss: 0.7936 - val_accuracy: 0.6450\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 3s 120ms/step - loss: 0.7954 - accuracy: 0.6465 - val_loss: 0.7969 - val_accuracy: 0.6450\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 3s 118ms/step - loss: 0.8057 - accuracy: 0.6420 - val_loss: 0.8000 - val_accuracy: 0.6450\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 3s 122ms/step - loss: 0.7979 - accuracy: 0.6411 - val_loss: 0.7964 - val_accuracy: 0.6450\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 3s 120ms/step - loss: 0.8161 - accuracy: 0.6321 - val_loss: 0.7943 - val_accuracy: 0.6457\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 3s 120ms/step - loss: 0.8140 - accuracy: 0.6307 - val_loss: 0.7955 - val_accuracy: 0.6443\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 3s 116ms/step - loss: 0.7998 - accuracy: 0.6453 - val_loss: 0.7896 - val_accuracy: 0.6443\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 3s 121ms/step - loss: 0.8153 - accuracy: 0.6306 - val_loss: 0.7871 - val_accuracy: 0.6457\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 3s 119ms/step - loss: 0.7988 - accuracy: 0.6398 - val_loss: 0.7953 - val_accuracy: 0.6457\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 3s 118ms/step - loss: 0.8059 - accuracy: 0.6344 - val_loss: 0.7899 - val_accuracy: 0.6450\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 3s 117ms/step - loss: 0.7983 - accuracy: 0.6386 - val_loss: 0.7913 - val_accuracy: 0.6477\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 3s 121ms/step - loss: 0.7968 - accuracy: 0.6423 - val_loss: 0.8012 - val_accuracy: 0.6457\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 3s 121ms/step - loss: 0.8154 - accuracy: 0.6303 - val_loss: 0.7943 - val_accuracy: 0.6463\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 3s 124ms/step - loss: 0.7995 - accuracy: 0.6453 - val_loss: 0.7859 - val_accuracy: 0.6450\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 3s 124ms/step - loss: 0.8134 - accuracy: 0.6400 - val_loss: 0.7960 - val_accuracy: 0.6463\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 3s 122ms/step - loss: 0.7968 - accuracy: 0.6449 - val_loss: 0.7897 - val_accuracy: 0.6463\n",
      "Epoch 28/50\n",
      "24/24 [==============================] - 3s 124ms/step - loss: 0.7927 - accuracy: 0.6426 - val_loss: 0.7926 - val_accuracy: 0.6463\n",
      "Epoch 29/50\n",
      "24/24 [==============================] - 3s 123ms/step - loss: 0.8056 - accuracy: 0.6359 - val_loss: 0.7944 - val_accuracy: 0.6463\n",
      "Epoch 30/50\n",
      "24/24 [==============================] - 3s 119ms/step - loss: 0.7893 - accuracy: 0.6442 - val_loss: 0.7913 - val_accuracy: 0.6470\n",
      "Epoch 31/50\n",
      "24/24 [==============================] - 3s 122ms/step - loss: 0.7978 - accuracy: 0.6419 - val_loss: 0.7919 - val_accuracy: 0.6463\n",
      "Epoch 32/50\n",
      "24/24 [==============================] - 3s 120ms/step - loss: 0.7934 - accuracy: 0.6486 - val_loss: 0.7927 - val_accuracy: 0.6457\n",
      "Epoch 33/50\n",
      "24/24 [==============================] - 3s 122ms/step - loss: 0.8056 - accuracy: 0.6363 - val_loss: 0.8027 - val_accuracy: 0.6477\n",
      "Epoch 34/50\n",
      "24/24 [==============================] - 3s 119ms/step - loss: 0.7987 - accuracy: 0.6451 - val_loss: 0.7930 - val_accuracy: 0.6484\n",
      "Epoch 35/50\n",
      "24/24 [==============================] - 3s 119ms/step - loss: 0.7899 - accuracy: 0.6447 - val_loss: 0.7916 - val_accuracy: 0.6470\n",
      "Epoch 00035: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hyeongbin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding으로 fit\n",
    "single_lstm_model = KerasClassifier(build_fn = lstm\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])\n",
    "single_lstm_model.fit(X3_train_ohe, y3_train_ohe)\n",
    "y3_pred_single_lstm = single_lstm_model.predict(X3_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f79c271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accouracy :  0.6387995712754555\n"
     ]
    }
   ],
   "source": [
    "print(\"accouracy : \", accuracy_score(y3_pred_single_lstm,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27440f68",
   "metadata": {},
   "source": [
    "### Single Layer LSTM (Label encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17b23630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5968 samples, validate on 1493 samples\n",
      "Epoch 1/50\n",
      "5968/5968 [==============================] - 3s 515us/sample - loss: 0.9884 - acc: 0.6344 - val_loss: 0.8277 - val_acc: 0.6450\n",
      "Epoch 2/50\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.8247 - acc: 0.6387 - val_loss: 0.8124 - val_acc: 0.6463\n",
      "Epoch 3/50\n",
      "5968/5968 [==============================] - 3s 438us/sample - loss: 0.8157 - acc: 0.6386 - val_loss: 0.8060 - val_acc: 0.6470\n",
      "Epoch 4/50\n",
      "5968/5968 [==============================] - 3s 444us/sample - loss: 0.8165 - acc: 0.6384 - val_loss: 0.8036 - val_acc: 0.6470\n",
      "Epoch 5/50\n",
      "5968/5968 [==============================] - 3s 445us/sample - loss: 0.8133 - acc: 0.6377 - val_loss: 0.8029 - val_acc: 0.6470\n",
      "Epoch 6/50\n",
      "5968/5968 [==============================] - 3s 439us/sample - loss: 0.8148 - acc: 0.6381 - val_loss: 0.8012 - val_acc: 0.6470\n",
      "Epoch 7/50\n",
      "5968/5968 [==============================] - 3s 444us/sample - loss: 0.8127 - acc: 0.6379 - val_loss: 0.8032 - val_acc: 0.6470\n",
      "Epoch 8/50\n",
      "5968/5968 [==============================] - 3s 455us/sample - loss: 0.8117 - acc: 0.6377 - val_loss: 0.7999 - val_acc: 0.6457\n",
      "Epoch 9/50\n",
      "5968/5968 [==============================] - 3s 452us/sample - loss: 0.8110 - acc: 0.6376 - val_loss: 0.7977 - val_acc: 0.6463\n",
      "Epoch 10/50\n",
      "5968/5968 [==============================] - 3s 472us/sample - loss: 0.8095 - acc: 0.6386 - val_loss: 0.8057 - val_acc: 0.6450\n",
      "Epoch 11/50\n",
      "5968/5968 [==============================] - 3s 450us/sample - loss: 0.8090 - acc: 0.6366 - val_loss: 0.7996 - val_acc: 0.6450\n",
      "Epoch 12/50\n",
      "5968/5968 [==============================] - 3s 441us/sample - loss: 0.8071 - acc: 0.6382 - val_loss: 0.7941 - val_acc: 0.6457\n",
      "Epoch 13/50\n",
      "5968/5968 [==============================] - 3s 442us/sample - loss: 0.8090 - acc: 0.6382 - val_loss: 0.7987 - val_acc: 0.6463\n",
      "Epoch 14/50\n",
      "5968/5968 [==============================] - 3s 450us/sample - loss: 0.8090 - acc: 0.6376 - val_loss: 0.7966 - val_acc: 0.6463\n",
      "Epoch 15/50\n",
      "5968/5968 [==============================] - 3s 455us/sample - loss: 0.8058 - acc: 0.6384 - val_loss: 0.7986 - val_acc: 0.6443\n",
      "Epoch 16/50\n",
      "5968/5968 [==============================] - 3s 455us/sample - loss: 0.8127 - acc: 0.6377 - val_loss: 0.7966 - val_acc: 0.6463\n",
      "Epoch 17/50\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.8056 - acc: 0.6389 - val_loss: 0.8008 - val_acc: 0.6463\n",
      "Epoch 18/50\n",
      "5968/5968 [==============================] - 3s 447us/sample - loss: 0.8040 - acc: 0.6382 - val_loss: 0.7942 - val_acc: 0.6463\n",
      "Epoch 19/50\n",
      "5968/5968 [==============================] - 3s 449us/sample - loss: 0.8032 - acc: 0.6389 - val_loss: 0.7935 - val_acc: 0.6463\n",
      "Epoch 20/50\n",
      "5968/5968 [==============================] - 3s 464us/sample - loss: 0.8038 - acc: 0.6396 - val_loss: 0.7944 - val_acc: 0.6463\n",
      "Epoch 21/50\n",
      "5968/5968 [==============================] - 3s 478us/sample - loss: 0.8032 - acc: 0.6387 - val_loss: 0.7971 - val_acc: 0.6463\n",
      "Epoch 22/50\n",
      "5968/5968 [==============================] - 3s 476us/sample - loss: 0.8028 - acc: 0.6404 - val_loss: 0.7935 - val_acc: 0.6470\n",
      "Epoch 23/50\n",
      "5968/5968 [==============================] - 3s 488us/sample - loss: 0.8052 - acc: 0.6392 - val_loss: 0.7898 - val_acc: 0.6470\n",
      "Epoch 24/50\n",
      "5968/5968 [==============================] - 3s 474us/sample - loss: 0.8016 - acc: 0.6394 - val_loss: 0.7916 - val_acc: 0.6463\n",
      "Epoch 25/50\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.8031 - acc: 0.6392 - val_loss: 0.7947 - val_acc: 0.6470\n",
      "Epoch 26/50\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.8014 - acc: 0.6397 - val_loss: 0.7920 - val_acc: 0.6457\n",
      "Epoch 27/50\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.8024 - acc: 0.6389 - val_loss: 0.7896 - val_acc: 0.6470\n",
      "Epoch 28/50\n",
      "5968/5968 [==============================] - 3s 460us/sample - loss: 0.8011 - acc: 0.6399 - val_loss: 0.7941 - val_acc: 0.6470\n",
      "Epoch 29/50\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.8021 - acc: 0.6394 - val_loss: 0.7936 - val_acc: 0.6470\n",
      "Epoch 30/50\n",
      "5968/5968 [==============================] - 3s 475us/sample - loss: 0.8029 - acc: 0.6396 - val_loss: 0.7935 - val_acc: 0.6470\n",
      "Epoch 31/50\n",
      "5968/5968 [==============================] - 3s 456us/sample - loss: 0.8001 - acc: 0.6399 - val_loss: 0.7913 - val_acc: 0.6477\n",
      "Epoch 32/50\n",
      "5968/5968 [==============================] - 3s 470us/sample - loss: 0.8021 - acc: 0.6401 - val_loss: 0.7975 - val_acc: 0.6477\n",
      "Epoch 33/50\n",
      "5968/5968 [==============================] - 3s 506us/sample - loss: 0.8034 - acc: 0.6399 - val_loss: 0.8021 - val_acc: 0.6470\n",
      "Epoch 34/50\n",
      "5968/5968 [==============================] - 3s 465us/sample - loss: 0.8001 - acc: 0.6399 - val_loss: 0.7920 - val_acc: 0.6484\n",
      "Epoch 35/50\n",
      "5968/5968 [==============================] - 3s 453us/sample - loss: 0.7980 - acc: 0.6404 - val_loss: 0.7995 - val_acc: 0.6457\n",
      "Epoch 36/50\n",
      "5968/5968 [==============================] - 3s 446us/sample - loss: 0.8005 - acc: 0.6397 - val_loss: 0.7941 - val_acc: 0.6484\n",
      "Epoch 37/50\n",
      "5968/5968 [==============================] - 3s 445us/sample - loss: 0.7987 - acc: 0.6409 - val_loss: 0.7889 - val_acc: 0.6484\n",
      "Epoch 38/50\n",
      "5968/5968 [==============================] - 3s 439us/sample - loss: 0.7960 - acc: 0.6414 - val_loss: 0.7907 - val_acc: 0.6484\n",
      "Epoch 39/50\n",
      "5968/5968 [==============================] - 3s 490us/sample - loss: 0.7955 - acc: 0.6418 - val_loss: 0.7970 - val_acc: 0.6463\n",
      "Epoch 40/50\n",
      "5968/5968 [==============================] - 3s 471us/sample - loss: 0.8052 - acc: 0.6406 - val_loss: 0.7982 - val_acc: 0.6484\n",
      "Epoch 41/50\n",
      "5968/5968 [==============================] - 3s 461us/sample - loss: 0.8007 - acc: 0.6411 - val_loss: 0.7935 - val_acc: 0.6470\n",
      "Epoch 42/50\n",
      "5968/5968 [==============================] - 3s 466us/sample - loss: 0.7982 - acc: 0.6418 - val_loss: 0.7953 - val_acc: 0.6470\n",
      "Epoch 43/50\n",
      "5968/5968 [==============================] - 3s 455us/sample - loss: 0.7960 - acc: 0.6418 - val_loss: 0.7923 - val_acc: 0.6463\n",
      "Epoch 44/50\n",
      "5968/5968 [==============================] - 3s 453us/sample - loss: 0.7959 - acc: 0.6423 - val_loss: 0.7868 - val_acc: 0.6490\n",
      "Epoch 45/50\n",
      "5968/5968 [==============================] - 3s 455us/sample - loss: 0.7987 - acc: 0.6406 - val_loss: 0.7936 - val_acc: 0.6490\n",
      "Epoch 46/50\n",
      "5968/5968 [==============================] - 3s 457us/sample - loss: 0.7946 - acc: 0.6438 - val_loss: 0.7883 - val_acc: 0.6490\n",
      "Epoch 47/50\n",
      "5968/5968 [==============================] - 3s 462us/sample - loss: 0.7955 - acc: 0.6411 - val_loss: 0.7927 - val_acc: 0.6463\n",
      "Epoch 48/50\n",
      "5968/5968 [==============================] - 3s 487us/sample - loss: 0.7954 - acc: 0.6429 - val_loss: 0.7916 - val_acc: 0.6477\n",
      "Epoch 49/50\n",
      "5968/5968 [==============================] - 3s 446us/sample - loss: 0.7931 - acc: 0.6426 - val_loss: 0.7982 - val_acc: 0.6477\n",
      "Epoch 50/50\n",
      "5968/5968 [==============================] - 3s 444us/sample - loss: 0.7965 - acc: 0.6421 - val_loss: 0.7927 - val_acc: 0.6484\n"
     ]
    }
   ],
   "source": [
    "# label encoding으로 fit\n",
    "single_lstm_model.fit(X3_train_label, y3_train_label)\n",
    "y3_pred_single_lstm_label = single_lstm_model.predict(X3_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c71f8909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accouracy :  0.6377277599142551\n"
     ]
    }
   ],
   "source": [
    "print(\"accouracy : \", accuracy_score(y3_pred_single_lstm_label,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ebf58",
   "metadata": {},
   "source": [
    "## multi layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1739052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape = inputs, return_sequences = True))\n",
    "    model.add(LSTM(units, return_sequences = False))\n",
    "    model.add(Dense(outputs))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035205af",
   "metadata": {},
   "source": [
    "### Multi Layer LSTM (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8c8067f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5968 samples, validate on 1493 samples\n",
      "Epoch 1/50\n",
      "5968/5968 [==============================] - 8s 1ms/sample - loss: 0.9401 - acc: 0.6302 - val_loss: 0.8190 - val_acc: 0.6477\n",
      "Epoch 2/50\n",
      "5968/5968 [==============================] - 9s 1ms/sample - loss: 0.8235 - acc: 0.6371 - val_loss: 0.8074 - val_acc: 0.6457\n",
      "Epoch 3/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8150 - acc: 0.6376 - val_loss: 0.8028 - val_acc: 0.6463\n",
      "Epoch 4/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8127 - acc: 0.6376 - val_loss: 0.8015 - val_acc: 0.6450\n",
      "Epoch 5/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8135 - acc: 0.6386 - val_loss: 0.8032 - val_acc: 0.6450\n",
      "Epoch 6/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8149 - acc: 0.6379 - val_loss: 0.8056 - val_acc: 0.6457\n",
      "Epoch 7/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8111 - acc: 0.6382 - val_loss: 0.7970 - val_acc: 0.6457\n",
      "Epoch 8/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8093 - acc: 0.6399 - val_loss: 0.7951 - val_acc: 0.6443\n",
      "Epoch 9/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8105 - acc: 0.6391 - val_loss: 0.7987 - val_acc: 0.6477\n",
      "Epoch 10/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8086 - acc: 0.6372 - val_loss: 0.7999 - val_acc: 0.6450\n",
      "Epoch 11/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8084 - acc: 0.6394 - val_loss: 0.7976 - val_acc: 0.6470\n",
      "Epoch 12/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8054 - acc: 0.6387 - val_loss: 0.7953 - val_acc: 0.6463\n",
      "Epoch 13/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8054 - acc: 0.6391 - val_loss: 0.8045 - val_acc: 0.6450\n",
      "Epoch 14/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8049 - acc: 0.6391 - val_loss: 0.7952 - val_acc: 0.6457\n",
      "Epoch 15/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8041 - acc: 0.6396 - val_loss: 0.7919 - val_acc: 0.6477\n",
      "Epoch 16/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8016 - acc: 0.6391 - val_loss: 0.7962 - val_acc: 0.6484\n",
      "Epoch 17/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8004 - acc: 0.6401 - val_loss: 0.7983 - val_acc: 0.6477\n",
      "Epoch 18/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.8005 - acc: 0.6397 - val_loss: 0.7961 - val_acc: 0.6477\n",
      "Epoch 19/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7979 - acc: 0.6397 - val_loss: 0.7919 - val_acc: 0.6490\n",
      "Epoch 20/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7964 - acc: 0.6402 - val_loss: 0.7964 - val_acc: 0.6470\n",
      "Epoch 21/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7999 - acc: 0.6397 - val_loss: 0.8073 - val_acc: 0.6477\n",
      "Epoch 22/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7963 - acc: 0.6401 - val_loss: 0.7974 - val_acc: 0.6463\n",
      "Epoch 23/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7966 - acc: 0.6404 - val_loss: 0.7912 - val_acc: 0.6484\n",
      "Epoch 24/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7919 - acc: 0.6411 - val_loss: 0.7955 - val_acc: 0.6497\n",
      "Epoch 25/50\n",
      "5968/5968 [==============================] - 12s 2ms/sample - loss: 0.7954 - acc: 0.6401 - val_loss: 0.7906 - val_acc: 0.6497\n",
      "Epoch 26/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7874 - acc: 0.6424 - val_loss: 0.7893 - val_acc: 0.6497\n",
      "Epoch 27/50\n",
      "5968/5968 [==============================] - 12s 2ms/sample - loss: 0.7956 - acc: 0.6402 - val_loss: 0.7907 - val_acc: 0.6504\n",
      "Epoch 28/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7930 - acc: 0.6439 - val_loss: 0.7898 - val_acc: 0.6470\n",
      "Epoch 29/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7872 - acc: 0.6424 - val_loss: 0.7918 - val_acc: 0.6484\n",
      "Epoch 30/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7884 - acc: 0.6413 - val_loss: 0.7950 - val_acc: 0.6490\n",
      "Epoch 31/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7922 - acc: 0.6434 - val_loss: 0.7834 - val_acc: 0.6524\n",
      "Epoch 32/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7820 - acc: 0.6441 - val_loss: 0.8007 - val_acc: 0.6470\n",
      "Epoch 33/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7818 - acc: 0.6434 - val_loss: 0.7976 - val_acc: 0.6484\n",
      "Epoch 34/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7857 - acc: 0.6439 - val_loss: 0.7841 - val_acc: 0.6510\n",
      "Epoch 35/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7795 - acc: 0.6456 - val_loss: 0.7827 - val_acc: 0.6551\n",
      "Epoch 36/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7849 - acc: 0.6428 - val_loss: 0.7910 - val_acc: 0.6504\n",
      "Epoch 37/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7908 - acc: 0.6404 - val_loss: 0.7873 - val_acc: 0.6490\n",
      "Epoch 38/50\n",
      "5968/5968 [==============================] - 12s 2ms/sample - loss: 0.7831 - acc: 0.6426 - val_loss: 0.7816 - val_acc: 0.6497\n",
      "Epoch 39/50\n",
      "5968/5968 [==============================] - 12s 2ms/sample - loss: 0.7858 - acc: 0.6446 - val_loss: 0.7848 - val_acc: 0.6490\n",
      "Epoch 40/50\n",
      "5968/5968 [==============================] - 12s 2ms/sample - loss: 0.7804 - acc: 0.6444 - val_loss: 0.7837 - val_acc: 0.6517\n",
      "Epoch 41/50\n",
      "5968/5968 [==============================] - 12s 2ms/sample - loss: 0.7968 - acc: 0.6443 - val_loss: 0.7906 - val_acc: 0.6497\n",
      "Epoch 42/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7871 - acc: 0.6443 - val_loss: 0.8095 - val_acc: 0.6417\n",
      "Epoch 43/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.8020 - acc: 0.6406 - val_loss: 0.7907 - val_acc: 0.6477\n",
      "Epoch 44/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7801 - acc: 0.6436 - val_loss: 0.7828 - val_acc: 0.6504\n",
      "Epoch 45/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7801 - acc: 0.6463 - val_loss: 0.7912 - val_acc: 0.6484\n",
      "Epoch 46/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7847 - acc: 0.6429 - val_loss: 0.7948 - val_acc: 0.6524\n",
      "Epoch 47/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7825 - acc: 0.6454 - val_loss: 0.7820 - val_acc: 0.6530\n",
      "Epoch 48/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7770 - acc: 0.6458 - val_loss: 0.7812 - val_acc: 0.6517\n",
      "Epoch 49/50\n",
      "5968/5968 [==============================] - 11s 2ms/sample - loss: 0.7824 - acc: 0.6446 - val_loss: 0.9009 - val_acc: 0.5834\n",
      "Epoch 50/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8356 - acc: 0.6290 - val_loss: 0.8089 - val_acc: 0.6443\n"
     ]
    }
   ],
   "source": [
    "multi_lstm_model = KerasClassifier(build_fn = stacked_lstm\n",
    "                        , epochs = epochs, batch_size = batch_size, verbose = 1\n",
    "                       , validation_split = 0.2, callbacks=[early_stopping])\n",
    "multi_lstm_model.fit(X3_train_ohe, y3_train_ohe)\n",
    "y3_pred_multi_lstm = multi_lstm_model.predict(X3_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41171b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM multi  layer accouracy :  0.6371918542336549\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM multi  layer accouracy : \", accuracy_score(y3_pred_multi_lstm,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1412f00c",
   "metadata": {},
   "source": [
    "### Multi Layer LSTM (Label encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0562249",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5968 samples, validate on 1493 samples\n",
      "Epoch 1/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.9523 - acc: 0.6139 - val_loss: 0.8372 - val_acc: 0.6457\n",
      "Epoch 2/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8254 - acc: 0.6371 - val_loss: 0.8178 - val_acc: 0.6477\n",
      "Epoch 3/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8171 - acc: 0.6374 - val_loss: 0.8056 - val_acc: 0.6457\n",
      "Epoch 4/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8138 - acc: 0.6386 - val_loss: 0.8020 - val_acc: 0.6457\n",
      "Epoch 5/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8134 - acc: 0.6386 - val_loss: 0.8017 - val_acc: 0.6470\n",
      "Epoch 6/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8117 - acc: 0.6387 - val_loss: 0.8050 - val_acc: 0.6463\n",
      "Epoch 7/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8105 - acc: 0.6392 - val_loss: 0.8017 - val_acc: 0.6463\n",
      "Epoch 8/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8110 - acc: 0.6394 - val_loss: 0.8006 - val_acc: 0.6470\n",
      "Epoch 9/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8113 - acc: 0.6369 - val_loss: 0.8025 - val_acc: 0.6470\n",
      "Epoch 10/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8092 - acc: 0.6399 - val_loss: 0.8001 - val_acc: 0.6463\n",
      "Epoch 11/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8061 - acc: 0.6401 - val_loss: 0.7935 - val_acc: 0.6457\n",
      "Epoch 12/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8039 - acc: 0.6379 - val_loss: 0.8032 - val_acc: 0.6450\n",
      "Epoch 13/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8060 - acc: 0.6399 - val_loss: 0.7977 - val_acc: 0.6470\n",
      "Epoch 14/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8052 - acc: 0.6394 - val_loss: 0.7933 - val_acc: 0.6477\n",
      "Epoch 15/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8054 - acc: 0.6397 - val_loss: 0.8011 - val_acc: 0.6470\n",
      "Epoch 16/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8029 - acc: 0.6406 - val_loss: 0.7934 - val_acc: 0.6484\n",
      "Epoch 17/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8009 - acc: 0.6406 - val_loss: 0.7920 - val_acc: 0.6477\n",
      "Epoch 18/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7991 - acc: 0.6409 - val_loss: 0.7912 - val_acc: 0.6484\n",
      "Epoch 19/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7973 - acc: 0.6409 - val_loss: 0.7952 - val_acc: 0.6470\n",
      "Epoch 20/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7987 - acc: 0.6416 - val_loss: 0.8042 - val_acc: 0.6484\n",
      "Epoch 21/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8028 - acc: 0.6401 - val_loss: 0.7937 - val_acc: 0.6490\n",
      "Epoch 22/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7965 - acc: 0.6434 - val_loss: 0.7959 - val_acc: 0.6490\n",
      "Epoch 23/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7936 - acc: 0.6431 - val_loss: 0.8030 - val_acc: 0.6463\n",
      "Epoch 24/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8035 - acc: 0.6396 - val_loss: 0.7961 - val_acc: 0.6457\n",
      "Epoch 25/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7962 - acc: 0.6418 - val_loss: 0.7868 - val_acc: 0.6484\n",
      "Epoch 26/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7915 - acc: 0.6426 - val_loss: 0.7942 - val_acc: 0.6484\n",
      "Epoch 27/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7915 - acc: 0.6436 - val_loss: 0.7883 - val_acc: 0.6490\n",
      "Epoch 28/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7929 - acc: 0.6408 - val_loss: 0.7939 - val_acc: 0.6463\n",
      "Epoch 29/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7997 - acc: 0.6409 - val_loss: 0.7904 - val_acc: 0.6504\n",
      "Epoch 30/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7887 - acc: 0.6423 - val_loss: 0.7928 - val_acc: 0.6490\n",
      "Epoch 31/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7876 - acc: 0.6448 - val_loss: 0.7875 - val_acc: 0.6504\n",
      "Epoch 32/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.7857 - acc: 0.6429 - val_loss: 0.7837 - val_acc: 0.6510\n",
      "Epoch 33/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.7859 - acc: 0.6454 - val_loss: 0.8285 - val_acc: 0.6484\n",
      "Epoch 34/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8254 - acc: 0.6377 - val_loss: 0.7981 - val_acc: 0.6484\n",
      "Epoch 35/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8099 - acc: 0.6382 - val_loss: 0.7947 - val_acc: 0.6490\n",
      "Epoch 36/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8059 - acc: 0.6391 - val_loss: 0.7929 - val_acc: 0.6450\n",
      "Epoch 37/50\n",
      "5968/5968 [==============================] - 9s 2ms/sample - loss: 0.8045 - acc: 0.6386 - val_loss: 0.7953 - val_acc: 0.6477\n",
      "Epoch 38/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8027 - acc: 0.6394 - val_loss: 0.7876 - val_acc: 0.6490\n",
      "Epoch 39/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8010 - acc: 0.6404 - val_loss: 0.7983 - val_acc: 0.6497\n",
      "Epoch 40/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8059 - acc: 0.6401 - val_loss: 0.7994 - val_acc: 0.6497\n",
      "Epoch 41/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8019 - acc: 0.6402 - val_loss: 0.7979 - val_acc: 0.6490\n",
      "Epoch 42/50\n",
      "5968/5968 [==============================] - 10s 2ms/sample - loss: 0.8027 - acc: 0.6381 - val_loss: 0.7987 - val_acc: 0.6457\n",
      "Epoch 42: early stopping\n"
     ]
    }
   ],
   "source": [
    "multi_lstm_model.fit(X3_train_label, y3_train_label)\n",
    "y3_pred_multi_lstm_label = multi_lstm_model.predict(X3_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5964a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM multi  layer accouracy :  0.6366559485530546\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM multi  layer accouracy : \", accuracy_score(y3_pred_multi_lstm_label,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbde336",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b458f5",
   "metadata": {},
   "source": [
    "one-hot encoding 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f7ff2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN single layer accouracy :  0.6382636655948553\n",
      "RNN multi  layer accouracy :  0.6366559485530546\n",
      "LSTM single layer accouracy :  0.6404072883172561\n",
      "LSTM multi  layer accouracy :  0.6371918542336549\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN single layer accouracy : \", accuracy_score(y3_pred_rnn_ohe,y3_test_label))\n",
    "print(\"RNN multi  layer accouracy : \", accuracy_score(y3_pred_multi_rnn_ohe,y3_test_label))\n",
    "print(\"LSTM single layer accouracy : \", accuracy_score(y3_pred_single_lstm,y3_test_label))\n",
    "print(\"LSTM multi  layer accouracy : \", accuracy_score(y3_pred_multi_lstm,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3a421",
   "metadata": {},
   "source": [
    "label encoding 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d727ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN single layer accouracy :  0.6393354769560557\n",
      "RNN multi  layer accouracy :  0.6361200428724545\n",
      "LSTM single layer accouracy :  0.6377277599142551\n",
      "LSTM multi  layer accouracy :  0.6366559485530546\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN single layer accouracy : \", accuracy_score(y3_pred_rnn_label,y3_test_label))\n",
    "print(\"RNN multi  layer accouracy : \", accuracy_score(y3_pred_multi_rnn_label,y3_test_label))\n",
    "print(\"LSTM single layer accouracy : \", accuracy_score(y3_pred_single_lstm_label,y3_test_label))\n",
    "print(\"LSTM multi  layer accouracy : \", accuracy_score(y3_pred_multi_lstm_label,y3_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c686389",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
